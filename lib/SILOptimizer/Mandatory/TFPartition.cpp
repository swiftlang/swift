//===--- TFPartition.cpp - Split Tensor ops out of mainline flow ----------===//
//
// This source file is part of the Swift.org open source project
//
// Copyright (c) 2014 - 2017 Apple Inc. and the Swift project authors
// Licensed under Apache License v2.0 with Runtime Library Exception
//
// See https://swift.org/LICENSE.txt for license information
// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors
//
//===----------------------------------------------------------------------===//
//
// This pass splits tensor operations out into seperate functions - one per
// TensorFlow graph that is generated by the TFLowerGraph functionality.
//
//===----------------------------------------------------------------------===//

#define DEBUG_TYPE "tf-partition"
#include "TensorFlow.h"
#include "swift/SILOptimizer/PassManager/Passes.h"
#include "swift/SILOptimizer/PassManager/Transforms.h"
#include "swift/SILOptimizer/Analysis/DominanceAnalysis.h"
#include "swift/SIL/CFG.h"
#include "swift/SIL/DebugUtils.h"
#include "swift/SIL/SILArgument.h"
#include "swift/SIL/SILCloner.h"
#include "swift/AST/DiagnosticsSIL.h"
#include "llvm/ADT/DepthFirstIterator.h"
#include "llvm/Support/CommandLine.h"
#undef DEBUG_TYPE
#include "llvm/Support/GenericDomTreeConstruction.h"
#define DEBUG_TYPE "tf-partition"
using namespace swift;
using namespace tf;

static llvm::cl::opt<bool>
TFDumpIntermediates("tf-dump-intermediates", llvm::cl::init(false),
              llvm::cl::desc("Dump intermediate results in TensorFlow passes"));


template<typename...T, typename...U>
static InFlightDiagnostic
diagnose(ASTContext &Context, SourceLoc loc, Diag<T...> diag, U &&...args) {
  return Context.Diags.diagnose(loc, diag, std::forward<U>(args)...);
}

static bool isOverflowCheckingIntegerOp(SILInstruction *inst) {
  auto *BI = dyn_cast<BuiltinInst>(inst);
  if (!BI) return false;

  switch (BI->getBuiltinInfo().ID) {
  default: return false;
  case BuiltinValueKind::UAddOver:
  case BuiltinValueKind::SAddOver:
  case BuiltinValueKind::USubOver:
  case BuiltinValueKind::SSubOver:
  case BuiltinValueKind::UMulOver:
  case BuiltinValueKind::SMulOver:
    return true;
  }
}

/// Given an overflow-checking integer operation, return true if the overflow
/// result will be unused in the tensor program.  This could be because the
/// overflow result is already dead, because it is extracted but the extract
/// isn't used, or if the result is only used by code that provably won't make
/// it into the tensor program.
static bool canMoveOverflowCheckingInstToTensorProgram(BuiltinInst *BI) {
  // Annoyingly, these builtins are modeled as returning a tuple, which then
  // has tuple extracts hanging off of it.
  for (auto *use : BI->getUses()) {
    auto *extract = dyn_cast<TupleExtractInst>(use->getUser());
    if (!extract) return false;

    // If this is a use of the normal result of the call, then we can ignore
    // it - this can be moved to the tensor program.
    if (extract->getFieldNo() == 0)
      continue;
    assert(extract->getFieldNo() == 1 && "Overflowing ops only have 2 results");

    // Check all uses of the TupleExtract.  If any of them are going to end up
    // in the tensor program, then we cannot move it.
    for (auto *overflowUse : extract->getUses()) {
      auto overflowUser = overflowUse->getUser();
      // CondFail won't get moved over.
      if (isa<CondFailInst>(overflowUser))
        continue;
      return false;
    }
  }
  return true;
}


/// If the specified scalar operation can be partitioned and run on the
/// accelerator, return the name of the op to use to implement it.
static std::string getPartitionedScalarOpName(SILInstruction *I) {
  // We can turn integer and FP literals into constant nodes if their type is
  // compatible.
  if (isa<IntegerLiteralInst>(I) || isa<FloatLiteralInst>(I)) {
    auto resultTy = I->getResults()[0]->getType();
    if (isValidTensorFlowElementType(resultTy.getSwiftRValueType()))
      return "__tfop_Const__dc__";
  }

  auto *BI = dyn_cast<BuiltinInst>(I);
  if (!BI) return std::string();

  // These are true if the first operand is int, fp, etc, not counting vectors
  // or other exotic types.  It is also only true if it is a type we can
  // represent in Tensor elements.
  bool isInt = false, isFP = false;
  if (BI->getNumOperands() != 0) {
    auto opTy = BI->getOperand(0)->getType();
    if (isValidTensorFlowElementType(opTy.getSwiftRValueType())) {
      isInt = opTy.is<BuiltinIntegerType>();
      isFP = opTy.is<BuiltinFloatType>();
    }
  }

  // Perform a final boolean validity check and add the "(t,t)->t" signature
  // for a tensor op.
  auto check = [&](bool cond, StringRef result) -> std::string {
    if (!cond) return std::string();
    return "__tfop_" + result.str() + "__tt__";
  };

  // Given an overflowing operation, return true if the overflow result will be
  // dead in the tensor program.
  auto overflowDead = [&]() -> bool {
    return canMoveOverflowCheckingInstToTensorProgram(BI);
  };

  switch (BI->getBuiltinInfo().ID) {
  default: return StringRef();
  // TODO: FP Comparisons.  Unsigned ops, which need int->uint casts.
  case BuiltinValueKind::ICMP_EQ: return check(isInt, "Equal");
  case BuiltinValueKind::ICMP_NE: return check(isInt, "NotEqual");
  case BuiltinValueKind::ICMP_SLT: return check(isInt, "Less");
  case BuiltinValueKind::ICMP_SGT: return check(isInt, "Greater");
//case BuiltinValueKind::ICMP_UGT: return check(isInt, "Greater");
  case BuiltinValueKind::ICMP_SLE: return check(isInt, "LessEqual");
//case BuiltinValueKind::ICMP_ULE: return check(isInt, "LessEqual");
  case BuiltinValueKind::ICMP_SGE: return check(isInt, "GreaterEqual");
//case BuiltinValueKind::ICMP_UGE: return check(isInt, "GreaterEqual");
  case BuiltinValueKind::Add: return check(isInt, "Add");
  case BuiltinValueKind::Sub: return check(isInt, "Sub");
  case BuiltinValueKind::Mul: return check(isInt, "Mul");
  case BuiltinValueKind::FAdd: return check(isFP, "Add");
  case BuiltinValueKind::FSub: return check(isFP, "Sub");
  case BuiltinValueKind::FMul: return check(isFP, "Mul");
  //case BuiltinValueKind::UAddOver:
  //case BuiltinValueKind::USubOver:
  //case BuiltinValueKind::UMulOver:
  case BuiltinValueKind::SAddOver: return check(isInt&overflowDead(), "Add");
  case BuiltinValueKind::SSubOver: return check(isInt&overflowDead(), "Sub");
  case BuiltinValueKind::SMulOver: return check(isInt&overflowDead(), "Mul");
  }
}



//===----------------------------------------------------------------------===//
//                  BlocksReachingTensorCode CFG Subset
//===----------------------------------------------------------------------===//

/// These nodes mirrors a CFG subset of the function being partitioned, which
/// makes it easy to reuse the dominator algorithms in LLVM.  While it seems
/// silly to make a clone of a graph to produce a filtered view on it, getting
/// the right view projecting iterators to work is more complicated than it is
/// worth at this point.
///
/// As such, this is a very simple representation, which can be optimized later
/// if it ever becomes a performance problem (doubtful).
///
namespace {
class BlocksReachingTensorCode;
struct SILBBSubsetNode {
  // We care about the addresses of these nodes, so disable these to avoid
  // accidental copies.
  SILBBSubsetNode() = delete;
  SILBBSubsetNode(const SILBBSubsetNode&) = delete;
public:
  SILBasicBlock *BB;
  BlocksReachingTensorCode *Parent;
  SILBBSubsetNode(SILBasicBlock *BB, BlocksReachingTensorCode *Parent)
    : BB(BB), Parent(Parent) {}

  SILBBSubsetNode(SILBBSubsetNode &&rhs) {
    BB = rhs.BB;
    Parent = rhs.Parent;
  }

  // These are predecessors and successors of the CFG subset.
  typedef std::vector<SILBBSubsetNode*> BBListTy;
  BBListTy Predecessors, Successors;


  // Requirements of the domtree implementation.
  BlocksReachingTensorCode *getParent() const { return Parent; }
  void printAsOperand(llvm::raw_ostream &O, bool printType = true) {
    BB->printAsOperand(O, printType);
  }
};
} // end anonymous namespace


namespace llvm {
  template <> struct GraphTraits<SILBBSubsetNode*> {
    using ChildIteratorType = SILBBSubsetNode::BBListTy::iterator;
    using Node = SILBBSubsetNode;
    using NodeRef = SILBBSubsetNode*;

    static ChildIteratorType child_begin(NodeRef N) {
      return N->Successors.begin();
    }
    static ChildIteratorType child_end(NodeRef N) {
      return N->Successors.end();
    }
  };

  template <> struct GraphTraits<Inverse<SILBBSubsetNode*>> {
    using ChildIteratorType = SILBBSubsetNode::BBListTy::iterator;
    using Node = SILBBSubsetNode;
    using NodeRef = SILBBSubsetNode*;
    static ChildIteratorType child_begin(NodeRef N) {
      return N->Predecessors.begin();
    }
    static ChildIteratorType child_end(NodeRef N) {
      return N->Predecessors.end();
    }
  };
} // end namespace llvm



namespace {
/// Represent a subset of a function in a way that we can fulfill the model of
/// the LLVM Graph abstractions, allowing us to get post-dominators for a subset
/// of the nodes and edges in a function.
///
/// A Swift function that contains tensor operations will often have a number
/// of blocks "hanging off it" that represent failure paths: integer overflow
/// checks, precondition failures, etc.  Also, the function may have a
/// significant amount of general computation that happens after the meat of
/// tensor computation happens.
///
/// Unfortunately, these exits blocks in particular will generally pessimize
/// PostDominator information, because the extraneous edges go to blocks that
/// end with unreachable.  This means that we frequently get degenerate blocks
/// where the only post dominator of two related blocks is the exit node for
/// the function, which unifies the blocks that end with 'return' and the
/// blocks that end with 'unreachable'.
///
/// To solve this, we compute the set of blocks that we need to represent in
/// the tensor slice because they can reach tensor computation or a direct use
/// of that computation.  This subset of the function is what we need to
/// generate accurate post dominator info.
///
class BlocksReachingTensorCode {
  friend struct llvm::GraphTraits<BlocksReachingTensorCode*>;
  friend struct llvm::GraphTraits<llvm::Inverse<BlocksReachingTensorCode*>>;

  /// The function this slice is a subset of.
  SILFunction &fn;

  /// These are all of the nodes themselves.
  std::vector<SILBBSubsetNode> nodes;

  /// This map contains all of the SILBBSubsetNode's that make up the subset
  /// graph.
  llvm::DenseMap<SILBasicBlock*, SILBBSubsetNode*> nodeMap;

  /// This is the post dominator tree built over our node subset.
  llvm::DominatorTreeBase<SILBBSubsetNode, true> PDI;
public:
  BlocksReachingTensorCode(SILFunction &fn) : fn(fn) {}
  void compute(ArrayRef<SILInstruction*> ops);

  SILBBSubsetNode *getNode(SILBasicBlock *BB) const {
    auto i = nodeMap.find(BB);
    assert(i != nodeMap.end() && i->second && "BasicBlock not in our subset");
    return i->second;
  }

  SILBBSubsetNode *getEntryBlock() const {
    return getNode(fn.getEntryBlock());
  }

  /// Return true if the specified block is in our subset of the function.
  bool contains(SILBasicBlock *bb) const {
    return nodeMap.count(bb);
  }

  bool postDominates(SILBasicBlock *dominator, SILBasicBlock *dominatee) {
    return PDI.dominates(getNode(dominator), getNode(dominatee));
  }

  SILBasicBlock *findNearestCommonPostDominator(SILBasicBlock *B1,
                                                SILBasicBlock *B2) {
    auto res = PDI.findNearestCommonDominator(getNode(B1), getNode(B2));
    return res ? res->BB : nullptr;
  }


  SILBasicBlock *getPostIDom(SILBasicBlock *BB) {
    auto PDINode = PDI[getNode(BB)]->getIDom();
    return PDINode ? PDINode->getBlock()->BB : nullptr;
  }

  void dump();

public:
  // Random stuff used by DominatorTree internals.  Don't use generally.
  SILBBSubsetNode &front() const { return *getEntryBlock(); }
};
} // end anonymous namespace

/// \brief An iterator type that allows iterating over the address of the
/// elements returned via some other iterator.
///
/// \code
///   using iterator = address_iterator<SmallVectorImpl<T>::iterator>;
/// \endcode
template <typename WrappedIteratorT,
          typename T = decltype(&*std::declval<WrappedIteratorT>())>
struct address_iterator
  : llvm::iterator_adaptor_base<
          address_iterator<WrappedIteratorT>, WrappedIteratorT,
          typename std::iterator_traits<WrappedIteratorT>::iterator_category,
          T> {
  address_iterator() = default;
  template <typename U>
  address_iterator(U &&u)
      : address_iterator::iterator_adaptor_base(std::forward<U &&>(u)) {}

  T operator*() const { return &*this->I; }
};

namespace llvm {
  template<>
  struct GraphTraits<BlocksReachingTensorCode*>
                            : public GraphTraits<SILBBSubsetNode*> {
    using GraphType = BlocksReachingTensorCode*;
    using NodeRef = SILBBSubsetNode*;

    static NodeRef getEntryNode(GraphType F) {
      return F->getEntryBlock();
    }

    typedef address_iterator<std::vector<SILBBSubsetNode>::iterator>
                               nodes_iterator;
    static nodes_iterator nodes_begin(GraphType F) {
      return nodes_iterator(F->nodes.begin());
    }
    static nodes_iterator nodes_end(GraphType F) {
      return nodes_iterator(F->nodes.end());
    }
                              
    static unsigned size(GraphType F) { return F->nodes.size(); }
  };

  template<> struct GraphTraits<Inverse<BlocksReachingTensorCode*>>
  : public GraphTraits<Inverse<swift::SILBasicBlock*>> {
    using GraphType = Inverse<BlocksReachingTensorCode*>;
    using NodeRef = SILBBSubsetNode*;

    typedef address_iterator<std::vector<SILBBSubsetNode>::iterator>
                              nodes_iterator;
    static nodes_iterator nodes_begin(GraphType F) {
      return nodes_iterator(F.Graph->nodes.begin());
    }
    static nodes_iterator nodes_end(GraphType F) {
      return nodes_iterator(F.Graph->nodes.end());
    }
    static unsigned size(GraphType F) { return F.Graph->nodes.size(); }
  };
} // end namespace llvm

// This template requires explicit instantiation.
template class llvm::DominatorTreeBase<SILBBSubsetNode, true>;

/// Compute the set of blocks that can reach the specified operations, and
/// uses of them.
void BlocksReachingTensorCode::compute(ArrayRef<SILInstruction*> ops) {
  assert(!ops.empty() && "Cannot compute empty CFG subset");
  SmallVector<SILBasicBlock*, 8> worklist;

  // We seed the worklist with the blocks that the operations occur in, along
  // with the blocks containing all uses of the operands.  These uses may not
  // themselves be tensor results that we partition: but they may be the send
  // operations that cause copy out or function results.
  for (auto *i : ops) {
    auto instBB = i->getParent();
    worklist.push_back(instBB);

    // Add the blocks that any users live in.
    for (auto result: i->getResults()) {
      for (auto user : result->getUses()) {
        if (user->getUser()->getParent() != instBB)
          worklist.push_back(user->getUser()->getParent());
      }
    }
  }

  // Make sure the nodes are never reallocated out from under us.
  nodes.reserve(fn.getBlocks().size());

  // Figure out all of the blocks that should be included.
  while (!worklist.empty()) {
    auto *bb = worklist.pop_back_val();

    // If we already visited this block, we're done.  Otherwise insert it,
    // creating the SILBBSubsetNode for this BB.
    auto &entry = nodeMap[bb];
    if (entry != nullptr) continue;

    nodes.emplace_back(SILBBSubsetNode(bb, this));
    entry = &nodes.back();

    // Its predecessors can also reach a tensor op.
    worklist.append(bb->pred_begin(), bb->pred_end());
  }

  assert(nodeMap.count(fn.getEntryBlock()) &&
         "Entry block should be reachable from Tensor work");

  // Now that all of the nodes are created, we can wire up the predecessor and
  // successor lists.
  for (auto &node : nodes) {
    for (auto *succ : node.BB->getSuccessorBlocks()) {
      auto it = nodeMap.find(succ);
      if (it != nodeMap.end())
        node.Successors.push_back(it->second);
    }
    for (auto *pred : node.BB->getPredecessorBlocks()) {
      auto it = nodeMap.find(pred);
      if (it != nodeMap.end())
        node.Predecessors.push_back(it->second);
    }
  }

  // Now that we have our CFG subset, compute the post dominator tree from it.
  PDI.recalculate(*this);
}

void BlocksReachingTensorCode::dump() {
  PDI.print(llvm::errs());
}


//===----------------------------------------------------------------------===//
//                             FunctionPartitioner
//===----------------------------------------------------------------------===//

namespace {
/// Marking values in the host program need to either be moved, copied, or have
/// their results sent over to the accelerator.
enum class Marking {
  Copy,      // This instruction is run on both the host and accelerator.
  Move,      // This instruction is run on the accelerator, not the host.
  Send,      // The value produced by this instruction is copied to accelerator.
  Argument,  // The value is passed as an argument to the tensor function.
  Delete,    // This instruction is simply deleted (e.g. debug_value)
};

class TFFunctionPartition {
public:
  SILFunction &fn;
  StructDecl *tensorCoreType;
  DominanceInfo &DI;
  BlocksReachingTensorCode tensorCodeBlocks;

  /// These are all the tensor ops found in the initial scan over the function.
  SmallPtrSet<SILInstruction*, 8> tensorOpsSet;

  /// This keeps track of the set of blocks that are marked as needing to be
  /// partitioned out to the accelerator.  If the block is in this set, then
  /// some instruction in the block has to run on the accelerator.
  SmallPtrSet<SILBasicBlock*, 8> markedBlocks;

  /// This contains a set of all basic blocks that are immediate successors of
  /// TensorOp blocks, but which are outside of the tensor program (typically
  /// these are error edges).  These blocks need to kill the tensor program if
  /// reached.
  SmallPtrSet<SILBasicBlock*, 8> tensorKillBlocks;

  /// As the primary tensor operations are marked, the nearest common ancestor
  /// (NCA) in the dominator tree of the tensor operations is found.  This will
  /// be the entry block of the tensor computation, and marks the point where
  /// the tensor computation is started (and where arguments are passed).
  ///
  /// Among other things, this trims off the front matter that often ends up at
  /// the beginning of functions.
  ///
  /// The instruction pointed to here is either the first Tensor operation (if
  /// there is one which dominates all other ops) or the terminator of the block
  /// that dominates all of the operations.
  SILInstruction *tensorStartPoint = nullptr;

  /// Similar to the start point, this indicates the first instruction after
  /// the last Tensor operation, which is when the computation should be
  /// completed and results are returned.  If there are no tensor ops in the
  /// final block, then this will be the entry instruction.
  SILInstruction *tensorEndPoint = nullptr;

  /// The values passed as arguments to the tensor function.
  SmallVector<SILValue, 4> tensorFnArguments;

  /// The instructions that are to be run on the accelerator.
  llvm::DenseMap<SILInstruction*, Marking> markedInstructions;

  /// BB Arguments that are marked as being moved or copied over.  If a marked
  /// argument is moved over, it is deleted from the host program.  If the
  /// host also uses the argument, then a copy will have to be inserted back
  /// from the accelerator to the host.
  llvm::DenseMap<SILArgument*, Marking> markedBBArguments;

  /// The set of values that must be sent to the accelerator.
  SmallPtrSet<SILValue, 8> valuesToSend;

  /// Set of all of the __tfop_send calls that silence copy-in warnings.
  SmallPtrSet<SILInstruction*, 8> explicitCopyMarkers;

  /// These are the results of tensor values that should be returned by the
  /// extracted function.
  SmallVector<SILValue, 4> resultValues;
public:
  TFFunctionPartition(SILFunction &Fn, StructDecl *tensorCoreType,
                      SILPassManager *PM)
    : fn(Fn),
      tensorCoreType(tensorCoreType),
      DI(*PM->getAnalysis<DominanceAnalysis>()->get(&Fn)),
      tensorCodeBlocks(Fn) {
  }

  SILFunction *run();


  void diagnoseCopyInIfNotSend(SILValue value);
  bool diagnoseCopyOutIfNotReceive(SILValue value, SILInstruction *user);

private:
  void markBlock(SILBasicBlock *BB);
  void markInstruction(SILInstruction &inst, Marking mark);
  void markArgument(SILArgument *arg);
  void markValue(SILValue value);
  void markFunction();
};
} // end anonymous namespace



/// Check to see if the specified value being copied into a partition for the
/// accelerator is our designated "send" operation.  If so, we're fine,
/// otherwise emit a warning to tell the programmer that they are doing
/// something that induces an implicit data transfer into their code.
void TFFunctionPartition::diagnoseCopyInIfNotSend(SILValue value) {
  // If it isn't the result of a "send" operation, then produce a warning about
  // an implicit copy to the accelerator.
  if (auto *apply = dyn_cast<ApplyInst>((SILNode*)value))
    if (auto *callee = dyn_cast<FunctionRefInst>(apply->getCallee()))
      if (callee->getReferencedFunction()->getName().contains("__tfop_send")) {
        explicitCopyMarkers.insert(apply);
        return;
      }

  // Try to determine a good source location to report.
  auto loc = getUserSourceLocation(value.getLoc(), value);

  // Emit the warning.
  diagnose(fn.getModule().getASTContext(), loc.getSourceLoc(),
           diag::tf_value_implicitly_copied_to_accel)
  .highlight(loc.getSourceRange());
}

bool TFFunctionPartition::
diagnoseCopyOutIfNotReceive(SILValue value, SILInstruction *user) {
  /// Check to see if the specified value being copied into a partition for the
  /// accelerator is our designated "receive" operation.  If so, we're fine,
  /// otherwise emit a warning to tell the programmer that they are doing
  /// something that induces an implicit data transfer into their code.

  // If it isn't the result of a "send" operation, then produce a warning about
  // an implicit copy to the accelerator.
  if (auto *apply = dyn_cast<ApplyInst>(user))
    if (auto *fn = dyn_cast<FunctionRefInst>(apply->getCallee()))
      if (fn->getReferencedFunction()->getName().contains("__tfop_receive")) {
        explicitCopyMarkers.insert(apply);
        return false;
      }

  // Try to determine a good source location to report.
  auto loc = getUserSourceLocation(value.getLoc(), value);

  // Emit the warning.
  diagnose(fn.getModule().getASTContext(), loc.getSourceLoc(),
           diag::tf_value_implicitly_copied_to_host)
  .highlight(loc.getSourceRange());

  auto userLoc = getUserSourceLocation(user->getLoc(), user);
  if (loc.getSourceLoc() != userLoc.getSourceLoc()) {
    diagnose(fn.getModule().getASTContext(), userLoc.getSourceLoc(),
             diag::tf_value_implicitly_copied_to_host_computed_used_here)
    .highlight(userLoc.getSourceRange());
  }
  return true;
}



/// Some instruction in the specified block needs to be split out to the
/// accelerator, so we mark it (and its control dependencies) as to-be-moved
/// over.
void TFFunctionPartition::markBlock(SILBasicBlock *BB) {
  // "tensorStartPoint" marks the start of the extracted function, so it must
  // dominate all blocks that are to be extracted.
  assert(DI.dominates(tensorStartPoint->getParent(), BB) &&
         "Marking instructions out of the tensor region?");

  // Insert the block into our set - if the block is already there, we have
  // nothing more to do.
  if (!markedBlocks.insert(BB).second)
    return;

  // Walk predecessors until we find marked blocks or other blocks we are
  // control dependent on.  We only scan the region post dominated by BB.
  //
  // Note that though this is bounded, that it isn't a very efficient algorithm
  // since each block marking can walk the entire function's CFG, but it is good
  // enough for now.  It would probably make more sense to walk the pdom tree
  // instead of walking the CFG.
  SmallVector<SILBasicBlock*, 8> worklist;
  worklist.push_back(BB);

  // The visited set keeps track of blocks that have been added to the worklist,
  // to ensure we don't process something more than once.
  SmallPtrSet<SILBasicBlock*, 32> visited;
  visited.insert(BB);

  // Walk up the CFG looking for terminators we are control dependent on.
  while (!worklist.empty()) {
    auto thisBB = worklist.pop_back_val();
    assert(tensorCodeBlocks.postDominates(BB, thisBB) &&
           "Should only be scanning the region pdom'd by BB");

    // If we found the start of the region we are extracting, then stop there.
    if (thisBB == tensorStartPoint->getParent())
      continue;

    // Check the predecessors of this block.  If any of them have multiple
    // successors, then we may be control dependent on that conditional.
    for (auto pred : thisBB->getPredecessorBlocks()) {
      // Count the number of successors of this block which are tensor related.
      // If we see successors that are not tensor related, we'll remember that
      // so we can insert a kill of the tensor program.
      unsigned numTensorSuccs = 0;
      for (auto succ : pred->getSuccessorBlocks()) {
        if (tensorCodeBlocks.contains(succ))
          ++numTensorSuccs;
        else {
          assert(succ->getSinglePredecessorBlock() &&
                 "Need to split critical edges??");
          tensorKillBlocks.insert(succ);
        }
      }

      // If the predecessor has a single tensor-related successor (us) then we
      // aren't control dependent on it.
      if (numTensorSuccs == 1)
        continue;

      // Check to see if pred is already visited or if its terminator is already
      // marked then we don't need to reprocess it.
      if (visited.count(pred) ||
          markedInstructions.count(pred->getTerminator()))
        continue;

      // Otherwise, check to see if BB is post-dominated by thisBB, but not by
      // pred - in other words that it is the post dominance frontier for BB.
      // If so, that means that the terminator in pred controls whether BB is
      // executed, so it must be marked.
      if (!tensorCodeBlocks.postDominates(BB, pred)) {
        auto predTerm = pred->getTerminator();
        if (isa<BranchInst>(predTerm) || isa<CondBranchInst>(predTerm)) {
          markInstruction(*predTerm, Marking::Copy);
          continue;
        }

        predTerm->dump();
        assert(0 && "FIXME: Handle non-branch terminators like try_apply, which"
               "cannot (in general) be moved to the accelerator");
      }

      // Otherwise, the predecessor is just another block post-dominated by BB,
      // continue walking it.
      worklist.push_back(pred);
      visited.insert(pred);
    }
  }
}


static bool shouldPartition(SILInstruction *I) {
  // We can handle (tuple_extract x, 0) if x is an overflow-checking integer
  // operation.
  if (auto *TE = dyn_cast<TupleExtractInst>(I)) {
    auto *op = dyn_cast<SILInstruction>((SILNode*)TE->getOperand());
    if (!op || TE->getFieldNo() != 0 || !isOverflowCheckingIntegerOp(op))
      return false;

    // We can only handle this tuple_extract if the underlying instruction can
    // be handled.  This can depend on dtype support, whether the overflow
    // flag is used, etc.
    return shouldPartition(op);
  }

  // TODO: We should do a bit more cost model analysis on this.  It doesn't make
  // sense to pull over a partial chain of computation when some root can't be
  // done there.  It is just shifting compute around.
  return !getPartitionedScalarOpName(I).empty();
}

void TFFunctionPartition::markInstruction(SILInstruction &inst, Marking mark) {
  // Insert the specified instruction into the marked set.  If it is already
  // there then we have nothing more to do.
  if (!markedInstructions.insert({&inst, mark}).second)
    return;

  // If we're moving a computation to the accelerator, we can remove any
  // debug_value and retain/release instructions using this one.
  if (mark == Marking::Move) {
    for (auto result : inst.getResults())
      for (auto *operand : result->getUses()) {
        auto user = operand->getUser();
        if (isa<DebugValueInst>(user) || isa<RefCountingInst>(user))
          markInstruction(*user, Marking::Delete);

        // Also handle retain(struct_extract(tensorcore)), which shouldn't
        // come up, but it does.
        // TODO: This will not happen when TensorCore is a builtin type, this
        // code can be deleted then.
        if (auto *SE = dyn_cast<StructExtractInst>(user)) {
          if (SE->hasOneUse()) {
            if (auto RCI = dyn_cast<RefCountingInst>(
                                             SE->getSingleUse()->getUser())) {
              markInstruction(*user, Marking::Delete);
              markInstruction(*RCI, Marking::Delete);
            }
          }
        }
      }
  }

  // If we are simply deleting this instruction, then we're done.
  if (mark == Marking::Delete)
    return;

  // Make sure the instruction's block is marked as being copied over to the
  // tensor program.
  markBlock(inst.getParent());

  // If we have an uncond branch with basic block arguments, don't add operands
  // as used values.  Instead, we'll use more careful conditional liveness
  // based on whether the BB args in the successor are live.
  if (isa<BranchInst>(&inst))
    return;

  // If we are moving the instruction over, then we know it is a tensor op, and
  // it get special attention.  Otherwise, we just recursively mark the
  // operands in question.
  if (mark != Marking::Move) {
    auto operandRange = inst.getAllOperands();

    // Don't specifically mark the first operand to an apply instruction:
    // it is a function_ref, which we handle specially as part of the op.
    if (isa<ApplyInst>(inst))
      operandRange = operandRange.drop_front();

    // Overflow-checking integer ops have a "should check" bit as their last
    // parameter, we don't remap it, so don't mark it.
    if (isOverflowCheckingIntegerOp(&inst)) {
      assert(operandRange.back().get()->getType().is<BuiltinIntegerType>());
      operandRange = operandRange.drop_back();
    }

    // Scan the operands to make sure they are available: either by moving the
    // computation over to the accelerator, by copying the value over, or by
    // passing as an argument to the tensor computation.
    for (auto &op : operandRange)
      markValue(op.get());
    return;
  }

  // Okay, we know that the instruction is a tensor op.  Decode its argument
  // list so we know how to handle the operands.
  TensorOpInfo tfopInfo(inst);
  bool isTensor = tfopInfo.decode();
  assert(isTensor && "Expected a tensor op"); (void)isTensor;

  unsigned nextOperand = 1; // skip the function ref.
  for (auto operandInfo : tfopInfo.operandDescriptors) {
    switch (operandInfo) {
    case OpCommand::AddDType:
      break;  // No operand corresponds to this.
    case OpCommand::Tensor:
      // Tensor operands are recursively marked.
      markValue(inst.getOperand(nextOperand++));
      break;
    case OpCommand::Constant:
      // No need to mark this operand.
      ++nextOperand;
      break;
    }
  }
}

void TFFunctionPartition::markArgument(SILArgument *arg) {
  // If we've already marked this argument, there is nothing more to do.
  if (markedBBArguments.count(arg))
    return;

  // Make sure the argument's block is marked as being copied.
  markBlock(arg->getParent());

  // If this BB argument is outside the region dominated by the start point,
  // then we pass its value in as an argument to the tensor function.
  if (!DI.properlyDominates(tensorStartPoint->getParent(), arg->getParent())) {
    markedBBArguments.insert({arg, Marking::Argument});
    tensorFnArguments.push_back(SILValue(arg));
    diagnoseCopyInIfNotSend(arg);
    return;
  }

  // Otherwise, if this is a value of TensorCore type, then we move it to the
  // accelerator.  If it is also used on the host, it will be copied back.
  if (isTensorCore(arg->getType().getSwiftRValueType())) {
    // We cannot move over arguments, but they should never be in the dominated
    // region anyway.
    assert(!isa<SILFunctionArgument>(arg) &&
           "Cannot move function parameters!");
    markedBBArguments.insert({arg, Marking::Move});
  } else {
    markedBBArguments.insert({arg, Marking::Copy});
  }

  // Otherwise, we mark the branches that contribute values to it, then mark
  // their formal BB argument values that correspond to this argument.
  bool hasUncondBr = false;
  for (auto *pred : arg->getParent()->getPredecessorBlocks()) {
    auto predTerm = pred->getTerminator();
    hasUncondBr |= isa<BranchInst>(predTerm);
    markInstruction(*predTerm, Marking::Copy);
  }

  // We handle conditional liveness of "phi node" like arguments which are set
  // by unconditional branches.  Arguments resulting from other terminators are
  // handled eagerly.
  if (hasUncondBr) {
    SmallVector<SILValue, 4> incomingValues;
    arg->getIncomingValues(incomingValues);
    for (auto v : incomingValues)
      markValue(v);
  }
}


/// Indicate that the specified value must be available on the accelerator.
/// This can be done by moving the computation over, or by inserting a data
/// transfer.
void TFFunctionPartition::markValue(SILValue value) {
  // We can safely ignore SILUndef, since SILCloner will just make another
  // one for us.
  if (isa<SILUndef>(value))
    return;

  if (auto *arg = dyn_cast<SILArgument>(value))
    return markArgument(arg);

  auto *inst = cast<SILInstruction>((SILNode*)value);
  if (markedInstructions.count(inst))
    return;

  // If this is a reference to a tensor op that we haven't gotten to yet, just
  // ignore it.  The outer marking loop will find it and mark it.
  if (tensorOpsSet.count(inst))
    return;

  // If the value is defined outside of the region dominated by the tensor
  // operations, then it is passed in as an argument to the tensor function.
  if (!DI.properlyDominates(tensorStartPoint, inst)) {
    markedInstructions.insert({inst, Marking::Argument});
    tensorFnArguments.push_back(value);
    diagnoseCopyInIfNotSend(value);
    return;
  }

  // If this is a scalar operation that we can partition to the accelerator and
  // if it makes sense, mark it as being copied over (this leads to its operands
  // being recursively copied as well).
  if (shouldPartition(inst))
    return markInstruction(*inst, Marking::Copy);

  // Otherwise, insert a send from the host to the accelerator.
  valuesToSend.insert(value);
  diagnoseCopyInIfNotSend(value);

  // Instead of cloning over this instruction, we'll add a send after it and
  // insert a receive in the accelerator code.
  markedInstructions.insert({inst, Marking::Send});
  markBlock(inst->getParent());
}

/// Given a list of tensor operations, find the nearest common ancestor of those
/// operations in the [post-]dominator-tree of the CFG.  In addition to finding
/// the NCA, this also returns the list of ops that are in that block (if any).
static SILBasicBlock *
findNCAOfTensorOps(ArrayRef<SILInstruction*> tensorOps,
                   SmallPtrSet<SILInstruction*, 8> &ncaBBOps,
         std::function<SILBasicBlock*(SILBasicBlock*,SILBasicBlock*)> findNCA) {
  assert(!tensorOps.empty() && "expect at least one tensor op");

  auto ncaBlock = tensorOps[0]->getParent(); // Arbitrary starting point.

  for (auto inst : tensorOps) {
    // If this op is in the ncaBlock, just remember it.
    auto instBB = inst->getParent();
    if (instBB == ncaBlock) {
      ncaBBOps.insert(inst);
      continue;
    }

    // Otherwise, it is possible that the startBB already dominates instBB.  If
    // so, the NCA Will not change.
    auto NCA = findNCA(ncaBlock, instBB);
    if (NCA == ncaBlock)
      continue;

    // Otherwise, the instBB dominated startBB, or neither dominated the other
    // one (meaning NCA is some unrelated parent block).  In either case, it
    // becomes our new startBB.
    ncaBlock = NCA;
    ncaBBOps.clear();
    ncaBBOps.insert(inst);
  }

  return ncaBlock;
}

/// Scan the function looking for blocks with tensor operations in them.  As
/// we find them, mark them as "to-be-partitioned", which marks (transitive)
/// data and control dependencies.
void TFFunctionPartition::markFunction() {
  // We walk the function in depth first order so that we only visit reachable
  // blocks and to slightly improve compile time performance of the 'marking'
  // operation.
  SmallVector<SILInstruction*, 32> tensorOps;
  for (auto *BB : llvm::depth_first(&fn)) {
    for (auto &inst : *BB) {
      if (TensorOpInfo(inst).decode()) {
        tensorOps.push_back(&inst);
        tensorOpsSet.insert(&inst);
      }
    }
  }

  // If there is nothing to do, don't touch this function.
  if (tensorOps.empty())
    return;

  // Compute the blocksReachingTensorCode set.
  tensorCodeBlocks.compute(tensorOps);

  // Next, compute the NCA of all of the core tensor operations as our "start
  // point".  This is where we will start the tensor computation, sending over
  // argument values defined outside the scope of the computation.
  SmallPtrSet<SILInstruction*, 8> bbOps;
  auto startBB = findNCAOfTensorOps(tensorOps, bbOps,
                                    [&] (SILBasicBlock *B1,
                                         SILBasicBlock *B2) -> SILBasicBlock* {
    return DI.findNearestCommonDominator(B1, B2);
  });

  // Compute the start point by doing a linear scan of the startBB to find the
  // first (of possibly many) tensor operations.
  tensorStartPoint = startBB->getTerminator();
  if (!bbOps.empty()) {
    for (auto &inst : *startBB) {
      if (bbOps.count(&inst)) {
        tensorStartPoint = &inst;
        break;
      }
    }
  }

  // Find the end point by doing the same check using post dominators.
  bbOps.clear();
  auto endBB = findNCAOfTensorOps(tensorOps, bbOps,
                                  [&] (SILBasicBlock *B1,
                                         SILBasicBlock *B2) -> SILBasicBlock* {
    return tensorCodeBlocks.findNearestCommonPostDominator(B1, B2);
  });

  // Compute the end point by doing a backward scan.
  if (bbOps.empty()) {
    tensorEndPoint = &endBB->front();
  } else {
    for (auto &inst : llvm::reverse(*endBB)) {
      if (bbOps.count(&inst))
        break;
      tensorEndPoint = &inst;
    }
  }

  // Now that we know the region we're extracting from, mark all of the
  // operations as being moved over to the graph, and recursively mark their
  // operands as appropriate.
  for (auto inst : tensorOps) {
    markInstruction(*inst, Marking::Move);

    // If the tensor operation is used by anything after the end point of the
    // region, then this can be modeled either as a result value of the program
    // or as a value sent from the accelerator to the host.  We prefer to model
    // it as an return value, so collect that now.
    for (auto result : inst->getResults())
      for (auto *operand : result->getUses()) {
        auto user = operand->getUser();

        // If the user is to be deleted, then we can safely ignore it.
        auto it = markedInstructions.find(user);
        if (it != markedInstructions.end() && it->second == Marking::Delete)
          continue;

        // If the end point dominates the out-of-model use, then we can
        // represent it with the return value of the tensor program.  Otherwise
        // it will turn into a send of data back to the host.
        if (DI.dominates(tensorEndPoint, user)) {
          resultValues.push_back(result);
          break;
        }
      }
  }
}




//===----------------------------------------------------------------------===//
//                              PartitionCloner
//===----------------------------------------------------------------------===//

namespace {
class PartitionCloner : public SILClonerWithScopes<PartitionCloner> {
  TFFunctionPartition &FP;

  /// This is a basic block on the newly created function which represents the
  /// exit node of the function.
  SILBasicBlock *exitBB;

  /// This is a counter we use to give each send/receive operation a unique ID.
  unsigned nextSendID = 0;

  /// This is the set of instructions that should be removed from the host code
  /// after the cloning operation is complete.
  SmallVector<SILInstruction*, 8> instructionsToRemove;
public:
  PartitionCloner(TFFunctionPartition &FP, SILFunction &NewFn)
    : SILClonerWithScopes(NewFn), FP(FP) {
  }

  void cloneFunction();
  void finalizeOriginal();

  void insertSend(SILInstruction &inst);
  void insertReceive(SILValue value);

  // Handle references to blocks from cloned code.
  SILBasicBlock *remapBasicBlock(SILBasicBlock *BB) {
    // If the block is included in the partition, directly reference it.
    auto bbIt = BBMap.find(BB);
    if (bbIt != BBMap.end())
      return bbIt->second;

    // Otherwise, it must be a jump to a block that wasn't included in the
    // partition.  Figure out which post-dominated block is included, and jump
    // to it instead.
    auto pdiBlock = BB;
    while (1) {
      // If the post-idom is in the partition, use it.  Otherwise keep scanning.
      pdiBlock = FP.tensorCodeBlocks.getPostIDom(pdiBlock);

      // If we found the exit node, use our exitBB.
      if (!pdiBlock)
        return BBMap[BB] = exitBB;

      auto bbIt = BBMap.find(pdiBlock);
      if (bbIt != BBMap.end())
        return BBMap[BB] = bbIt->second;
    }
  }

  SILType remapType(SILType Ty) {
    // If this is a valid TensorFlow element type "T", like Builtin.Int32, turn
    // it into a TensorCore<T> type.
    if (!isValidTensorFlowElementType(Ty.getSwiftRValueType()))
      return Ty;

    auto tensorType =
      BoundGenericStructType::get(FP.tensorCoreType, /*parent*/Type(),
                                  Ty.getSwiftRValueType());

    return SILType::getPrimitiveObjectType(tensorType->getCanonicalType());
  }


  void visitApplyInst(ApplyInst *inst);
  void visitBuiltinInst(BuiltinInst *inst);
  void visitLiteralInst(LiteralInst *inst);
  void visitIntegerLiteralInst(IntegerLiteralInst *inst) {
    visitLiteralInst(inst);
  }
  void visitFloatLiteralInst(FloatLiteralInst *inst) {
    visitLiteralInst(inst);
  }

  void visitTupleExtractInst(TupleExtractInst *inst);

  void visitBranchInst(BranchInst *inst);
  void visitCondBranchInst(CondBranchInst *inst);

private:
  void initBlock(SILBasicBlock *BB);
  void cloneBlock(SILBasicBlock *BB);
};
} // end anonymous namespace


/// We create each block in an initial pass over the function, before cloning
/// over the instructions.  This allows us to know that there is always a block
/// in our block mapping as we start cloning over branch instructions.
void PartitionCloner::initBlock(SILBasicBlock *BB) {
  auto newBB = Builder.getFunction().createBasicBlock();
  BBMap[BB] = newBB;

  // If the basic block has arguments, map over any marked ones.
  for (auto *arg : BB->getArguments()) {
    if (!FP.markedBBArguments.count(arg))
      continue;

    // Create the argument and copy it into the ValueMap so future references
    // use it.
    ValueMap[arg] = newBB->createPHIArgument(remapType(arg->getType()),
                                             ValueOwnershipKind::Trivial,
                                             arg->getDecl());
  }

  // If this is the entry block for our computation, add the parameter BB
  // arguments.
  if (BB == FP.tensorStartPoint->getParent()) {
    for (auto arg : FP.tensorFnArguments) {
      auto newArg = newBB->createFunctionArgument(arg->getType());
      ValueMap[arg] = SILValue(newArg);
    }
  }
}

// Provide special handling for unconditional branch instructions: we only
// clone over marked bb arguments, not all of them.
void PartitionCloner::visitBranchInst(BranchInst *inst) {
  SmallVector<SILValue, 4> operands;
  operands.reserve(inst->getNumArgs());
  auto destBB = inst->getDestBB();
  unsigned opNum = 0;
  for (auto &arg : inst->getAllOperands()) {
    if (FP.markedBBArguments.count(destBB->getArgument(opNum++)))
      operands.push_back(remapValue(arg.get()));
  }

  getBuilder().setCurrentDebugScope(getOpScope(inst->getDebugScope()));
  auto br = getBuilder().createBranch(getOpLocation(inst->getLoc()),
                                      getOpBasicBlock(inst->getDestBB()),
                                      operands);
  doPostProcess(inst, br);
}

/// For conditional branches, we do exactly what the normal cloner does, except
/// that if we see a branch on a Tensor<Int1>, we unwrap it into an Int1.  We
/// know (by construction) that this only happens when the Tensor is a 0D value.
void PartitionCloner::visitCondBranchInst(CondBranchInst *inst) {
  auto TrueArgs = getOpValueArray<8>(inst->getTrueArgs());
  auto FalseArgs = getOpValueArray<8>(inst->getFalseArgs());
  auto &B = getBuilder();
  B.setCurrentDebugScope(getOpScope(inst->getDebugScope()));

  auto cond = getOpValue(inst->getCondition());

  if (auto eltTy = isTensorCore(cond->getType().getSwiftRValueType())) {
    assert(eltTy->isBuiltinIntegerType(1) && "expected Tensor<i1>");

    auto name = B.getASTContext().getIdentifier("tf_tensor_to_i1");
    cond = B.createBuiltin(getOpLocation(inst->getLoc()), name,
                   SILType::getPrimitiveObjectType(eltTy->getCanonicalType()),
                           /*substitutionlist*/{}, cond);
  }

  doPostProcess(inst,
                B.createCondBranch(getOpLocation(inst->getLoc()),
                                   cond,
                                   getOpBasicBlock(inst->getTrueBB()), TrueArgs,
                                   getOpBasicBlock(inst->getFalseBB()),
                                   FalseArgs, inst->getTrueBBCount(),
                                   inst->getFalseBBCount()));
}


// We know that all apply instructions that get moved over are due to Tensor
// ops.  Transform them into builtin' instructions, which have the benefit of
// eliminating the function_ref instruction.
void PartitionCloner::visitApplyInst(ApplyInst *inst) {
  TensorOpInfo tfopInfo(*inst);
  bool isTensor = tfopInfo.decode();
  assert(isTensor && "Expected a tensor op"); (void)isTensor;

  auto &B = getBuilder();
  auto &ctx = B.getASTContext();
  SmallVector<SILValue, 4> args;
  auto loc = remapLocation(inst->getLoc());

  unsigned nextOperand = 1; // skip the function ref.
  for (auto operandInfo : tfopInfo.operandDescriptors) {
    switch (operandInfo) {
    case OpCommand::Tensor:
      // Tensor operands just become operands.
      args.push_back(remapValue(inst->getOperand(nextOperand++)));
      break;
    case OpCommand::Constant: {
      auto cst = tfopInfo.getTensorConstantOperand(nextOperand++);
      auto ourCst = cst->clone();
      ourCst->setDebugLocation(B.getSILDebugLocation(loc));
      B.getInsertionBB()->push_back(ourCst);
      args.push_back(SILValue(ourCst));
      break;
    }
    case OpCommand::AddDType:
      break;  // This only affects GraphGen.
    }
  }

  auto name = ctx.getIdentifier("__tfop_" + tfopInfo.opName.str() + "__" +
                                tfopInfo.operandDescriptorStr.str() + "__");
  auto result =
    B.createBuiltin(loc, name, inst->getType(), /*substitutionlist*/{}, args);
  ValueMap[inst] = result;
}

// If we're copying over a builtin instruction, it is due to a scalar operation
// that corresponds to an LLVM IR instruction.
void PartitionCloner::visitBuiltinInst(BuiltinInst *inst) {
  // This should correspond to an op.
  auto opName = getPartitionedScalarOpName(inst);
  assert(!opName.empty() && "Should correspond to an op");
  auto &B = getBuilder();
  auto name = B.getASTContext().getIdentifier(opName);

  // Overflow checking integer instructions get special treatment.
  bool isOverflowCheckingInst = isOverflowCheckingIntegerOp(inst);


  // These are all simple things, just remap the operands directly.
  auto operandRange = inst->getAllOperands();

  // Overflow-checking integer ops have a "should check" bit as their last
  // parameter, which we don't remap.
  if (isOverflowCheckingInst)
    operandRange = operandRange.drop_back();

  SmallVector<SILValue, 4> operands;
  for (auto &op : operandRange)
    operands.push_back(remapValue(op.get()));

  // The type of the new builtin is usually the same as the input type, but
  // "remapped", which turns Float into TensorCore<Float>.
  auto resultType = inst->getType();
  if (isOverflowCheckingInst) {
    // In the input program, cverflow checking instructions return something
    // like (Int64, i1).  During the marking process, we've determined that the
    // overflow bit is dead, so we only produce the normal result.
    resultType = resultType.getTupleElementType(0);
  }

  auto result =
    B.createBuiltin(remapLocation(inst->getLoc()), name,
                    remapType(resultType), /*substitutionlist*/{}, operands);
  ValueMap[inst] = result;
}

/// We clone over simple literals like:
///
///    %X = integer_literal $Builtin.Int32, 0
/// into:
///    %Y = integer_literal $Builtin.Int32, 0
///    %X = builtin "tfop_Const"(%Y)
///
void PartitionCloner::visitLiteralInst(LiteralInst *inst) {
  auto loc = remapLocation(inst->getLoc());

  auto opName = getPartitionedScalarOpName(inst);
  assert(!opName.empty() && "Should correspond to an op");
  auto &B = getBuilder();
  auto name = B.getASTContext().getIdentifier(opName);
  auto ourCst = inst->clone();
  ourCst->setDebugLocation(B.getSILDebugLocation(loc));
  B.getInsertionBB()->push_back(ourCst);

  auto result =
    B.createBuiltin(loc, name, remapType(inst->getType()),
                    /*substitutionlist*/{}, SILValue(ourCst));
  ValueMap[inst] = result;
}


/// We clone over tuple_extract(x, 0) into x's value.  The only time
/// tuple_extract instructions get marked is when this is safe.
void PartitionCloner::visitTupleExtractInst(TupleExtractInst *inst) {
  assert(inst->getFieldNo() == 0 && "Unexpected extract to remap");
  ValueMap[inst] = remapValue(inst->getOperand());
}

static SILValue createReceive(SILBuilder &B, SILLocation loc,
                              SILType valueTy, unsigned idNumber) {
  auto &ctx = B.getASTContext();

  auto name = ctx.getIdentifier("tensorflowReceive_"+ llvm::utostr(idNumber));
  return // tensorflowReceive has type <T> () -> T
    B.createBuiltin(loc, name, valueTy,
                    Substitution(valueTy.getSwiftRValueType(), {}), {});
}

static void createSend(SILBuilder &B, SILLocation loc,
                       SILValue value, unsigned idNumber) {
  auto &ctx = B.getASTContext();
  auto voidTy = B.getModule().Types.getEmptyTupleType();
  auto name = ctx.getIdentifier("tensorflowSend_"+ llvm::utostr(idNumber));

  // tensorflowSend has type <T> (T) -> ()
  B.createBuiltin(loc, name, voidTy,
                   Substitution(value->getType().getSwiftRValueType(), {}),
                   {value});
}

/// Insert a send of values from the specified instruction result(s) to the
/// accelerator, and insert receives in it.
void PartitionCloner::insertSend(SILInstruction &inst) {
  assert(!isa<TermInst>(inst) && "Cannot insert after a terminator");

  SILBuilder BH(++SILBasicBlock::iterator(&inst)); // Builder for host.
  auto BA = getBuilder();        // Builder for accelerator.

  for (auto result : inst.getResults()) {
    // Create the receive in the accelerator code.  Each send/receive pair gets
    // a unique ID to associate one with the other.
    this->ValueMap[result] =
      createReceive(BA, inst.getLoc(), remapType(result->getType()),
                    nextSendID);

    // Create the send in the host code.
    createSend(BH, inst.getLoc(), result, nextSendID);
    nextSendID++;
  }
}

void PartitionCloner::insertReceive(SILValue value) {
  assert(isa<SILInstruction>((SILNode*)value) || isa<SILArgument>(value) &&
         "Don't know how to receive this value");

  SILBuilder BH(FP.fn);          // Builder for the host.
  auto BA = getBuilder();        // Builder for accelerator.

  if (auto *inst = dyn_cast<SILInstruction>((SILNode*)value)) {
    assert(!isa<TermInst>(inst) && "Cannot move a terminator");
    BH.setInsertionPoint(++SILBasicBlock::iterator(inst));

    auto otherInst = cast<SILInstruction>((SILNode*)ValueMap[value]);
    BA.setInsertionPoint(++SILBasicBlock::iterator(otherInst));

  } else {
    auto *arg = cast<SILArgument>(value);
    BH.setInsertionPoint(&arg->getParent()->front());

    auto otherBB = remapBasicBlock(arg->getParent());
    BA.setInsertionPoint(&otherBB->front());
  }

  // Diagnose implicit data transfers.
  for (auto *use : value->getUses()) {
    FP.diagnoseCopyOutIfNotReceive(value, use->getUser());
  }

  // Create the send in the accelerator code.  Each send/receive pair gets
  // a unique ID to associate one with the other.
  createSend(BA, value.getLoc(), remapValue(value), nextSendID);

  // Create the receive in the host code.
  auto newVal = createReceive(BH, value.getLoc(), value->getType(),
                              nextSendID);
  value->replaceAllUsesWith(newVal);
  nextSendID++;
}


/// Move and clone code over from the input block into this block, inserting
/// transfers between the host and destination code as necessary.
void PartitionCloner::cloneBlock(SILBasicBlock *BB) {
  if (!FP.markedBlocks.count(BB))
    return;  // Ignore blocks that aren't in accelerator code.
  auto newBB = BBMap[BB];

  Builder.setInsertionPoint(newBB);
  for (auto &inst : *BB) {
    // If the specified instruction is used by the accelerator program somehow,
    // we have to copy the instruction over, copy it, or send the result.
    auto it = FP.markedInstructions.find(&inst);
    if (it == FP.markedInstructions.end()) continue;

    switch (it->second) {
    case Marking::Move:
      instructionsToRemove.push_back(&inst);
      LLVM_FALLTHROUGH;
    case Marking::Copy:
      visit(&inst);
      break;
    case Marking::Send:
      insertSend(inst);
      break;
    case Marking::Argument:
      // Already handled.
      break;
    case Marking::Delete:
      instructionsToRemove.push_back(&inst);
      break;
    }
  }

  // If the terminator of this block wasn't live then it was either an
  // unconditional branch (which never matter for control dependence analysis)
  // or some kind of conditional branch that wasn't important because control
  // eventually flowed to a post dominator that didn't care about the direction
  // of this branch.  In either case, we can provide a terminator by introducing
  // an unconditional branch to the post dominator of the block.
  if (newBB->empty() || !isa<TermInst>(newBB->back())) {
    auto PDI = FP.tensorCodeBlocks.getPostIDom(BB);
    auto destBB = PDI ? remapBasicBlock(PDI) : exitBB;
    Builder.createBranch(BB->getTerminator()->getLoc(), destBB);
  }
}

void PartitionCloner::cloneFunction() {
  // Go through and create all the blocks before we start cloning the
  // instructions over.  This allows us to remap instructions when we clone
  // them over.
  initBlock(FP.tensorStartPoint->getParent());  // First block first.

  for (auto &BB : FP.fn) {
    // If the BB is unmarked, we don't need it for the accelerator.
    if (!FP.markedBlocks.count(&BB) ||
        &BB == FP.tensorStartPoint->getParent())
      continue;

    initBlock(&BB);
  }

  // Create a block for the exit node in case we need it.
  exitBB = Builder.getFunction().createBasicBlock();

  // Now that all the basic blocks and BBArguments are created, we can walk the
  // function in depth first order copying the code over.  Because we're working
  // in depth first order and have BB Arguments resolved, we're guaranteed to
  // see all definitions before uses.
  for (auto *BB : llvm::depth_first(&FP.fn)) {
    cloneBlock(BB);
  }

  // We can end up with to-be-deleted instructions (like debug_value's) outside
  // of the marked region.  These won't be seen in the cloneBlock walk we just
  // did, but we do want to remove them.  Check to see if we have any of these,
  // and arrange for them to be removed.
  for (auto ip : FP.markedInstructions) {
    if (ip.second == Marking::Delete &&
        !FP.markedBlocks.count(ip.first->getParent()))
      instructionsToRemove.push_back(ip.first);
  }

  // Okay at this point we're done except for setting up the exitBB.  Check to
  // see if it is unused.  If so, we nuke it, otherwise we add a return.
  if (exitBB->pred_empty()) {
    exitBB->eraseFromParent();
  } else {
    Builder.setInsertionPoint(exitBB);

    // Create a return of N values, producing a tuple if necessary.
    SILValue result;
    if (FP.resultValues.size() == 1)
      result = remapValue(FP.resultValues[0]);
    else {
      SmallVector<SILValue, 4> results;
      for (auto r : FP.resultValues)
        results.push_back(remapValue(r));

      result = Builder.createTuple(FP.fn.getLocation(), results);
    }

    Builder.createReturn(FP.fn.getLocation(), result);
  }
}


/// Now that all of the interesting instructions are cloned over, we need to
/// clean up the input function by removing the instructions, and inserting
/// sends of results from the accelerator code back to the host code.
///
void PartitionCloner::finalizeOriginal() {
  // Start by dropping interdependent references so we don't get confused by
  // uses that have moved over.
  for (auto i : instructionsToRemove) {
    i->dropAllReferences();
  }

  // For BBArguments, we remove the uses from the branches first (which breaks
  // interdependent references) and delete the actual arguments later.
  for (auto arg : FP.markedBBArguments) {
    // If the argument is copied over, obviously don't zap it.
    if (arg.second != Marking::Move) {
      assert((arg.second == Marking::Copy || arg.second == Marking::Argument) &&
             "Only move/copy/argument supported for arguments right now");
      continue;
    }

    // Remove it from the block that it lives in.
    auto *bb = arg.first->getParent();
    auto argIndex = arg.first->getIndex();

    // Remove the formal values provided by any branches that jump to that
    // block.
    for (auto pi : bb->getPredecessorBlocks()) {
      auto *br = cast<BranchInst>(pi->getTerminator());
      SmallVector<SILValue, 8> operands;
      for (unsigned i = 0, e = br->getNumOperands(); i != e; ++i)
        if (i != argIndex)
          operands.push_back(br->getOperand(i));
      SILBuilder(br).createBranch(br->getLoc(), br->getDestBB(), operands);
      br->eraseFromParent();
    }
  }

  // Ok, now all interdependent references have been dropped.  If there are any
  // uses of values that we moved over to the accelerator, then we must insert
  // a receive from the accelerator of the computed value.  Regardless, we can
  // now delete the defining instruction/argument.
  for (auto argMarking : FP.markedBBArguments) {
    // If this is a copy, we don't need to do anything more.
    if (argMarking.second != Marking::Move)
      continue;
    auto arg = argMarking.first;

    // If the argument has any non-debug-non-retain/release instructions using
    // it, then we need to insert a copy.
    SmallVector<SILInstruction*, 2> instToRemove;
    bool needsCopy = false;
    for (auto operand : arg->getUses()) {
      auto user = operand->getUser();
      if (isa<DebugValueInst>(user) || isa<RefCountingInst>(user)) {
        instToRemove.push_back(user);
        continue;
      }
      // Also handle retain(struct_extract(tensorcore)), which shouldn't
      // come up, but it does.
      // TODO: This will not happen when TensorCore is a builtin type, this
      // code can be deleted then.
      if (auto *SE = dyn_cast<StructExtractInst>(user)) {
        if (SE->hasOneUse()) {
          if (auto RCI = dyn_cast<RefCountingInst>(
                                           SE->getSingleUse()->getUser())) {
            instToRemove.push_back(RCI);
            instToRemove.push_back(SE);
            continue;
          }
        }
      }

      needsCopy = true;
      break;
    }

    if (needsCopy)
      // If we leave around a copy we currently leave the retain/release ops
      // as well.
      // FIXME: Figure out a proper ownership story for the tensor values
      // flowing in and out of these runtime calls.
      insertReceive(arg);
    else {
      for (auto *inst : instToRemove)
        inst->eraseFromParent();
    }

    // Remove it from the block that it lives in.
    arg->getParent()->eraseArgument(arg->getIndex());
  }

  // Next, add sends back of values that are used by the host code, and remove
  // the original instructions.
  for (auto i : instructionsToRemove) {
    for (auto result : i->getResults())
      if (!result->use_empty())
        insertReceive(result);

    i->eraseFromParent();
  }

  // The copy-in/out markers should be removed now.  They are noops which serve
  // no purpose now that we have emitted diagnostics.
  for (auto ecm : FP.explicitCopyMarkers) {
    assert(isa<ApplyInst>(ecm) && ecm->getResults().size() == 1 &&
           ecm->getNumOperands() == 2 && "unknown copy in/out instruction");
    auto callee = ecm->getOperand(1);
    ecm->getResults()[0]->replaceAllUsesWith(ecm->getOperand(1));
    ecm->eraseFromParent();

    if (callee->use_empty())  // Remove the function_ref too.
      cast<SingleValueInstruction>(callee)->eraseFromParent();
  }

  // If the host program reaches any of the tensorKillBlocks, it should abort
  // execution of the tensor program.
  for (auto *killBB : FP.tensorKillBlocks) {
    SILBuilder B(&killBB->front());
    B.createBuiltin(FP.fn.getLocation(),
                    FP.fn.getASTContext().getIdentifier("tensorFlow_abort"),
                    FP.fn.getModule().Types.getEmptyTupleType(),
                    /*no substitutions*/{}, /*no arguments*/{});
  }
}


// Our partitioning can leave around lots of unconditional branches between
// blocks that formerly had control edges.  Go through and merge those to make
// later passes simpler.
static void contractUncondBranches(SILFunction *fn) {
  // Iterate carefully to avoid invalidating iterators: we mutate the block list
  // while we walk it.
  for (auto bbi = fn->begin(), e = fn->end(); bbi != e; ) {
    auto *bb = &*bbi;
    ++bbi;  // Increment the iterator in case we do no transformation.

    if (auto succ = bb->getSingleSuccessorBlock()) {
      if (succ != bb && succ->getSinglePredecessorBlock()) {
        if (auto *BI = dyn_cast<BranchInst>(bb->getTerminator())) {
          // If there are any BB arguments in the destination, replace them with
          // the branch operands, since they must dominate the dest block.
          for (unsigned i = 0, e = BI->getArgs().size(); i != e; ++i) {
            assert(succ->getArgument(i) != BI->getArg(i) &&
                   "Cloned code regions are always reachable");
            succ->getArgument(i)->replaceAllUsesWith(BI->getArg(i));
          }

          // Zap BI and move all of the instructions from DestBB into this one.
          BI->eraseFromParent();
          bb->spliceAtEnd(succ);
          succ->eraseFromParent();

          // Revisit this node: we have new successor(s) and may need to
          // contract them as well.  Also, bbi may be invalidated at this point.
          bbi = SILFunction::iterator(bb);
        }
      }
    }
  }
}

/// Run the TensorFlow partitioning pass.  This pass is a very close relative to
/// the standard "Aggressive Dead Code Elimination" (ADCE) optimization which is
/// implemented using post-dominance frontiers and control dependence
/// information, but instead of determining live code, we're determining
/// operations and a subset of the CFG that is profitable and interesting to
/// move to the accelerator.
///
/// This returns null if there is no tensor work to extract, or a function that
/// has been generated if there is.
///
SILFunction *TFFunctionPartition::run() {
  // Mark the tensor operations that we need to move to the accelerator.
  markFunction();

  // If no blocks are marked, then there are no tensor ops in it, so there is
  // nothing to do!
  if (markedBlocks.empty()) return nullptr;

  if (TFDumpIntermediates) {
    llvm::outs() << "---- INPUT FUNCTION " << fn.getName() << " ----------\n";
    fn.print(llvm::outs());
    llvm::outs() << "---- END OF INPUT FUNCTION ----------\n";
  }

  // Create the builtin in the host program that kicks off the tensor program,
  // setting the argument values.
  SILBuilder B(tensorStartPoint);
  B.createBuiltin(fn.getLocation(),
                  fn.getASTContext().getIdentifier("tensorFlow_start"),
                  fn.getModule().Types.getEmptyTupleType(),
                  /*no substitutions*/{}, tensorFnArguments);

  // Create the builtin in the host program that rendezvous with the tensor
  // program and returns the results.
  SILType resultType;
  if (resultValues.size() == 0)
    resultType = fn.getModule().Types.getEmptyTupleType();
  else if (resultValues.size() == 1)
    resultType = resultValues[0]->getType();
  else {
    SmallVector<TupleTypeElt, 4> resultTypes;
    for (auto r : resultValues)
      resultTypes.push_back({r->getType().getSwiftRValueType()});
    auto tuple = TupleType::get(resultTypes, fn.getASTContext());
    resultType = SILType::getPrimitiveObjectType(tuple->getCanonicalType());
  }

  B.setInsertionPoint(tensorEndPoint);
  auto resultInst = B.createBuiltin(fn.getLocation(),
                            fn.getASTContext().getIdentifier("tensorFlow_done"),
                                    resultType, /*no substitutions*/{},
                                    /*no arguments*/{});

  {
    // Go over all of the returned values, replacing uses dominated by the end
    // point with uses of the returned values.
    unsigned resultNumber = 0;
    for (SILValue result : resultValues) {
      SILValue newValue = resultInst;
      if (resultValues.size() != 1) {
        newValue = B.createTupleExtract(resultInst->getLoc(), newValue,
                                        resultNumber, result->getType());
      }

      // Manually walk the use list in a custom way to avoid invalidating the
      // iterator as we potentially change it.
      for (auto UI = result->use_begin(), UE = result->use_end(); UI != UE; ) {
        auto *operand = *UI++;
        auto user = operand->getUser();

        // Users may be either inside (e.g. another tensor op, or a non-tensor
        // op that causes a copy back to the host) or outside the tensor
        // program.  If it is after the tensor op, we can replace the use with
        // the corresponding result value.
        if (DI.dominates(tensorEndPoint, user)) {
          operand->set(newValue);
        }
      }
      ++resultNumber;
    }
  }

  // Calculate the parameter list for the new function.
  SmallVector<SILParameterInfo, 4> params;
  for (auto v : tensorFnArguments)
    params.push_back(SILParameterInfo(v->getType().getSwiftRValueType(),
                                      ParameterConvention::Direct_Unowned));

  SmallVector<SILResultInfo, 4> results;
  for (auto r : resultValues)
    results.push_back(SILResultInfo(r->getType().getSwiftRValueType(),
                                    ResultConvention::Unowned));


  // Create the partitioned function, which never has arguments or result
  // values, since they get sent and received back and forth.
  auto newFnType =
    SILFunctionType::get(/*genericSig*/nullptr, SILFunctionType::ExtInfo(),
                         SILCoroutineKind::None,
                         ParameterConvention::Direct_Owned, params,
                         /*interfaceYields*/{},
                         results, /*interfaceErrorResult*/None,
                         fn.getModule().getASTContext());
  auto newFn =
    fn.getModule().getOrCreateFunction(fn.getLocation(),
                                       fn.getName().str()+".tf_partition",
                                       SILLinkage::Private, newFnType,
                                       /*What's this*/IsBare, IsNotTransparent,
                                       IsNotSerialized);

  {
    PartitionCloner PC(*this, *newFn);
    // Fill in the cloned function body.
    PC.cloneFunction();

    // Clean up the source function, removing the tensor code.
    PC.finalizeOriginal();
  }

  // Our partitioning can leave around lots of unconditional branches between
  // blocks that formerly had control edges.  Go through and merge those to make
  // later passes simpler.
  contractUncondBranches(newFn);

#if 0
  llvm::errs() << "---- INPUT FUNCTION RESULT ----------\n";
  fn.dump();
  llvm::errs() << "---- END OF INPUT FUNCTION RESULT ----------\n";
#endif

  llvm::errs() << "---- PARTITIONING RESULT FUNCTION ----------\n";
  newFn->dump();
  llvm::errs() << "---- END OF PARTITIONING RESULT ----------\n";
  return newFn;
}


//===----------------------------------------------------------------------===//
//                              Top Level Driver
//===----------------------------------------------------------------------===//

/// Find the TensorCore declaration so we can synthesize types.
/// FIXME: This should go away if/when TensorCore becomes a builtin type, or
/// when we have a formal TensorFlow Swift library.
static StructDecl *findTensorCoreDecl(ModuleDecl *tensorFlowModule) {
  // Get the module that the function we're partitioning lives in.

  SmallVector<ValueDecl *, 1> results;
  auto name = tensorFlowModule->getASTContext().getIdentifier("TensorCore");
  tensorFlowModule->lookupValue({ }, name, NLKind::UnqualifiedLookup, results);

  for (auto result : results)
    if (auto SD = dyn_cast<StructDecl>(result))
      return SD;
  return nullptr;
}

namespace {
class TFPartition : public SILFunctionTransform {
  bool isTest = false;
public:
  TFPartition(bool isTest) : isTest(isTest) {}

  /// The entry point to the transformation.
  void run() override {
    SILFunction *fn = getFunction();
    auto &Ctx = fn->getASTContext();

    // If the TensorFlow module hasn't been imported by the program, don't do
    // anything.  This avoids impacting compile time for non-TensorFlow using
    // Swift programs by doing extraneous analysis.
    auto tfModule = Ctx.getLoadedModule(Ctx.getIdentifier("TensorFlow"));

    // Ignore non-public functions.
    if (fn->getLinkage() != SILLinkage::Public)
      return;

    // Make sure that the TensorCore type is findable.
    // TODO: TensorCore will eventually become a builtin type, and this will go
    // away.
    auto tensorCore = findTensorCoreDecl(tfModule);
    if (!tensorCore) {
      llvm::errs() << " *** DIDN'T FIND TensorCore<> TYPE\n";
      return;
    }

    // TODO: When we get into autodiff, we'll probably want to put the autodiff
    // code into a separate SILModule so the rest of the SIL optimizer pipeline
    // isn't affected (we can also just reuse existing function names that way
    // instead of appending .tfcompiler.  For now, we just create the
    // partitioned function in the current module, emit it to a graph, then
    // delete it.

    // Try to partition the specified function.
    auto *partitionedFn = TFFunctionPartition(*fn, tensorCore, PM).run();
    if (!partitionedFn) return;

    // If we got something, we can process the tensor program further.

    // Verify that the generated function is ok.
    partitionedFn->verify();

    // If this is called from sil-opt, we currently just print out the results
    // and quit.  This allows writing regression tests for the tf-partition
    // pass in isolation.  This is pretty unconventional for a SIL pass, but
    // this is an unconventional pass!
    if (isTest || TFDumpIntermediates) {
      llvm::outs() << "--- TFPartition Host Result: " << fn->getName() << "\n";
      fn->print(llvm::outs());
      llvm::outs() << "--- TFPartition Accelerator Result: "
                   << partitionedFn->getName() << "\n";
      partitionedFn->print(llvm::outs());
    }

    if (isTest) {
      // Finally, we're done.  Remove the partitioned function so it doesn't go
      // through the normal compiler flow.
      partitionedFn->getModule().eraseFunction(fn);
      return;
    }


    // TODO: Once the partitioned function is in its own SIL module, we'll set
    // up a PassManager here, and the other TF subsystems will become passes.
    // This will enable us to write testcases with sil-opt.

    // TODO: Implement autodiff.

    // Next translate it to a graph.
    emitTensorFlowGraph(partitionedFn, partitionedFn->getName().str());

    // Finally, we're done.  Remove the partitioned function so it doesn't go
    // through the normal compiler flow.
    partitionedFn->getModule().eraseFunction(partitionedFn);
  }
};

} // end anonymous namespace

SILTransform *swift::createTFPartition() {
  return new TFPartition(/*isTest*/false);
}

/// Create a version of the tf-partition pass that is used by sil-opt for
/// testcases.  TF-Partition is not a normal pass, so we need an unconventional
/// approach here.
SILTransform *swift::createTFPartitionTest() {
  return new TFPartition(/*isTest*/true);
}
