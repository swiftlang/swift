//===--- TFPartition.cpp - Split Tensor ops out of mainline flow ----------===//
//
// This source file is part of the Swift.org open source project
//
// Copyright (c) 2014 - 2017 Apple Inc. and the Swift project authors
// Licensed under Apache License v2.0 with Runtime Library Exception
//
// See https://swift.org/LICENSE.txt for license information
// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors
//
//===----------------------------------------------------------------------===//
//
// This pass splits tensor operations out into seperate functions - one per
// TensorFlow graph that is generated by the TFLowerGraph functionality.
//
//===----------------------------------------------------------------------===//

#define DEBUG_TYPE "tf-partition"
#include "TFCanonicalizeCFG.h"
#include "TFUtilities.h"
#include "swift/AST/DiagnosticsSIL.h"
#include "swift/AST/Expr.h"
#include "swift/AST/GenericEnvironment.h"
#include "swift/AST/GenericSignatureBuilder.h"
#include "swift/AST/Module.h"
#include "swift/Demangling/Demangle.h"
#include "swift/SIL/CFG.h"
#include "swift/SIL/DebugUtils.h"
#include "swift/SIL/GraphFunctionDeviceInfo.h"
#include "swift/SIL/GraphOperationBuilder.h"
#include "swift/SIL/SILArgument.h"
#include "swift/SIL/SILCloner.h"
#include "swift/SIL/SILConstants.h"
#include "swift/SILOptimizer/Analysis/DominanceAnalysis.h"
#include "swift/SILOptimizer/PassManager/Passes.h"
#include "swift/SILOptimizer/PassManager/Transforms.h"
#include "swift/SILOptimizer/Utils/CFG.h"
#include "swift/SILOptimizer/Utils/Local.h"
#include "swift/SILOptimizer/Utils/SILOptFunctionBuilder.h"
#include "llvm/ADT/BitVector.h"
#include "llvm/Support/CommandLine.h"
#ifdef SWIFT_ENABLE_TENSORFLOW
#include "tensorflow/c/c_api.h"
#endif

#undef DEBUG_TYPE
#include "llvm/Support/GenericDomTreeConstruction.h"
#define DEBUG_TYPE "tf-partition"
using namespace swift;
using namespace tf;
using llvm::DenseMap;

/// This is a counter we use to give each send/receive operation a unique ID.
static int nextSendID = 0;

static llvm::cl::opt<bool> TFModuleLevelGraph(
    "tf-module-level-graph", llvm::cl::init(true),
    llvm::cl::desc(
        "When true, generate 1 TF graph per module. Otherwise generate 1 TF "
        "graph per partitionable function, for ease of writing tests and "
        "verifying test outputs. Only set to false in unit tests,"
        "and when the unit tests do not involve function-typed attributes."));

static llvm::cl::opt<bool> TFWarnScalarTransfer(
    "tf-warn-scalar-transfer", llvm::cl::init(false),
    llvm::cl::desc(
        "Emit warnings for sends/receives that transfer values that are "
        "known to be scalar."));

// TODO: Remove this short-term flag once we migrate over all unit tests.
static llvm::cl::opt<bool> TFSendRecvOpaqueHandle(
    "tf-send-recv-opaque-handle", llvm::cl::init(true),
    llvm::cl::desc("When true, variant and resource handles can be sent via "
                   "eager API as tensor handles."));

template <typename... T, typename... U>
static InFlightDiagnostic diagnose(ASTContext &Context, SourceLoc loc,
                                   Diag<T...> diag, U &&... args) {
  return Context.Diags.diagnose(loc, diag, std::forward<U>(args)...);
}

/// Returns true if the partitioning pass should ignore this user.
static bool isUserIgnoredByPartitioning(SILInstruction *inst) {
  auto optMode = inst->getFunction()->getEffectiveOptimizationMode();
  // `debug_value` instructions are used for value inspection during debugging.
  if (auto *DVI = dyn_cast<DebugValueInst>(inst)) {
    // Ignore all debug_value instructions for an optimized build so that no
    // send/receive will be triggered.
    if (optMode != OptimizationMode::NoOptimization)
      return true;
    // Ignore opaque handle because they cannot be sent/received.
    return isOpaqueHandle(DVI->getOperand()->getType());
  }
  // Reference counting instructions are always ignored.
  return isa<RefCountingInst>(inst);
}

/// Given a decl for a struct or class that has a single field (typically
/// because it is known to be a standard library type like Int or Float), return
/// the canonical type of the single member, asserting and aborting if we get
/// something unexpected.
static CanType getSingleElementDeclFieldType(NominalTypeDecl *decl) {
  auto *field = tf::getFieldIfContainsSingleField(decl);
  assert(field && "Struct should have one member");
  return field->getType()->getCanonicalType();
}

/// Classification of instructions that are interesting to the partitioning
/// pass for various reasons.
enum class PartitioningClass {
  Unknown,

  /// This is an apply instruction of the __tf_get_scalar_or_die family.
  /// Its result is a scalar, operand #0 is the callee function_ref, operand #1
  /// is a TensorHandle, and operand #2 is a metatype because these are defined
  /// as methods.
  GetScalarOrDie,

  /// This is an apply instruction of the __tf_hoistable family.  While it may
  /// appear that these functions have side effects, it is well known that they
  /// may be hoisted above the start of the function safely ignoring them.
  Hoistable,

  /// Scalar instructions that check for overflow like "sadd.with.overflow" and
  /// friends.
  OverflowCheckingInst,

  /// This is an explicit send to the accelerator, which is sugared as
  /// '.toAccelerator()'.
  ExplicitToAccel,

  /// This is an explicit receive from the accelerator, which is sugared as
  /// '.toHost()'.
  ExplicitToHost,
};

static PartitioningClass classifyInst(SILInstruction *inst) {
  if (!inst)
    return PartitioningClass::Unknown;

  if (auto *BI = dyn_cast<BuiltinInst>(inst)) {
    switch (BI->getBuiltinInfo().ID) {
    default:
      return PartitioningClass::Unknown;
    case BuiltinValueKind::UAddOver:
    case BuiltinValueKind::SAddOver:
    case BuiltinValueKind::USubOver:
    case BuiltinValueKind::SSubOver:
    case BuiltinValueKind::UMulOver:
    case BuiltinValueKind::SMulOver:
      return PartitioningClass::OverflowCheckingInst;
    }
  }

  // Classify well-known functions defined in the TensorFlow module.
  if (auto *apply = dyn_cast<ApplyInst>(inst)) {
    if (auto fn = apply->getCalleeFunction()) {
      if (fn->getName().startswith("__tf_get_scalar_or_die_"))
        return PartitioningClass::GetScalarOrDie;
      if (fn->getName().startswith("__tf_hoistable_"))
        return PartitioningClass::Hoistable;
      if (fn->getName() == "__tf_to_accel")
        return PartitioningClass::ExplicitToAccel;
      if (fn->getName() == "__tf_to_host")
        return PartitioningClass::ExplicitToHost;
    }
  }

  return PartitioningClass::Unknown;
}

/// Given an overflow-checking integer operation, return true if the overflow
/// result will be unused in the tensor program.  This could be because the
/// overflow result is already dead, because it is extracted but the extract
/// isn't used, or if the result is only used by code that provably won't make
/// it into the tensor program.
static bool canPromoteOverflowCheckingInstToTensorProgram(BuiltinInst *BI) {
  // Annoyingly, these builtins are modeled as returning a tuple, which then
  // has tuple extracts hanging off of it.
  for (auto *use : BI->getUses()) {
    auto *extract = dyn_cast<TupleExtractInst>(use->getUser());
    if (!extract)
      return false;

    // If this is a use of the normal result of the call, then we can ignore
    // it - this can be moved to the tensor program.
    if (extract->getFieldNo() == 0)
      continue;
    assert(extract->getFieldNo() == 1 && "Overflowing ops only have 2 results");

    // Check all uses of the TupleExtract.  If any of them are going to end up
    // in the tensor program, then we cannot move it.
    for (auto *overflowUse : extract->getUses()) {
      auto overflowUser = overflowUse->getUser();
      // CondFail won't get moved over.
      if (isa<CondFailInst>(overflowUser))
        continue;
      return false;
    }
  }
  return true;
}

/// When classifying a builtin operation as a scalar to be promoted to a tensor,
/// this returns the kind of operation it is.
enum class PromotedScalarKind {
  Invalid,
  Literal,
  TensorToScalar,
  Conversion,
  Binary,
  OverflowingBinary,
};

/// If the specified scalar operation can be partitioned and run on the
/// accelerator, return the name of the op to use to implement it.
static std::pair<const char *, PromotedScalarKind>
classifyPromotedScalarOp(SILInstruction *inst) {
  // We can turn integer and FP literals into constant nodes if their type is
  // compatible.
  if (isa<IntegerLiteralInst>(inst) || isa<FloatLiteralInst>(inst)) {
    auto resultTy = inst->getResults()[0]->getType();
    if (isValidTensorFlowElementType(resultTy.getASTType()))
      return {"Const", PromotedScalarKind::Literal};
  }

  auto instClass = classifyInst(inst);

  // __tf_get_scalar_or_die cannot be marked as having no side effects
  // because it takes a +1 value as its argument.  That said, it is safe to
  // hoist and sink it.
  if (instClass == PartitioningClass::GetScalarOrDie)
    return {"tfc.getScalarOrDie", PromotedScalarKind::TensorToScalar};

  auto *builtin = dyn_cast<BuiltinInst>(inst);
  if (!builtin)
    return {nullptr, PromotedScalarKind::Invalid};

  // Verify that the instruction is processing dtypes that are supported by
  // TensorFlow nodes - we don't want to handle SIMD vectors or other exotic
  // types.
  if (builtin->getNumOperands() != 0) {
    auto opTy = builtin->getOperand(0)->getType();
    if (!isValidTensorFlowElementType(opTy.getASTType()))
      return {nullptr, PromotedScalarKind::Invalid};
  }
  // Verify the result is a valid tensorflow type, or a 2-element tuple that
  // starts with one (used by the overflowing ops).
  if (!isValidTensorFlowElementType(builtin->getType().getASTType())) {
    auto *tt = builtin->getType().getASTType()->getAs<TupleType>();
    if (!tt || tt->getNumElements() != 2 ||
        !isValidTensorFlowElementType(tt->getElementType(0)))
      return {nullptr, PromotedScalarKind::Invalid};
  }

  auto binary =
      [&](const char *name) -> std::pair<const char *, PromotedScalarKind> {
    return {name, PromotedScalarKind::Binary};
  };

  auto overflowingBinary =
      [&](const char *name) -> std::pair<const char *, PromotedScalarKind> {
    if (!canPromoteOverflowCheckingInstToTensorProgram(builtin))
      return {nullptr, PromotedScalarKind::Invalid};
    return {name, PromotedScalarKind::OverflowingBinary};
  };

  auto conversion = [&]() -> std::pair<const char *, PromotedScalarKind> {
    return {"Cast", PromotedScalarKind::Conversion};
  };

  switch (builtin->getBuiltinInfo().ID) {
  default:
    return {nullptr, PromotedScalarKind::Invalid};
  // TODO: Unsigned comparisons: ICMP_UGT, ICMP_ULE, ICMP_UGE
  // TODO: FP Comparisons.
  case BuiltinValueKind::ICMP_EQ:
    return binary("Equal");
  case BuiltinValueKind::ICMP_NE:
    return binary("NotEqual");
  case BuiltinValueKind::ICMP_SLT:
    return binary("Less");
  case BuiltinValueKind::ICMP_SGT:
    return binary("Greater");
  case BuiltinValueKind::ICMP_SLE:
    return binary("LessEqual");
  case BuiltinValueKind::ICMP_SGE:
    return binary("GreaterEqual");
  case BuiltinValueKind::Add:
    return binary("Add");
  case BuiltinValueKind::Sub:
    return binary("Sub");
  case BuiltinValueKind::Mul:
    return binary("Mul");
  case BuiltinValueKind::FAdd:
    return binary("Add");
  case BuiltinValueKind::FSub:
    return binary("Sub");
  case BuiltinValueKind::FMul:
    return binary("Mul");
  // TODO: UAddOver, USubOver, UMulOver:
  case BuiltinValueKind::SAddOver:
    return overflowingBinary("Add");
  case BuiltinValueKind::SSubOver:
    return overflowingBinary("Sub");
  case BuiltinValueKind::SMulOver:
    return overflowingBinary("Mul");

  // TODO: Unsigned conversions.
  case BuiltinValueKind::SIToFP:
  case BuiltinValueKind::FPToSI:
  case BuiltinValueKind::Trunc:
  case BuiltinValueKind::TruncOrBitCast:
  case BuiltinValueKind::SExt:
  case BuiltinValueKind::SExtOrBitCast:
    return conversion();
  }
}

//===----------------------------------------------------------------------===//
//                  BlocksReachingTensorCode CFG Subset
//===----------------------------------------------------------------------===//

/// These nodes mirrors a CFG subset of the function being partitioned, which
/// makes it easy to reuse the dominator algorithms in LLVM.  While it seems
/// silly to make a clone of a graph to produce a filtered view on it, getting
/// the right view projecting iterators to work is more complicated than it is
/// worth at this point.
///
/// As such, this is a very simple representation, which can be optimized later
/// if it ever becomes a performance problem (doubtful).
///
namespace {
class BlocksReachingTensorCode;
struct SILBBSubsetNode {
  // We care about the addresses of these nodes, so disable these to avoid
  // accidental copies.
  SILBBSubsetNode() = delete;
  SILBBSubsetNode(const SILBBSubsetNode &) = delete;

public:
  SILBasicBlock *BB;
  BlocksReachingTensorCode *Parent;
  SILBBSubsetNode(SILBasicBlock *BB, BlocksReachingTensorCode *Parent)
      : BB(BB), Parent(Parent) {}

  SILBBSubsetNode(SILBBSubsetNode &&rhs) {
    BB = rhs.BB;
    Parent = rhs.Parent;
  }

  // These are predecessors and successors of the CFG subset.
  typedef std::vector<SILBBSubsetNode *> BBListTy;
  BBListTy Predecessors, Successors;

  // Requirements of the domtree implementation.
  BlocksReachingTensorCode *getParent() const { return Parent; }
  void printAsOperand(llvm::raw_ostream &O, bool printType = true) {
    BB->printAsOperand(O, printType);
  }
};
} // end anonymous namespace

namespace llvm {
template <> struct GraphTraits<SILBBSubsetNode *> {
  using ChildIteratorType = SILBBSubsetNode::BBListTy::iterator;
  using Node = SILBBSubsetNode;
  using NodeRef = SILBBSubsetNode *;

  static ChildIteratorType child_begin(NodeRef N) {
    return N->Successors.begin();
  }
  static ChildIteratorType child_end(NodeRef N) { return N->Successors.end(); }
};

template <> struct GraphTraits<Inverse<SILBBSubsetNode *>> {
  using ChildIteratorType = SILBBSubsetNode::BBListTy::iterator;
  using Node = SILBBSubsetNode;
  using NodeRef = SILBBSubsetNode *;
  static ChildIteratorType child_begin(NodeRef N) {
    return N->Predecessors.begin();
  }
  static ChildIteratorType child_end(NodeRef N) {
    return N->Predecessors.end();
  }
};
} // end namespace llvm

namespace {
/// Represent a subset of a function in a way that we can fulfill the model of
/// the LLVM Graph abstractions, allowing us to get post-dominators for a subset
/// of the nodes and edges in a function.
///
/// A Swift function that contains tensor operations will often have a number
/// of blocks "hanging off it" that represent failure paths: integer overflow
/// checks, precondition failures, etc.  Also, the function may have a
/// significant amount of general computation that happens after the meat of
/// tensor computation happens.
///
/// Unfortunately, these exits blocks in particular will generally pessimize
/// PostDominator information, because the extraneous edges go to blocks that
/// end with unreachable.  This means that we frequently get degenerate blocks
/// where the only post dominator of two related blocks is the exit node for
/// the function, which unifies the blocks that end with 'return' and the
/// blocks that end with 'unreachable'.
///
/// To solve this, we compute the set of blocks that we need to represent in
/// the tensor slice because they can reach tensor computation or a direct use
/// of that computation.  This subset of the function is what we need to
/// generate accurate post dominator info.
///
class BlocksReachingTensorCode {
  friend struct llvm::GraphTraits<BlocksReachingTensorCode *>;
  friend struct llvm::GraphTraits<llvm::Inverse<BlocksReachingTensorCode *>>;

  /// The function this slice is a subset of.
  SILFunction &fn;

  /// These are all of the nodes themselves.
  std::vector<SILBBSubsetNode> nodes;

  /// This map contains all of the SILBBSubsetNode's that make up the subset
  /// graph.
  DenseMap<SILBasicBlock *, SILBBSubsetNode *> nodeMap;

  /// This is the post dominator tree built over our node subset.
  llvm::DominatorTreeBase<SILBBSubsetNode, true> PDI;

public:
  BlocksReachingTensorCode(SILFunction &fn) : fn(fn) {}
  void compute(ArrayRef<SILInstruction *> ops);

  SILBBSubsetNode *getNode(SILBasicBlock *BB) const {
    auto i = nodeMap.find(BB);
    assert(i != nodeMap.end() && i->second && "BasicBlock not in our subset");
    return i->second;
  }

  SILBBSubsetNode *getEntryBlock() const { return getNode(fn.getEntryBlock()); }

  /// Return true if the specified block is in our subset of the function.
  bool contains(SILBasicBlock *bb) const { return nodeMap.count(bb); }

  bool postDominates(SILBasicBlock *dominator, SILBasicBlock *dominatee) {
    return PDI.dominates(getNode(dominator), getNode(dominatee));
  }

  bool properlyPostDominates(SILBasicBlock *dominator,
                             SILBasicBlock *dominatee) {
    return PDI.properlyDominates(getNode(dominator), getNode(dominatee));
  }

  SILBasicBlock *findNearestCommonPostDominator(SILBasicBlock *B1,
                                                SILBasicBlock *B2) {
    auto res = PDI.findNearestCommonDominator(getNode(B1), getNode(B2));
    return res ? res->BB : nullptr;
  }

  SILBasicBlock *getPostIDom(SILBasicBlock *BB) {
    auto PDINode = PDI[getNode(BB)]->getIDom();
    return PDINode && PDINode->getBlock() ? PDINode->getBlock()->BB : nullptr;
  }

  void dump();

public:
  // Random stuff used by DominatorTree internals.  Don't use generally.
  SILBBSubsetNode &front() const { return *getEntryBlock(); }
};
} // end anonymous namespace

/// \brief An iterator type that allows iterating over the address of the
/// elements returned via some other iterator.
///
/// \code
///   using iterator = address_iterator<SmallVectorImpl<T>::iterator>;
/// \endcode
template <typename WrappedIteratorT,
          typename T = decltype(&*std::declval<WrappedIteratorT>())>
struct address_iterator
    : llvm::iterator_adaptor_base<
          address_iterator<WrappedIteratorT>, WrappedIteratorT,
          typename std::iterator_traits<WrappedIteratorT>::iterator_category,
          T> {
  address_iterator() = default;
  template <typename U>
  address_iterator(U &&u)
      : address_iterator::iterator_adaptor_base(std::forward<U &&>(u)) {}

  T operator*() const { return &*this->I; }
};

namespace llvm {
template <>
struct GraphTraits<BlocksReachingTensorCode *>
    : public GraphTraits<SILBBSubsetNode *> {
  using GraphType = BlocksReachingTensorCode *;
  using NodeRef = SILBBSubsetNode *;

  static NodeRef getEntryNode(GraphType F) { return F->getEntryBlock(); }

  typedef address_iterator<std::vector<SILBBSubsetNode>::iterator>
      nodes_iterator;
  static nodes_iterator nodes_begin(GraphType F) {
    return nodes_iterator(F->nodes.begin());
  }
  static nodes_iterator nodes_end(GraphType F) {
    return nodes_iterator(F->nodes.end());
  }

  static unsigned size(GraphType F) { return F->nodes.size(); }
};

template <>
struct GraphTraits<Inverse<BlocksReachingTensorCode *>>
    : public GraphTraits<Inverse<swift::SILBasicBlock *>> {
  using GraphType = Inverse<BlocksReachingTensorCode *>;
  using NodeRef = SILBBSubsetNode *;

  typedef address_iterator<std::vector<SILBBSubsetNode>::iterator>
      nodes_iterator;
  static nodes_iterator nodes_begin(GraphType F) {
    return nodes_iterator(F.Graph->nodes.begin());
  }
  static nodes_iterator nodes_end(GraphType F) {
    return nodes_iterator(F.Graph->nodes.end());
  }
  static unsigned size(GraphType F) { return F.Graph->nodes.size(); }
};
} // end namespace llvm

// This template requires explicit instantiation.
template class llvm::DominatorTreeBase<SILBBSubsetNode, true>;

/// Compute the set of blocks that can reach the specified operations, and
/// uses of them.
void BlocksReachingTensorCode::compute(ArrayRef<SILInstruction *> ops) {
  assert(!ops.empty() && "Cannot compute empty CFG subset");
  SmallVector<SILBasicBlock *, 8> worklist;

  // We seed the worklist with the blocks that the operations occur in, along
  // with the blocks containing all uses of the results.  These uses may not
  // themselves be tensor results that we partition: but they may be the send
  // operations that cause copy out or function results.
  for (auto *i : ops) {
    auto instBB = i->getParent();
    worklist.push_back(instBB);

    // Add the blocks that any users live in.
    for (auto result : i->getResults()) {
      for (auto use : result->getUses()) {
        auto user = use->getUser();
        if (user->getParent() != instBB &&
            // Ignore values that don't need to keep the tensor value alive.
            !isUserIgnoredByPartitioning(user))
          worklist.push_back(user->getParent());
      }
    }
  }

  // In addition to blocks with ops in them, we also add any block that has a
  // return, since that will be the head of the post dominator tree, and it is
  // possible that there are no ops in that block.
  for (auto &bb : fn.getBlocks())
    if (isa<ReturnInst>(bb.getTerminator()))
      worklist.push_back(&bb);

  // Make sure the nodes are never reallocated out from under us.
  nodes.reserve(fn.getBlocks().size());

  // Figure out all of the blocks that should be included.
  while (!worklist.empty()) {
    auto *bb = worklist.pop_back_val();

    // If we already visited this block, we're done.  Otherwise insert it,
    // creating the SILBBSubsetNode for this BB.
    auto &entry = nodeMap[bb];
    if (entry != nullptr)
      continue;

    nodes.emplace_back(SILBBSubsetNode(bb, this));
    entry = &nodes.back();

    // Its predecessors can also reach a tensor op.
    worklist.append(bb->pred_begin(), bb->pred_end());
  }

  assert(nodeMap.count(fn.getEntryBlock()) &&
         "Entry block should be reachable from Tensor work");

  // Now that all of the nodes are created, we can wire up the predecessor and
  // successor lists.
  for (auto &node : nodes) {
    for (auto *succ : node.BB->getSuccessorBlocks()) {
      auto it = nodeMap.find(succ);
      if (it != nodeMap.end())
        node.Successors.push_back(it->second);
    }
    for (auto *pred : node.BB->getPredecessorBlocks()) {
      auto it = nodeMap.find(pred);
      if (it != nodeMap.end())
        node.Predecessors.push_back(it->second);
    }
  }

  // Now that we have our CFG subset, compute the post dominator tree from it.
  PDI.recalculate(*this);
}

void BlocksReachingTensorCode::dump() { PDI.print(llvm::errs()); }

//===----------------------------------------------------------------------===//
//                             FunctionPartitioner
//===----------------------------------------------------------------------===//

namespace {
/// Marking values in the host program need to either be moved, copied, or have
/// their results sent over to the accelerator.
enum class Marking {
  Copy,     // This instruction is run on both the host and accelerator.
  Move,     // This instruction is run on the accelerator, not the host.
  Send,     // The value produced by this instruction is copied to accelerator.
  Argument, // The value is passed as an argument to the tensor function.
  Delete,   // This instruction is simply deleted (e.g. debug_value)
};

// Each string should be no more than 6 characters, so that a string like
// "[Delete]\t" can be aligned with "[Arg]\t" or just "\t".
static const char *markingStr[]{
    "Copy", "Move", "Send", "Arg", "Delete",
};

class TFFunctionPartition {
public:
  SILTransform &transform;
  SILFunction &hostFn;
  ModuleDecl &tensorFlowModule; // TensorFlow standard library.
  TFGraphLowering *const graphLowering;
  /// For partitionable functions, map the SIL host function names to the
  /// lowered TF graph artifacts.
  DenseMap<StringRef, std::unique_ptr<LoweredGraphFunction>> &graphFunctions;
  GraphFunctionDeviceInfo deviceInfo; // Device placement info.
  DominanceInfo &DI;
  BlocksReachingTensorCode tensorCodeBlocks;

  /// These are the tensor ops to be executed in the extracted graph function.
  // They are found in the initial scan over the function.
  // Note when a graphOp inst is marked to run out-of-graph
  // (graphOp->getRunOutOfGraph() is true), it will not be included here.
  SmallPtrSet<SILInstruction *, 8> tensorOpsSet;

  /// This keeps track of the set of blocks that are marked as needing to be
  /// partitioned out to the accelerator.  If the block is in this set, then
  /// some instruction in the block has to run on the accelerator.
  SmallPtrSet<SILBasicBlock *, 8> markedBlocks;

  /// This contains a set of all basic blocks that are immediate successors of
  /// TensorOp blocks, but which are outside of the tensor program (typically
  /// these are error edges).  These blocks need to kill the tensor program if
  /// reached.
  SmallPtrSet<SILBasicBlock *, 8> tensorKillBlocks;

  /// As the primary tensor operations are marked, the nearest common ancestor
  /// (NCA) in the dominator tree of the tensor operations is found.  This will
  /// be the entry block of the tensor computation, and marks the point where
  /// the tensor computation is started (and where arguments are passed).
  ///
  /// Among other things, this trims off the front matter that often ends up at
  /// the beginning of functions.
  ///
  /// The instruction pointed to here is either the first Tensor operation (if
  /// there is one which dominates all other ops) or the terminator of the block
  /// that dominates all of the operations.
  SILInstruction *tensorStartPoint = nullptr;

  /// Similar to the start point, this indicates the first instruction after
  /// the last Tensor operation, which is when the computation should be
  /// completed and results are returned.  If there are no tensor ops in the
  /// final block, then this will be the entry instruction.
  SILInstruction *tensorEndPoint = nullptr;

  /// The values passed as arguments to the tensor function.
  SmallVector<SILValue, 4> tensorFnArguments;

  /// The instructions that are to be run on the accelerator.
  DenseMap<SILInstruction *, Marking> markedInstructions;

  /// BB Arguments that are marked as being moved, deleted, copied, or used as
  /// arguments.
  ///
  /// If a marked argument is moved over, it is deleted from the host program.
  /// If the host also uses the argument, then a copy will have to be inserted
  /// back from the accelerator to the host.
  ///
  /// We capture source location information for BB arguments during the marking
  /// phase, because once we start chopping up instructions we can't reliably
  /// get it.
  DenseMap<SILArgument *, std::pair<Marking, SILLocation>> markedBBArguments;

  /// The set of values that must be sent to the accelerator.
  SmallPtrSet<SILValue, 8> valuesToSend;

  /// Set of all of the __tf_to_accel calls that silence copy-in warnings.
  SmallPtrSet<SILInstruction *, 8> explicitCopyMarkers;

  /// Set of source locations where we have issued copy-to-host warnings.
  llvm::DenseSet<SourceRange> copyToHostWarningLocs;
  /// Set of source locations where we have issued copy-to-accelerator warnings.
  llvm::DenseSet<SourceRange> copyToAccelWarningLocs;

  struct PartitionedTensorProgram {
    // Initialize all members to NULL.
    PartitionedTensorProgram() {}

    PartitionedTensorProgram(
        StringLiteralInst *programPlaceholder,
        IntegerLiteralInst *programLengthPlaceholder,
        StringLiteralInst *entryFunctionBaseNamePlaceholder,
        IntegerLiteralInst *helperFunctionCountPlaceholder,
        SILValue theTensorComputation)
        : programPlaceholder(programPlaceholder),
          programLengthPlaceholder(programLengthPlaceholder),
          entryFunctionBaseNamePlaceholder(entryFunctionBaseNamePlaceholder),
          helperFunctionCountPlaceholder(helperFunctionCountPlaceholder),
          theTensorComputation(theTensorComputation) {}

    /// These are placeholder instructions inserted during partitioning to
    /// represent the tensor program itself.  These will be replaced when the
    /// tensor function is lowered to a TF graph.
    StringLiteralInst *programPlaceholder = nullptr;
    IntegerLiteralInst *programLengthPlaceholder = nullptr;
    StringLiteralInst *entryFunctionBaseNamePlaceholder = nullptr;
    IntegerLiteralInst *helperFunctionCountPlaceholder = nullptr;

    /// This is the "TensorFlow.TensorComputation" object returned by the
    /// '_swift_tfc_StartTensorComputation' runtime API entrypoint.  This is
    /// returned as null if the tensorflow module is invalid and no
    /// transformation has been made.
    SILValue theTensorComputation;
  };

  PartitionedTensorProgram tensorProgram;
  // The accelerator function corresponding to `tensorProgram`.
  SILFunction *acceleratorFn = nullptr;

public:
  TFFunctionPartition(SILTransform &transform, SILFunction &Fn,
                      SILPassManager *PM, ModuleDecl &tensorFlowModule,
                      TFGraphLowering *graphLowering,
                      DenseMap<StringRef, std::unique_ptr<LoweredGraphFunction>>
                          &graphFunctions)
      : transform(transform), hostFn(Fn), tensorFlowModule(tensorFlowModule),
        graphLowering(graphLowering), graphFunctions(graphFunctions),
        // TODO: remote this call once partition pass is folded into
        // deabstraction.
        deviceInfo(
            GraphFunctionDeviceInfo::getForFunction(hostFn,
                                                    /*removeConfigInst*/ true)),
        DI(*PM->getAnalysis<DominanceAnalysis>()->get(&Fn)),
        tensorCodeBlocks(Fn) {}

  ~TFFunctionPartition() {
    // Remove the partitioned function so it doesn't go through the normal
    // compiler flow.
    if (acceleratorFn) {
      transform.getPassManager()->notifyWillDeleteFunction(acceleratorFn);
      acceleratorFn->getModule().eraseFunction(acceleratorFn);
    }
  }

  /// Run the marking/analysis phase on this function. Return true on error.
  bool markFunction(bool &hasTensorOps);

  TFGraphLowering *getGraphLoweringTest() { return graphLowering; }

  /// Partition and lower this function to a graph, and add an entry to
  /// `graphFunctions`.  Return true on error.
  ///
  /// Should only be called when markFunction() succeeds.
  bool partitionAndLowerGraph(bool isTest);

  /// For a non-accelerator-only function, complete the host function
  /// rewrite, by "installing" the serialized protobuf bytes into that function.
  void finalizeHostFunction(const std::vector<char> &bytes,
                            StringRef entryFnBaseName);

  void diagnoseCopyToAccelerator(SILValue value, SILInstruction *user,
                                 bool isTensorProgramArgument);
  void diagnoseUsesFromHost(SILValue value, SILLocation loc);
  void diagnoseCopyToHost(SILValue value, SILInstruction *user,
                          SILLocation loc);
  void diagnoseOpaqueHandleCopy(SILValue value, SILInstruction *user);

private:
  // Marking.
  /// Return true if we've encountered an error. In that case the diagnostic
  /// must have been emitted.
  bool markBlock(SILBasicBlock *BB);
  bool markInstruction(SILInstruction &inst, Marking mark);
  bool markArgument(SILArgument *arg, SILInstruction *user);
  bool markValue(SILValue value, SILInstruction *user);

  void sinkValueIntoRegionForPromotion(SILInstruction *&inst);

  void promoteCopyToMove();
  void markTensorBBArgumentsForDeletion();

  /// Partially construct `tensorProgram`, by generating an accelerator function
  /// from partitioning the host function.
  /// Return true on error.
  bool partition(bool isTest);

  /// Lower the accelerator function into a graph, add an entry to
  /// `graphFunctions`. Also delete the accelerator function so that it does not
  /// go through the normal compiler flow. Return true on error.
  ///
  /// Should only be called when partition() succeeds.
  bool lowerGraph(bool isTest);

  /// Rewrite the host program, inserting a call to _TFCStartTensorComputation
  /// at the start point of the tensor function, passing in the tensor program
  /// itself, input tensor arguments etc.
  ///
  /// The `resultValues` list is the set of tensor handle values moved to the
  /// accelerator program that should be returned as program results instead of
  /// being sent back.
  void
  insertTensorComputationStartEndTerminate(ArrayRef<SILValue> resultValues);

  // `oldResult` is a tensor in the original host program which has some use
  // beyond the tensor end point, and `newResult` is its correpsponding tensor
  // in the rewritten host program.
  //
  // This function balances the retain/release count of `newResult` in the
  // rewritten host program, by inserting 0 or more strong_retain's right after
  // the tensor end point.
  void balanceRetainReleaseCount(SILValue oldResult, SILValue newResult,
                                 SILBuilder &B);
};
} // end anonymous namespace

/// Determine the marking strategy (Mark::Copy / Mark::Send) according
/// to the type of TermInst of a predecessor BasicBlock.
/// For general terminators (like br/cond_br), we'd generate code to
/// run on accelerator;
/// for other SIL-specific terminators (like try_apply), we'd keep it
/// running on the host and send its result to accelerator.
//@{
static inline bool shouldMarkCopy(TermInst *predTerm) {
  auto termKind = predTerm->getTermKind();
  return termKind == TermKind::BranchInst ||
         termKind == TermKind::CondBranchInst;
}
static inline bool shouldMarkSend(TermInst *predTerm) {
  // TODO : support SwitchValueInst and CheckedCastValueBranchInst
  // once they are properly tested.
  auto termKind = predTerm->getTermKind();
  return termKind == TermKind::SwitchEnumInst ||
         termKind == TermKind::SwitchEnumAddrInst ||
         termKind == TermKind::TryApplyInst ||
         termKind == TermKind::DynamicMethodBranchInst ||
         termKind == TermKind::CheckedCastBranchInst ||
         termKind == TermKind::CheckedCastAddrBranchInst;
}
//@}

/// Check to see if the specified value being copied into a partition for the
/// accelerator is our designated "send" operation.  If so, we're fine,
/// otherwise emit a warning to tell the programmer that they are doing
/// something that induces an implicit data transfer into their code.
void TFFunctionPartition::diagnoseCopyToAccelerator(
    SILValue value, SILInstruction *user, bool isTensorProgramArgument) {
  // If it isn't the result of a "send" operation, then produce a warning about
  // an implicit copy to the accelerator.
  if (auto *apply = dyn_cast<ApplyInst>(value))
    if (classifyInst(apply) == PartitioningClass::ExplicitToAccel) {
      explicitCopyMarkers.insert(apply);
      return;
    }

  auto &ctx = hostFn.getModule().getASTContext();

  // If scalar transfer warnings are turned off, then don't warn about transfers
  // that are definitely scalars.
  if (!TFWarnScalarTransfer &&
      isValidTensorFlowElementType(value->getType().getASTType()))
    return;

  // If we are running this in the context of an expression run in the REPL or
  // playgrounds, or script mode, then we should never emit a warning: we know
  // we're going to be implicitly copying things in as warnings all the time.
  if (hostFn.getName() == SWIFT_ENTRY_POINT_FUNCTION)
    return;

  // If this is an implicit tensor program argument in a Playground, don't emit
  // a warning.
  // TODO: We need to determine what we're going to do with tensor program
  // arguments in general.  We should have a single consistent approach and not
  // special case playgrounds here.  This only affects @noinline functions
  // though so it isn't particularly important to figure out right now.
  if (isTensorProgramArgument && hostFn.getASTContext().LangOpts.Playground)
    return;

  // Since we're in early development and don't support copies, we always show
  // the using instruction that caused the copy.
  // TODO: Remove this as the stack matures.
  if (!isTensorProgramArgument) {
    if (auto *outs = getTFDumpIntermediateStream()) {
      if (auto *inst = value->getDefiningInstruction())
        *outs << "IMPLICIT COPY TO ACCEL OF: " << *inst;
      *outs << "IMPLICIT COPY TO ACCEL BY: " << *user;
      outs->flush();
    }
  }

  // Try to determine a good source location to report.
  auto loc = getUserSourceLocation(value);

  // Opaque handles can never be sent or passed as tensor program arguments.
  // Type checking must have rejected host functions that are either a)
  // public with private ABI or b) marked @inline(never).
  assert(TFSendRecvOpaqueHandle ||
         (!isOpaqueHandle(value->getType()) &&
          "Opaque handles should never have been on the host"));

  // Try to make a useful description of the value being copied to help
  // disambiguate.
  std::string description = "value";
  Diag<StringRef> diagID = diag::tf_value_implicitly_copied_to_accel;

  if (auto expr = loc.getAsASTNode<Expr>()) {
    expr = expr->getSemanticsProvidingExpr();

    // Look through LoadExpr's, since they are just part of accesses to mutable
    // values.
    if (auto *load = dyn_cast<LoadExpr>(expr))
      expr = load->getSubExpr()->getSemanticsProvidingExpr();

    // If this is a reference to a class/existential property, diagnose it
    // specifically, as dynamic accesses are never deabstracted.
    if (auto *mre = dyn_cast<MemberRefExpr>(expr)) {
      if (mre->getBase()->getType()->isAnyClassReferenceType()) {
        description = "properties in classes";
        diagID = diag::tf_value_implicitly_copied_to_accel_always;
      } else if (mre->getBase()->getType()->isExistentialType()) {
        description = "properties in dynamic protocols";
        diagID = diag::tf_value_implicitly_copied_to_accel_always;
      }
    }

    if (auto *ae = dyn_cast<ApplyExpr>(expr->getSemanticsProvidingExpr())) {
      if (auto dsc = dyn_cast<DotSyntaxCallExpr>(ae->getFn())) {
        // If this is a call to a class/existential method, diagnose it
        // specifically, as dynamic accesses are never deabstracted.
        if (dsc->getBase()->getType()->isAnyClassReferenceType()) {
          description = "class methods";
          diagID = diag::tf_value_implicitly_copied_to_accel_always;
        } else if (dsc->getBase()->getType()->isExistentialType()) {
          description = "dynamic protocol methods";
          diagID = diag::tf_value_implicitly_copied_to_accel_always;
        } else {
          description = "method result";
        }
      } else if (isa<ConstructorRefCallExpr>(ae->getFn())) {
        description = "'" + expr->getType()->getString() + "'";
      }
    }
  } else if (auto decl = loc.getAsASTNode<Decl>()) {
    if (auto pd = dyn_cast<ParamDecl>(decl))
      description = "'" + pd->getName().str().str() + "'";
  }

  // Emit the warning on this value, if that has not been done before.
  if (!copyToAccelWarningLocs.insert(loc.getSourceRange()).second)
    return;

  diagnose(ctx, loc.getSourceLoc(), diagID, description)
      .highlight(loc.getSourceRange());
  auto userLoc = getUserSourceLocation(user);

  if (loc.isNull()) {
    // If there was no source location, then we emitted an unhelpful error at
    // line 0 of a file that doesn't exist.  At least utter the demangled
    // function name so we have some way to track this down.
    auto name = Demangle::demangleSymbolAsString(
        hostFn.getName(),
        Demangle::DemangleOptions::SimplifiedUIDemangleOptions());
    diagnose(ctx, hostFn.getLocation().getSourceLoc(), diag::tf_op_misuse,
             "located in " + name + " aka '" + hostFn.getName().str() + "'");
    return;
  }

  // If the use is on a different line, emit a note showing where it is.
  auto &SM = ctx.SourceMgr;
  if (!userLoc.isNull() &&
      (SM.findBufferContainingLoc(loc.getSourceLoc()) !=
           SM.findBufferContainingLoc(userLoc.getSourceLoc()) ||
       SM.getLineNumber(loc.getSourceLoc()) !=
           SM.getLineNumber(userLoc.getSourceLoc()))) {
    diagnose(ctx, userLoc.getSourceLoc(),
             diag::tf_value_used_here)
        .highlight(userLoc.getSourceRange());
  }
}

/// Check to see if the specified value being copied into a partition for the
/// accelerator is our designated "ToHost" operation.  If so, we're fine,
/// otherwise emit a warning to tell the programmer that they are doing
/// something that induces an implicit data transfer into their code.
void TFFunctionPartition::diagnoseUsesFromHost(SILValue value,
                                               SILLocation loc) {
  for (auto *use : value->getUses()) {
    auto *user = use->getUser();

    // If it is used by a "receive" operation, remember the receive and don't
    // emit a warning.
    if (classifyInst(user) == PartitioningClass::ExplicitToHost) {
      explicitCopyMarkers.insert(user);
      continue;
    }

    // If this is a retain/release or debug instruction, don't emit the warning
    // here.  It won't be very useful.
    if (isUserIgnoredByPartitioning(user))
      continue;

    // If the value is a non-copyable opaque handle, emit an error.
    if (!TFSendRecvOpaqueHandle && isOpaqueHandle(value->getType())) {
      diagnoseOpaqueHandleCopy(value, user);
      continue;
    }

    // If we are running this in the context of an expression run in the REPL or
    // playgrounds, or script mode, then we should never emit a warning: we know
    // we're going to be implicitly copying things in as warnings all the time.
    if (hostFn.getName() == SWIFT_ENTRY_POINT_FUNCTION)
      continue;

    // Only emit one warning per value, even if it has multiple host uses.
    if (copyToHostWarningLocs.insert(loc.getSourceRange()).second)
      diagnoseCopyToHost(value, user, loc);
  }
}

void TFFunctionPartition::diagnoseCopyToHost(SILValue value,
                                             SILInstruction *user,
                                             SILLocation loc) {
  // If scalar transfer warnings are turned off, then don't warn about transfers
  // that are definitely scalars.
  if (!TFWarnScalarTransfer &&
      classifyInst(user) == PartitioningClass::GetScalarOrDie) {
    return;
  }

  // Since we're in early development and don't support copies, we always show
  // the using instruction that caused the copy.
  // TODO: Remove this as the stack matures.
  if (auto *outs = getTFDumpIntermediateStream()) {
    if (auto *inst = value->getDefiningInstruction())
      *outs << "IMPLICIT COPY TO HOST OF: " << *inst;
    *outs << "IMPLICIT COPY TO HOST BY: " << *user;
    outs->flush();
  }

  auto &ctx = hostFn.getModule().getASTContext();

  // Emit the warning.
  diagnose(ctx, loc.getSourceLoc(), diag::tf_value_implicitly_copied_to_host)
      .highlight(loc.getSourceRange());
  auto userLoc = getUserSourceLocation(user);

  if (loc.isNull()) {
    // If there was no source location, then we emitted an unhelpful error at
    // line 0 of a file that doesn't exist.  At least utter the demangled
    // function name so we have some way to track this down.
    auto name = Demangle::demangleSymbolAsString(
        hostFn.getName(),
        Demangle::DemangleOptions::SimplifiedUIDemangleOptions());
    diagnose(ctx, hostFn.getLocation().getSourceLoc(), diag::tf_op_misuse,
             "located in " + name + " aka '" + hostFn.getName().str() + "'");
    return;
  }

  // If the use is at a different position, emit a note showing where it is.
  if (!userLoc.isNull() && loc.getSourceLoc() != userLoc.getSourceLoc()) {
    diagnose(ctx, userLoc.getSourceLoc(),
             diag::tf_value_used_here)
        .highlight(userLoc.getSourceRange());
  }
}

/// Emit an error for invalid send/receive of opaque handles.
void TFFunctionPartition::diagnoseOpaqueHandleCopy(SILValue value,
                                                   SILInstruction *user) {
  assert(!TFSendRecvOpaqueHandle);
  assert(isOpaqueHandle(value->getType()) &&
         "Shouldn't emit an error for opaque handle copy when the value is not "
         "an opaque handle");
  auto &ctx = value->getFunction()->getASTContext();
  diagnose(ctx, getUserSourceLocation(value).getSourceLoc(),
           diag::tfop_value_no_send_receive);
  diagnose(ctx, getUserSourceLocation(user).getSourceLoc(),
           diag::tf_value_used_here);
}

/// Some instruction in the specified block needs to be split out to the
/// accelerator, so we mark it (and its control dependencies) as to-be-moved
/// over.
/// Return true if we've encountered an error. In that case the diagnostic must
/// have been emitted.
bool TFFunctionPartition::markBlock(SILBasicBlock *BB) {
  // "tensorStartPoint" marks the start of the extracted function, so it must
  // dominate all blocks that are to be extracted.
  assert(DI.dominates(tensorStartPoint->getParent(), BB) &&
         "Marking instructions out of the tensor region?");

  // Insert the block into our set - if the block is already there, we have
  // nothing more to do.
  if (!markedBlocks.insert(BB).second)
    return false;

  // Walk predecessors until we find marked blocks or other blocks we are
  // control-dependent on.
  //
  // We only scan the region post-dominated by BB.  In other words, elements in
  // worklist must be post-dominated by BB.
  //
  // Note that though this is bounded, that it isn't a very efficient algorithm
  // since each block marking can walk the entire function's CFG, but it is good
  // enough for now.  It would probably make more sense to walk the pdom tree
  // instead of walking the CFG.
  SmallVector<SILBasicBlock *, 8> worklist;
  worklist.push_back(BB);

  // The addedToList set keeps track of blocks that have been added to the
  // worklist, to ensure we don't process something more than once.
  SmallPtrSet<SILBasicBlock *, 32> addedToList;
  addedToList.insert(BB);

  // Walk up the CFG looking for terminators we are control-dependent on.
  while (!worklist.empty()) {
    // The basic block being processed in this loop iteration.
    auto thisBB = worklist.pop_back_val();
    assert(tensorCodeBlocks.postDominates(BB, thisBB) &&
           "Should only be scanning the region pdom'd by BB");

    // If we found the start of the region we are extracting, then stop there.
    if (thisBB == tensorStartPoint->getParent())
      continue;

    // Check the predecessors of this block.  If any of them have multiple
    // successors, then we may be control-dependent on that conditional.
    for (auto pred : thisBB->getPredecessorBlocks()) {
      if (pred->getSingleSuccessorBlock() == pred) {
        // Edge case: we've entered an infinite loop, so we'll just error
        // out here.
        // FIXME: Consider using a different error code than `tf_op_misuse`.
        diagnose(hostFn.getASTContext(), hostFn.getLocation().getSourceLoc(),
                 diag::tf_op_misuse,
                 "Functions containing infinite loops are not supported by "
                 "TensorFlow yet");
        return true;
      }

      // Count the number of successors of this block which are tensor related,
      // for sanity-check purposes.
      //
      // If we see successors that are not tensor related, we'll remember that
      // so we can insert a kill of the tensor program.
      unsigned numTensorSuccs = 0;
      for (auto succ : pred->getSuccessorBlocks()) {
        if (succ->getSingleSuccessorBlock() == succ) {
          // Edge case: we've entered an infinite loop, so we'll just error
          // out here.
          // FIXME: Consider using a different error code than `tf_op_misuse`.
          diagnose(hostFn.getASTContext(), hostFn.getLocation().getSourceLoc(),
                   diag::tf_op_misuse,
                   "Functions containing infinite loops are not supported by "
                   "TensorFlow yet");
          return true;
        }
        if (tensorCodeBlocks.contains(succ))
          ++numTensorSuccs;
        else {
          assert(succ->getSinglePredecessorBlock() &&
                 "Critical edges should've been split");
          tensorKillBlocks.insert(succ);
        }
      }

      if (numTensorSuccs == 1) {
        // Some sanity checks.
        assert(tensorCodeBlocks.properlyPostDominates(thisBB, pred));
        assert(tensorCodeBlocks.properlyPostDominates(BB, pred));
      }
      // We aren't control-dependent on `pred`, if `pred` is just another block
      // post-dominated by `BB`. This includes the case where `pred` has a
      // single tensor-related successor (us).
      // In those cases, continue walking `pred` if we haven't processed it,
      // since it may be control-dependent on something.
      if (tensorCodeBlocks.properlyPostDominates(BB, pred)) {
        if (addedToList.insert(pred).second)
          worklist.push_back(pred);
        continue;
      }

      // When BB does not properly post-dominate pred, pred is the post
      // dominance frontier for BB, and that means that the terminator in pred
      // controls whether BB is executed, so it must be marked.
      // For example, BB is control-dependent on pred in this CFG:
      //
      //    pred
      //    /  \
      //   ..  thisBB
      //    \   |
      //    ..  BB
      //      \ |
      //       ..
      //
      // There is a special case when `BB` and `pred` are the same block. In
      // that case we sanity-check that `BB` must have multiple successors,
      // since otherwise the {list of nodes post-dominated by `BB` up to
      // `thisBB`, processed via `worklist`} would form a cycle, with no "escape
      // edges" to some exit block (e.g. the block containing tensor end
      // point). That would not be a valid CFG.
      // When `BB` and `pred` are the same block, we know the tensor ops in the
      // block are control-dependent on the term inst of the block (a cond_br),
      // so we need to mark that term inst.
      // Example CFG: bb0 -> bb1 -> {bb2, bb3}; bb2 -> bb1
      // Here bb1 is the loop header, bb2 the loop body.
      // `BB` and `pred` can both be bb1, with `thisBB` being bb2.
      assert(BB != pred || numTensorSuccs > 1);

      auto predTerm = pred->getTerminator();
      if (shouldMarkCopy(predTerm)) {
        if (markInstruction(*predTerm, Marking::Copy))
          return true;
        continue;
      }

      if (shouldMarkSend(predTerm)) {
        // We intend for `predTerm` to still run on host (and thus did not
        // mark its parent block), but send its result to accelerator.
        markedInstructions.insert({predTerm, Marking::Send});
        // Even though we don't need to mark `pred` for accelerator if it has no
        // block which it is control-dependent on, when there is a block that
        // `pred` is control-dependent on, we need to mark that block. This is
        // done by marking `pred` first. Otherwise we can miss marking an
        // encompassing loop where `pred` and `thisBB` are part of the loop
        // body.
        if (markBlock(pred))
          return true;
        continue;
      }

      diagnose(hostFn.getModule().getASTContext(),
               predTerm->getLoc().getSourceLoc(), diag::tf_internal_error,
               "TermInst " + getSILInstructionName(predTerm->getKind()).str() +
                   " not supported for TFFunctionPartition");
    }
  }
  return false;
}

/// When considering whether we should promote a scalar operation to a tensor
/// op in the graph, we have several cases.
namespace {
enum class ScalarPromoteClass {
  NeverPromote,  ///< Do not promote this operation.
  CanPromote,    ///< We can promote this operation, but an argument is also ok.
  ShouldPromote, ///< This is cheaper to run in graph than on the host.
};
} // end anonymous namespace.

/// Determine whether we can promote the specified scalar instruction to a
/// tensor operation in the graph.
static ScalarPromoteClass shouldPromoteToTensorOp(SILInstruction *inst,
                                                  TFFunctionPartition &tffp) {
  if (auto *TE = dyn_cast<TupleExtractInst>(inst)) {
    auto *opInst = TE->getOperand()->getDefiningInstruction();
    if (!opInst)
      return ScalarPromoteClass::NeverPromote;

    // We can handle (tuple_extract x, c) when the operand is a tensor op, since
    // it is just extracting a result from the tensor op.
    if (tffp.tensorOpsSet.count(opInst))
      return ScalarPromoteClass::ShouldPromote;

    // We can handle (tuple_extract x, 0) if x is an overflow-checking integer
    // operation.
    if (TE->getFieldNo() != 0 ||
        classifyInst(opInst) != PartitioningClass::OverflowCheckingInst)
      return ScalarPromoteClass::NeverPromote;

    // We can only handle this tuple_extract if the underlying instruction can
    // be handled.  This can depend on dtype support, whether the overflow
    // flag is used, etc.
    return shouldPromoteToTensorOp(opInst, tffp);
  }

  // We can handle (struct_extract x, 0) if x is __tf_get_scalar_or_die.
  if (auto *SE = dyn_cast<StructExtractInst>(inst)) {
    auto *op = SE->getOperand()->getDefiningInstruction();
    if (op && SE->getFieldNo() == 0 &&
        classifyInst(op) == PartitioningClass::GetScalarOrDie)
      return ScalarPromoteClass::ShouldPromote;
  }

  // Check to see if we know how to promote this to a tensor operation.  If not,
  // we reject it.
  auto scalarClass = classifyPromotedScalarOp(inst).second;
  if (scalarClass == PromotedScalarKind::Invalid)
    return ScalarPromoteClass::NeverPromote;

  // Determine which scalar operations make sense to pull into the graph, even
  // if they are defined above the current start point.  We can always sink them
  // down into the tensor region if we want to, but that's pointless if it just
  // puts scalar computation onto the device for no reason.
  //
  // Cases that are handled here should be handled by
  // sinkValueIntoRegionForPromotion.

  // We prefer to put integer and floating point literals in graph because this
  // allows TensorFlow to do constant propagation.  We prefer to put
  // TensorToScalar into the graph because that avoids a pointless conversion
  // which forces the tensor data itself onto the host.
  if (scalarClass == PromotedScalarKind::Literal ||
      scalarClass == PromotedScalarKind::TensorToScalar)
    return ScalarPromoteClass::ShouldPromote;

  // If this is a conversion from an instruction that should be promoted (like
  // a literal), then try hard to promote the conversion.
  if (scalarClass == PromotedScalarKind::Conversion)
    if (auto *op = inst->getOperand(0)->getDefiningInstruction())
      if (shouldPromoteToTensorOp(op, tffp) ==
          ScalarPromoteClass::ShouldPromote)
        return ScalarPromoteClass::ShouldPromote;

  // Otherwise, we can promote this if desired.
  return ScalarPromoteClass::CanPromote;
}

/// In addition to marking this inst itself for accelerator (Copy or Move), also
/// mark the block, and the operands of this inst. In addition, for
/// Marking::Copy, handle a few special types of scalar promotion.
bool TFFunctionPartition::markInstruction(SILInstruction &inst, Marking mark) {
  assert(mark == Marking::Copy || mark == Marking::Move);

  // Insert the specified instruction into the marked set.  If it is already
  // there then we have nothing more to do.
  if (!markedInstructions.insert({&inst, mark}).second)
    return false;

  // Make sure the instruction's block is marked as being copied over to the
  // tensor program.
  if (markBlock(inst.getParent()))
    return true;

  // If we have an uncond branch with basic block arguments, don't add operands
  // as used values.  Instead, we'll use more careful conditional liveness
  // based on whether the BB args in the successor are live.
  if (isa<BranchInst>(&inst))
    return false;

  // If we are copying the instruction over (for scalar promotion), we just
  // recursively mark the operands in question.
  // TODO: This special case will go away when Tensor ops are not representing
  // attributes as operands.
  if (mark == Marking::Copy) {
    auto operandRange = inst.getAllOperands();

    // Some instructions require special handling during marking.
    switch (classifyInst(&inst)) {
    default:
      break; // No special handling.

    case PartitioningClass::OverflowCheckingInst:
      // Overflow-checking integer ops have a "should check" bit as their last
      // parameter, we don't remap it, so don't mark it.
      assert(operandRange.back().get()->getType().is<BuiltinIntegerType>());
      operandRange = operandRange.drop_back();
      break;

    case PartitioningClass::GetScalarOrDie:
      // The __tf_get_scalar_or_die has a callee and a metatype to ignore.
      operandRange = operandRange.drop_front().drop_back();
      break;
    }

    // Scan the operands to make sure they are available: either by moving the
    // computation over to the accelerator, by copying the value over, or by
    // passing as an argument to the tensor computation.
    for (auto &op : operandRange)
      if (markValue(op.get(), &inst))
        return true;
    return false;
  }

  assert(mark == Marking::Move);
  // If we are moving the instruction over, then we know it is a tensor op, and
  // it gets special attention.

  // Sanity check that we never need to need with multi-result tensors encoded
  // in a tuple.
#ifndef NDEBUG
  for (auto result : inst.getResults())
    for (auto *use : result->getUses()) {
      auto user = use->getUser();
      assert(!isa<TupleExtractInst>(user) &&
             "No result value of a graph_op inst should be a tuple!");
    }
#endif // NDEBUG

  assert(!isa<TupleExtractInst>(inst) &&
         "tuple_extract over a tensor value should not be marked!");

  // Okay, we know that the instruction is a tensor op.  Decode its argument
  // list so we know how to handle the operands.
  auto *graphOp = dyn_cast<GraphOperationInst>(&inst);
  if (!graphOp)
    return false;
  for (unsigned i = 0, e = graphOp->getNumOperands(); i != e; ++i) {
    // Tensor and scalar input operands are recursively marked.
    if (markValue(graphOp->getOperand(i), graphOp))
      return true;
  }
  return false;
}

bool TFFunctionPartition::markArgument(SILArgument *arg, SILInstruction *user) {
  // If we've already marked this argument, there is nothing more to do.
  if (markedBBArguments.count(arg))
    return false;

  // If this BB argument is outside the region dominated by the start point,
  // then we pass its value in as an argument to the tensor function.
  if (!DI.properlyDominates(tensorStartPoint->getParent(), arg->getParent())) {
    markedBBArguments.insert(
        {arg, {Marking::Argument, getUserSourceLocation(arg)}});
    tensorFnArguments.push_back(SILValue(arg));
    diagnoseCopyToAccelerator(arg, user, /*tensorProgramArgument*/ true);
    return false;
  }

  // Ok, since we're marking it, we need to make sure the argument's block is
  // marked as being copied.
  if (markBlock(arg->getParent()))
    return true;

  // If this is a value of TensorFlow value type, then we move it to the
  // accelerator.  If it is also used on the host, it will be copied back.
  if (isTensorFlowValue(arg->getType())) {
    // We cannot move over function arguments, but they should never be in the
    // dominated region anyway.
    assert(!isa<SILFunctionArgument>(arg) &&
           "Cannot move function parameters!");
    markedBBArguments.insert(
        {arg, {Marking::Move, getUserSourceLocation(arg)}});
  } else {
    markedBBArguments.insert(
        {arg, {Marking::Copy, getUserSourceLocation(arg)}});
  }

  // Otherwise, we mark the branches that contribute values to it, then mark
  // their formal BB argument values that correspond to this argument.
  bool hasUncondBr = false;
  for (auto *pred : arg->getParent()->getPredecessorBlocks()) {
    auto predTerm = pred->getTerminator();
    hasUncondBr |= isa<BranchInst>(predTerm);
    markInstruction(*predTerm, Marking::Copy);
  }

  // We handle conditional liveness of "phi node" like arguments which are set
  // by unconditional branches.  Arguments resulting from other terminators are
  // handled eagerly.
  if (hasUncondBr) {
    SmallVector<SILValue, 4> incomingValues;
    arg->getIncomingPhiValues(incomingValues);
    for (auto v : incomingValues)
      if (markValue(v, user))
        return true;
  }
  return false;
}

/// Determine whether we are able to move the specified instruction across
/// arbitrary other instructions.  This is basically "side effect free" in the
/// most liberal sense.
///
/// If `plusZeroTensorOperand` and `*plusZeroTensorOperand` are both non-NULL,
/// when the call returns true, it points to a TensorHandle-typed operand `o`
/// where `inst` takes that operand at +0. In that case, simply sinking `inst`
/// can lead to `operand` being deallocated before `inst`. As such, in order for
/// caller to sink `inst`, caller should generate a pair of retain/release on
/// `o` to wrap `inst`, and sink `inst` along with the release.
static bool canMoveInstruction(SILInstruction *inst,
                               SILValue *plusZeroTensorOperand) {
  if (plusZeroTensorOperand)
    *plusZeroTensorOperand = SILValue();

  // Instructions that SIL knows are always side-effect-free can generally be
  // moved.
  if (inst->getMemoryBehavior() == SILInstruction::MemoryBehavior::None) {
    if (isa<TermInst>(inst))
      return false;
    // Can't hoist allocation and dealloc stacks.
    if (isa<AllocationInst>(inst) || isa<DeallocStackInst>(inst))
      return false;
    return true;
  }

  // Some other instructions have side effects because they take their argument
  // as +1, but we can still move them around.
  if (isa<PartialApplyInst>(inst))
    return true;

  SILValue tensorOperand;
  switch (classifyInst(inst)) {
  case PartitioningClass::GetScalarOrDie:
  case PartitioningClass::ExplicitToAccel:
  case PartitioningClass::ExplicitToHost:
    // For the these functions, we know its first parameter is a TensorHandle,
    // with @guaranteed calling convention.
    tensorOperand = inst->getOperand(1);
    if (plusZeroTensorOperand)
      *plusZeroTensorOperand = tensorOperand;
    LLVM_FALLTHROUGH;
  case PartitioningClass::Hoistable:
    return true;
  default:
    return false;
  }
}

/// The specified instruction is in the region dominated by the start point of
/// the tensor computation and needs to be copied into it.  Try to hoist above
/// the start point, since we prefer arguments to the tensor program rather than
/// send and receives.  This returns true if it successfully hoists the
/// computation or if the value is already above the start point.
static bool hoistValueAboveStartPoint(SILInstruction *inst,
                                      SILInstruction *tensorStartPoint,
                                      DominanceInfo &DI) {
  // If this instruction already dominates the start point, then we're good to
  // go.  Don't move anything.
  if (DI.properlyDominates(inst, tensorStartPoint))
    return true;

  // It doesn't make sense to hoist the start point above itself.
  if (inst == tensorStartPoint)
    return false;

  // In general, we need to check to see if we have a chain of side-effect free
  // instructions whose ultimate inputs dominate the start point.
  if (canMoveInstruction(inst, /*plusZeroTensorOperand*/ nullptr)) {
    // We can hoist one of these instructions if all of their operands are
    // hoistable.
    for (auto &op : inst->getAllOperands()) {
      if (auto *opInst = op.get()->getDefiningInstruction()) {
        if (!hoistValueAboveStartPoint(opInst, tensorStartPoint, DI))
          return false;
      } else if (!DI.properlyDominates(op.get(), tensorStartPoint))
        return false;
    }

    // If all of the operands are hoisted above the start point, then this
    // instruction can be too.
    inst->moveBefore(tensorStartPoint);
    return true;
  }

  // Otherwise, we can't handle this instruction.
  return false;
}

/// The specified instruction is using a value defined in the tensor program.
/// Try to sink it below the end point of the program, since we prefer result
/// values from the tensor program rather than send and receives.  This returns
/// true if it successfully sinks the computation or if the value is already
/// below the end point.
static bool sinkValueAfterEndPoint(SILInstruction *inst,
                                   SILInstruction *&tensorEndPoint,
                                   DominanceInfo &DI) {
  // If this instruction is already dominated by the end point, then we're good
  // to go.  Don't move anything.
  if (DI.dominates(tensorEndPoint, inst))
    return true;

  // In general, we need to check to see if we have a chain of side-effect free
  // instructions whose ultimate results can all be sunk after the endpoint.
  SILValue plusZeroTensorOperand;
  if (canMoveInstruction(inst, &plusZeroTensorOperand)) {
    // Make sure the end point is dominated by any operands.
    //
    // TODO: We could make this more aggressive through various techniques, e.g.
    // hoisting instructions to dominate the start point (which is known to also
    // dominate the end points).
    for (auto &op : inst->getAllOperands())
      if (!DI.properlyDominates(op.get(), tensorEndPoint))
        return false;

    for (auto result : inst->getResults())
      for (auto use : result->getUses())
        if (!sinkValueAfterEndPoint(use->getUser(), tensorEndPoint, DI))
          return false;

    // If all of the uses are sunk after the end point, then this
    // instruction can be too.
    if (plusZeroTensorOperand) {
      SILBuilder B(inst);
      B.createStrongRetain(inst->getLoc(), plusZeroTensorOperand,
                           Atomicity::Atomic);
    }

    // The tensorEndPoint is the first non-tensor instruction in the program.
    // Insert our sunk instruction immediately before it, and this instruction
    // becomes the new end point.
    inst->moveBefore(tensorEndPoint);
    tensorEndPoint = inst;
    if (plusZeroTensorOperand) {
      // Create strong_release right after `inst`.
      SILBuilder B(inst);
      auto *releaseInst = B.createStrongRelease(
          tensorEndPoint->getLoc(), plusZeroTensorOperand, Atomicity::Atomic);
      releaseInst->moveAfter(inst);
    }
    return true;
  }

  // Otherwise, we can't handle this instruction.
  return false;
}

/// The specified instruction is known to dominate the start point for the
/// program, and is known to be promotable to a tensor op.   Try to sink it down
/// to be part of the tensor program and return true if successful.
///
void TFFunctionPartition::sinkValueIntoRegionForPromotion(
    SILInstruction *&inst) {
  // Determine whether the instruction is only used by things after the
  // tensorStartPoint.  If so, we can move it into place.
  bool hasProblematicUsers = false;
  for (auto result : inst->getResults())
    for (auto *use : result->getUses())
      if (!DI.dominates(tensorStartPoint, use->getUser())) {
        hasProblematicUsers = true;
        break;
      }

  // If all the users are after the start point, we can just move it into place.
  if (!hasProblematicUsers) {
    inst->moveBefore(tensorStartPoint);
    tensorStartPoint = inst;
    return;
  }

  // This instruction has multiple users, some of which are ahead of the
  // tensorStartPoint.  Instead of moving it, we clone it.
  auto oldInst = inst;
  inst = inst->clone(tensorStartPoint);
  inst->setDebugLocation(oldInst->getDebugLocation());
  tensorStartPoint = inst;

  // Replace uses of the original instruction with the new one, if they are
  // within the tensor program.
  for (auto result : oldInst->getResults()) {
    for (auto it = result->use_begin(), e = result->use_end(); it != e;) {
      auto *operand = *it++;
      auto user = operand->getUser();

      // If the start point dominates this use, replace it.
      if (DI.dominates(tensorStartPoint, user))
        operand->set(cast<SingleValueInstruction>(inst));
    }
  }
}

/// Indicate that the specified value must be available on the accelerator.
/// This can be done by moving the computation over, or by inserting a data
/// transfer.
bool TFFunctionPartition::markValue(SILValue value, SILInstruction *user) {
  // We can safely ignore SILUndef, since SILCloner will just make another
  // one for us.
  if (isa<SILUndef>(value))
    return false;

  if (auto *arg = dyn_cast<SILArgument>(value))
    return markArgument(arg, user);

  auto *inst = value->getDefiningInstruction();
  assert(inst);
  if (markedInstructions.count(inst))
    return false;

  // If this is a reference to a tensor op that we haven't gotten to yet, just
  // ignore it.  The outer marking loop will find it and mark it.
  if (tensorOpsSet.count(inst))
    return false;

  // Determine whether the instruction is lexically before the tensor program
  // start point, and whether it is something we can promote into the graph.
  bool isBeforeStartPoint = !DI.properlyDominates(tensorStartPoint, inst);
  ScalarPromoteClass promotionClass = shouldPromoteToTensorOp(inst, *this);

  // If this is a scalar operation that we really want to promote to a tensor
  // operation, then try to do so.
  if (promotionClass == ScalarPromoteClass::ShouldPromote) {
    // If the value is defined before the program region, is used by the
    // tensor program, and if it is better to have it in the tensor program than
    // for it to be an argument, sink it into the tensor program and mark it.
    // This is useful for constants.  Consider code like this:
    //
    //   x = <opaque value>
    //   y = tensor_literal 1
    //   z = x+y
    //
    // In this case, the operation "+" will be the start point, but we'd like to
    // sink the constant '1' into the region (it will actually become the new
    // start point).
    if (isBeforeStartPoint)
      sinkValueIntoRegionForPromotion(inst);

    // If the instruction is in the tensor program region (either because it was
    // already or because we just moved it) then we can mark it to be copied in.
    return markInstruction(*inst, Marking::Copy);
  }

  // If the value is defined outside of the region dominated by the tensor
  // operations (or it can be hoisted above it), then it is passed in as an
  // argument to the tensor function.
  if (isBeforeStartPoint ||
      // If we can hoist it above the start point then it can be an argument.
      hoistValueAboveStartPoint(inst, tensorStartPoint, DI)) {
    // Args to the tensor program cannot be tensor ops.
    assert(!tensorOpsSet.count(inst));
    markedInstructions.insert({inst, Marking::Argument});
    tensorFnArguments.push_back(value);

    // If the function is not accelerator-only, diagnose implicit copy to
    // the accelerator.
    if (!isAcceleratorOnly(hostFn))
      diagnoseCopyToAccelerator(value, user, /*tensorProgramArgument*/ true);
    return false;
  }

  // If this is a scalar operation that can be promoted to a tensor op on the
  // accelerator, mark it as being copied over (this leads to its operands
  // being recursively copied as well).
  if (promotionClass == ScalarPromoteClass::CanPromote)
    return markInstruction(*inst, Marking::Copy);

  // Otherwise, insert a send from the host to the accelerator.
  valuesToSend.insert(value);
  diagnoseCopyToAccelerator(value, user, /*tensorProgramArgument*/ false);

  // Instead of cloning over this instruction, we'll add a send after it and
  // insert a receive in the accelerator code.
  markedInstructions.insert({inst, Marking::Send});
  return markBlock(inst->getParent());
}

/// Given a set of tensor operations, find the nearest common ancestor of those
/// operations in the [post-]dominator-tree of the CFG.  In addition to finding
/// the NCA, this also returns the list of ops that are in that block (if any).
static SILBasicBlock *findNCAOfTensorOps(
    const SmallPtrSetImpl<SILInstruction *> &tensorOps,
    SmallPtrSet<SILInstruction *, 8> &ncaBBOps,
    ArrayRef<SILBasicBlock *> extraBlocks,
    std::function<SILBasicBlock *(SILBasicBlock *, SILBasicBlock *)> findNCA) {
  assert(!tensorOps.empty() && "expect at least one tensor op");

  // Pick an arbitrary starting point.
  auto ncaBlock = (*tensorOps.begin())->getParent();

  // If there are extra blocks to consider, process them first.
  for (auto extraBB : extraBlocks)
    ncaBlock = findNCA(ncaBlock, extraBB);

  for (auto inst : tensorOps) {
    // If this op is in the ncaBlock, just remember it.
    auto instBB = inst->getParent();
    if (instBB == ncaBlock) {
      ncaBBOps.insert(inst);
      continue;
    }

    // Otherwise, it is possible that the startBB already dominates instBB.  If
    // so, the NCA Will not change.
    auto NCA = findNCA(ncaBlock, instBB);
    if (NCA == ncaBlock)
      continue;

    // Otherwise, the instBB dominated startBB, or neither dominated the other
    // one (meaning NCA is some unrelated parent block).  In either case, it
    // becomes our new startBB.
    ncaBlock = NCA;
    ncaBBOps.clear();
    ncaBBOps.insert(inst);
  }

  return ncaBlock;
}

/// Optimize the host code (and avoid copies back to the host in some cases) by
/// changing scalar operations marked as "Copy" into "Move" when all of their
/// users are known to be moved or deleted.
void TFFunctionPartition::promoteCopyToMove() {
  // Keep a worklist of instructions to process, allowing us to mark operands
  // of instructions that are used by other instructions that get moved over.

  // TODO: This should be an optimistic algorithm in general, and should include
  // arguments as well.  This would allow moving over of cyclic references.
  SmallVector<SILInstruction *, 16> worklist;
  for (auto &markInfo : markedInstructions) {
    auto *inst = markInfo.first;
    if (markInfo.second == Marking::Copy)
      worklist.push_back(inst);
  }

  // Iteratively process instructions until we converge.
  while (!worklist.empty()) {
    auto *inst = worklist.pop_back_val();
    auto mii = markedInstructions.find(inst);

    // Ignore terminators or instructions that are not marked as copies.
    if (mii == markedInstructions.end() || mii->second != Marking::Copy ||
        isa<TermInst>(inst))
      continue;

    bool canMove = true;
    for (auto result : inst->getResults())
      for (auto *use : result->getUses()) {
        auto *user = use->getUser();
        // If this user is marked for deletion or to be moved, then it won't
        // require the instruction to be around.
        auto it = markedInstructions.find(user);
        if (it != markedInstructions.end() &&
            (it->second == Marking::Move || it->second == Marking::Delete))
          continue;

        // If this is a debug_value or retain/release instruction that becomes
        // obsolete if we move the value, then keep going.
        if (isUserIgnoredByPartitioning(user))
          continue;

        // Otherwise, there are host instructions that need this value.  Leave
        // the computation on the host.
        canMove = false;
        break;
      }

    // If we can't move this instruction, then ignore it but keep processing
    // worklist entries.
    if (!canMove)
      continue;

    // Okay, we can move this instruction.
    markedInstructions[inst] = Marking::Move;

    // Moving this over means that any operands of the instruction may have
    // become movable.  Add them (back?) to the worklist for further
    // consideration.
    for (auto &op : inst->getAllOperands()) {
      if (auto opInst = op.get()->getDefiningInstruction())
        worklist.push_back(opInst);
    }
  }
}

/// Scan over all of the unmarked tensor basic block arguments in the function,
/// to see if any of them can be removed.  BB arguments can be removed when
/// they are only (possibly transitively) used by retain/release instructions.
/// If this is the case, we want to delete them to avoid spurious copies back
/// to the host.
void TFFunctionPartition::markTensorBBArgumentsForDeletion() {
  // We use an optimistic algorithm where we assume that all tensor BB arguments
  // are deletable until we find evidence otherwise.  When/if we find such
  // evidence, we mark them as live, which propagates this liveness upwards,
  // marking other BB arguments potentially live.
  SmallPtrSet<SILArgument *, 16> potentiallyDeadArguments;
  for (auto *block : llvm::depth_first(&hostFn)) {
    for (auto *arg : block->getArguments()) {
      // Ignore non-tensor arguments.
      if (!isTensorFlowValue(arg->getType()))
        continue;
      // Ignore tensor arguments that are already marked.
      if (markedBBArguments.count(arg))
        continue;

      // If this argument is fed by a terminator other than an unconditional
      // branch then we can't delete it.
      bool hasAnyNonBranchPredecessors = false;
      for (auto pred : block->getPredecessorBlocks()) {
        if (!isa<BranchInst>(pred->getTerminator())) {
          hasAnyNonBranchPredecessors = true;
          break;
        }
      }

      if (hasAnyNonBranchPredecessors)
        continue;

      // Otherwise, this could be dead!
      potentiallyDeadArguments.insert(arg);
    }
  }

  // Early exit in the common case that we didn't find anything.
  if (potentiallyDeadArguments.empty())
    return;

  SmallVector<SILArgument *, 16> worklist(potentiallyDeadArguments.begin(),
                                          potentiallyDeadArguments.end());
  while (!worklist.empty()) {
    auto *arg = worklist.pop_back_val();

    // If we already know that this argument is live, don't revisit it.
    if (!potentiallyDeadArguments.count(arg))
      continue;

    // Check the use list of the specified argument.  If it contains any
    // users other than retain/release instructions or uncond branches feeding
    // other blocks, then it must be live.
    bool hasImportantUser = false;
    for (auto *operand : arg->getUses()) {
      auto user = operand->getUser();
      // debug_value and retain/release instructions can be deleted if the
      // argument is deleted.
      if (isUserIgnoredByPartitioning(user))
        continue;

      // Deleted instructions won't make this value live.
      auto it = markedInstructions.find(user);
      if (it != markedInstructions.end() && it->second == Marking::Delete)
        continue;

      // Unconditional branches mark the value live if the successor argument
      // makes it live.
      if (auto *branch = dyn_cast<BranchInst>(user)) {
        auto opNumber = operand->getOperandNumber();
        auto arg = branch->getDestBB()->getArgument(opNumber);

        if (potentiallyDeadArguments.count(arg))
          continue;
      }
      hasImportantUser = true;
    }

    // If we have no important users, then this argument appears dead for now.
    if (!hasImportantUser)
      continue;

    // Otherwise, this argument became live.  Remove it from our potentially
    // dead list and mark any arguments that feed it as needing to be revisited,
    // since they aren't dead either.
    potentiallyDeadArguments.erase(arg);

    // If any of the values fed into this argument from predecessors were
    // derived from potentially dead arguments, then they are live: revisit them
    // to propagate liveness upwards.
    for (auto pred : arg->getParent()->getPredecessorBlocks()) {
      auto branch = cast<BranchInst>(pred->getTerminator());
      auto argVal = branch->getOperand(arg->getIndex());
      if (auto *argValArg = dyn_cast<SILArgument>(argVal))
        if (potentiallyDeadArguments.count(argValArg))
          worklist.push_back(argValArg);
    }
  }

  // If we have any dead arguments, go ahead and mark them as such now.
  for (auto arg : potentiallyDeadArguments) {
    markedBBArguments.insert(
        {arg, {Marking::Delete, getUserSourceLocation(arg)}});

    // Mark trivial users as being deleted.
    for (auto *operand : arg->getUses()) {
      auto user = operand->getUser();
      // debug_value and retain/release instructions can be deleted if the
      // argument is deleted.
      if (isUserIgnoredByPartitioning(user))
        markedInstructions[user] = Marking::Delete;
    }
  }
}

static const char *markingEnumToStr(Marking m) {
  return markingStr[static_cast<int>(m)];
}

/// Scan the function looking for blocks with tensor operations in them.  As
/// we find them, mark them as "to-be-partitioned", which marks (transitive)
/// data and control dependencies.
/// Return true if we've encountered an error, in which case the diagnostic must
/// have been emitted. Otherwise, return false, set `hasTensorOps` based on
/// whether this function has any tensor ops, and add any SIL functions
/// referenced in the tfops of this function as function-typed attributes to
/// `worklist` and `addedToWorklist`
bool TFFunctionPartition::markFunction(bool &hasTensorOps) {
  bool loggedInput = false;

  // Print out the input to the partitioning pass on the first op we encounter.
  auto logInput = [&]() {
    // If this is the first op we've found, print out the input to the
    // partitioning pass.  This would ideally be hoisted to the top level, but
    // we want to make sure to print this even if we encounter an error (so we
    // can diagnose the errors).
    if (!loggedInput) {
      if (auto *outs = getTFDumpIntermediateStream()) {
        *outs << "---- INPUT FUNCTION " << hostFn.getName() << " ----------\n";
        hostFn.print(*outs);
        *outs << "---- END OF INPUT FUNCTION ----------\n";
        outs->flush();
      }
      loggedInput = true;
    }
  };

  // Split all critical edges for block marking.
  // FIXME: Remove this when deabstraction is fully implemented.
  splitAllCondBrCriticalEdgesWithNonTrivialArgs(hostFn, &DI, /*LI*/ nullptr);

  // We walk the function in depth first order so that we only visit reachable
  // blocks and to slightly improve compile time performance of the 'marking'
  // operation.
  SmallVector<SILInstruction *, 32> tensorOps;
  bool invalidOpFound = false;
  for (auto *BB : llvm::depth_first(&hostFn)) {
    for (auto bbi = BB->begin(), e = BB->end(); bbi != e; ++bbi) {
      auto *inst = &*bbi;

      // Graph operations are tensor ops.
      auto *graphOp = dyn_cast<GraphOperationInst>(inst);
      if (!graphOp)
        continue;
      logInput();

      // If we need to run this op out-of-graph (e.g. because it has a dynamic
      // attribute), do not add it to `tensorOps`.
      if (graphOp->getNoClustering())
        continue;

      tensorOps.push_back(inst);
      tensorOpsSet.insert(inst);

      auto opDevice = getDeviceType(GraphOperationInfo(graphOp));
      deviceInfo.markDeviceUsed(opDevice);
    }
  }
  hasTensorOps = !tensorOps.empty();

  // If there is nothing to do, or the ops in this function are malformed,
  // don't touch this function.
  if (invalidOpFound)
    return true;
  if (!hasTensorOps)
    return false;

  // Compute the blocksReachingTensorCode set.
  tensorCodeBlocks.compute(tensorOps);

  // Next, compute the NCA of all of the core tensor operations as our "start
  // point".  This is where we will start the tensor computation, sending over
  // argument values defined outside the scope of the computation.
  SmallPtrSet<SILInstruction *, 8> bbOps;
  auto startBB = findNCAOfTensorOps(
      tensorOpsSet, bbOps, /*no extra blocks*/ {},
      [&](SILBasicBlock *B1, SILBasicBlock *B2) -> SILBasicBlock * {
        return DI.findNearestCommonDominator(B1, B2);
      });

  // Compute the start point by doing a linear scan of the startBB to find the
  // first (of possibly many) tensor operations.
  tensorStartPoint = startBB->getTerminator();
  if (!bbOps.empty()) {
    for (auto &inst : *startBB) {
      if (bbOps.count(&inst)) {
        tensorStartPoint = &inst;
        break;
      }
    }
  }

  // Now that we know the region we're extracting from, mark all of the
  // operations as being moved over to the graph, and recursively mark their
  // operands as appropriate.
  for (auto inst : tensorOps)
    if (markInstruction(*inst, Marking::Move))
      return true;

  // Optimize the host code (and avoid copies back to the host in some cases) by
  // changing scalar operations marked as "Copy" into "Move" when all of their
  // users are known to be moved or deleted.
  promoteCopyToMove();

  // Scan over all of the unmarked tensor basic block arguments in the function,
  // to see if any of them can be removed.  BB arguments can be removed when
  // they are only (possibly transitively) used by retain/release instructions.
  // If this is the case, we want to delete them to avoid spurious copies back
  // to the host.
  markTensorBBArgumentsForDeletion();

  if (isAcceleratorOnly(hostFn))
    for (auto markInfo : markedInstructions)
      if (markInfo.second == Marking::Copy ||
          markInfo.second == Marking::Send) {
        diagnose(hostFn.getASTContext(),
                 markInfo.first->getLoc().getSourceLoc(),
                 diag::tf_convention_tf_host_code_not_allowed);
        return true;
      }

  // Not that we know all of the instructions we'll be moving over, find the end
  // point by finding the nearest common post dominating ancestor of the marked
  // instructions.
  //
  // As we walk through all markings, if there's any Copy/Send while the
  // function is @convention(tensorflow), we emit an error.
  bbOps.clear();
  SmallPtrSet<SILInstruction *, 16> instrsToCheck;
  SmallVector<SILBasicBlock *, 16> extraBlocksToCheck;

  for (auto markInfo : markedInstructions) {
    // Ignore instructions that will be deleted or are input arguments to the
    // tensor program.
    if (markInfo.second == Marking::Delete ||
        markInfo.second == Marking::Argument)
      continue;
    auto *inst = markInfo.first;
    instrsToCheck.insert(inst);

    // If the marked instruction is a terminator, then make sure that the
    // successor blocks are considered as part of our NCA evaluation as well.
    // This ensures that we have a block to put the end point outside of a loop
    // if the start point is also outside of it.
    if (auto *ti = dyn_cast<TermInst>(inst))
      for (auto &succ : ti->getSuccessors())
        extraBlocksToCheck.push_back(succ.getBB());
  }

  auto endBB = findNCAOfTensorOps(
      instrsToCheck, bbOps, extraBlocksToCheck,
      [&](SILBasicBlock *B1, SILBasicBlock *B2) -> SILBasicBlock * {
        return tensorCodeBlocks.findNearestCommonPostDominator(B1, B2);
      });
  assert(endBB && "Didn't find an end point for the tensor program");

  // Compute the end point by doing a backward scan.
  if (bbOps.empty()) {
    tensorEndPoint = &endBB->front();
  } else {
    for (auto &inst : llvm::reverse(*endBB)) {
      if (bbOps.count(&inst))
        break;
      tensorEndPoint = &inst;
    }
  }
  assert(tensorEndPoint && "Failed to compute an end point");

  // When the function is @convention(tensorflow), host code before the tensor
  // start point and the tensor end point is not allowed.
  //
  // TODO(rxwei): Check code before/after tensor start/end point. This can be
  // done either by setting the start/end point to be the first/last instruction
  // in the function, or by checking it after normal partitioning. Currently the
  // former approach is marking unnecessary copies (including `strong_retain`
  // etc) and needs investigation.
  //
  //     if (isTFConvention) {
  //
  //     }

  if (auto *outs = getTFDumpIntermediateStream()) {
    *outs << "\n---- ANALYSIS STATE FOR FUNCTION " << hostFn.getName()
          << " ----------\n";
    *outs << "Tensor start point: ";
    tensorStartPoint->print(*outs);
    *outs << "Tensor end point: ";
    tensorEndPoint->print(*outs);

    *outs << "SIL with markings:\n";
    for (auto &BB : hostFn.getBlocks()) {
      SILPrintContext Ctx(*outs);
      *outs << "\n" << Ctx.getID(&BB) << ":\n";
      for (auto *arg : BB.getArguments()) {
        if (markedBBArguments.count(arg)) {
          auto it = markedBBArguments.find(arg);
          assert(it != markedBBArguments.end());
          *outs << "[" << markingEnumToStr(it->second.first) << "]";
        }
        *outs << "\t";
        arg->print(*outs);
      }
      for (auto &I : BB) {
        if (markedInstructions.count(&I)) {
          *outs << "[" << markingEnumToStr(markedInstructions[&I]) << "]";
        }
        *outs << "\t";
        I.print(*outs);
      }
    }

    *outs << "---- END OF ANALYSIS STATE FOR FUNCTION ----------\n";
    outs->flush();
  }

  assert(DI.dominates(startBB, endBB));

  return false;
}

//===----------------------------------------------------------------------===//
//                              PartitionCloner
//===----------------------------------------------------------------------===//

namespace {
class PartitionCloner : public SILClonerWithScopes<PartitionCloner> {
  TFFunctionPartition &FP;

  /// This is a basic block on the newly created function which represents the
  /// exit node of the function.
  SILBasicBlock *exitBB;

  /// This is the set of instructions that should be removed from the host code
  /// after the cloning operation is complete.
  SmallVector<SILInstruction *, 8> instructionsToRemove;

  SILValue tensorComputation;
  ModuleDecl &tensorFlowModule;

public:
  PartitionCloner(TFFunctionPartition &FP, SILFunction &NewFn,
                  SILValue tensorComputation, ModuleDecl &tensorFlowModule)
      : SILClonerWithScopes(NewFn), FP(FP),
        tensorComputation(tensorComputation),
        tensorFlowModule(tensorFlowModule) {}

  void cloneFunction(ArrayRef<SILValue> resultValues);
  // On error, returns false.
  bool finalizeOriginal();

  void handleSendRecvForTerminator(TermInst *inst);
  void insertHostToAccelTransfer(SILInstruction &inst);
  // On error, returns false.
  bool insertAccelToHostTransfer(SILValue value, SILLocation loc);
  // On error, returns false.
  bool handleHostReferencesOfMovedValue(SILValue value, SILLocation loc);

  // Handle references to blocks from cloned code.
  SILBasicBlock *remapBasicBlock(SILBasicBlock *BB) {
    // If the block is included in the partition, directly reference it.
    auto bbIt = BBMap.find(BB);
    if (bbIt != BBMap.end())
      return bbIt->second;

    // Otherwise, it must be a jump to a block that wasn't included in the
    // partition.  Figure out which post-dominated block is included, and jump
    // to it instead.
    auto pdiBlock = BB;
    while (1) {
      // If the post-idom is in the partition, use it.  Otherwise keep scanning.
      pdiBlock = FP.tensorCodeBlocks.getPostIDom(pdiBlock);

      // If we found the exit node, use our exitBB.
      if (!pdiBlock)
        return BBMap[BB] = exitBB;

      auto bbIt = BBMap.find(pdiBlock);
      if (bbIt != BBMap.end())
        return BBMap[BB] = bbIt->second;
    }
  }

  SILType remapType(SILType ty) {
    return convertElementTypeToTensorValueType(ty);
  }

  void visitGraphOperationInst(GraphOperationInst *inst);
  void visitScalarInst(SingleValueInstruction *inst);

  void visitScalarOrOpInst(SingleValueInstruction *inst) {
    // Otherwise it is a scalar to promote.
    visitScalarInst(inst);
  }

  void visitBuiltinInst(BuiltinInst *inst) { visitScalarInst(inst); }
  // For scalar promotion.
  void visitApplyInst(ApplyInst *inst) { visitScalarInst(inst); }
  void visitIntegerLiteralInst(IntegerLiteralInst *inst) {
    visitScalarInst(inst);
  }
  void visitFloatLiteralInst(FloatLiteralInst *inst) { visitScalarInst(inst); }

  void visitTupleExtractInst(TupleExtractInst *inst);
  void visitStructExtractInst(StructExtractInst *inst);

  void visitBranchInst(BranchInst *inst);
  void visitCondBranchInst(CondBranchInst *inst);

private:
  // Check to see if the argument was marked in a way that indicates we should
  // copy it over to the tensor program.
  bool shouldCloneArgument(SILArgument *arg) const {
    auto it = FP.markedBBArguments.find(arg);
    if (it == FP.markedBBArguments.end())
      return false;

    switch (it->second.first) {
    case Marking::Copy:
    case Marking::Move:
      return true;
    case Marking::Send:
    case Marking::Argument:
    case Marking::Delete:
      return false;
    }
  }

  void initBlock(SILBasicBlock *BB);
  void cloneBlock(SILBasicBlock *BB);

  // If `type` conforms to TensorSendableReceivable protocol,
  // looks up and returns the SIL function associated with T based on `fnName`.
  // Otherwise emits error diagnostics, and returns NULL.
  SILFunction *lookupSendReceiveFunction(StringRef fnName, SILType type,
                                         SILLocation loc);
};
} // end anonymous namespace

/// We create each block in an initial pass over the function, before cloning
/// over the instructions.  This allows us to know that there is always a block
/// in our block mapping as we start cloning over branch instructions.
void PartitionCloner::initBlock(SILBasicBlock *BB) {
  auto newBB = Builder.getFunction().createBasicBlock();
  BBMap[BB] = newBB;

  // If the basic block has arguments, map over any marked ones.
  for (auto *arg : BB->getArguments()) {
    if (!shouldCloneArgument(arg))
      continue;

    // Create the argument and copy it into the ValueMap so future references
    // use it.
    ValueMap[arg] = newBB->createPhiArgument(
        remapType(arg->getType()), ValueOwnershipKind::Trivial, arg->getDecl());
  }

  // If this is the entry block for our computation, add the parameter BB
  // arguments.
  if (BB == FP.tensorStartPoint->getParent()) {
    for (auto arg : FP.tensorFnArguments) {
      auto argTy = convertElementTypeToTensorValueType(arg->getType());
      auto newArg = newBB->createFunctionArgument(argTy);
      ValueMap[arg] = SILValue(newArg);
    }
  }
}

// Provide special handling for unconditional branch instructions: we only
// clone over marked bb arguments, not all of them.
void PartitionCloner::visitBranchInst(BranchInst *inst) {
  SmallVector<SILValue, 4> operands;
  operands.reserve(inst->getNumArgs());
  auto destBB = inst->getDestBB();
  unsigned opNum = 0;
  for (auto &arg : inst->getAllOperands()) {
    if (shouldCloneArgument(destBB->getArgument(opNum++)))
      operands.push_back(remapValue(arg.get()));
  }

  getBuilder().setCurrentDebugScope(getOpScope(inst->getDebugScope()));
  auto br =
      getBuilder().createBranch(getOpLocation(inst->getLoc()),
                                getOpBasicBlock(inst->getDestBB()), operands);
  recordClonedInstruction(inst, br);
}

/// For conditional branches, we do exactly what the normal cloner does, except
/// that if we see a branch on a Tensor<Int1>/Tensor<Bool>, we unwrap it into
/// an Int1. We know (by construction) that this only happens when the Tensor
/// is a 0D value.
void PartitionCloner::visitCondBranchInst(CondBranchInst *inst) {
  auto TrueArgs = getOpValueArray<8>(inst->getTrueArgs());
  auto FalseArgs = getOpValueArray<8>(inst->getFalseArgs());
  auto &B = getBuilder();
  B.setCurrentDebugScope(getOpScope(inst->getDebugScope()));

  auto cond = getOpValue(inst->getCondition());
  auto condTy = cond->getType().getASTType();

  if (auto eltTy = getTensorHandleElementType(condTy)) {
    // The TensorHandle's element type here could be Bool or Builtin.il.
    // In either case, we eventually get Builtin.il to create GraphOp.
    auto &ctx = B.getASTContext();
    if (eltTy->isBool())
      eltTy = SILType::getBuiltinIntegerType(/*bitWidth*/ 1, ctx).getASTType();

    assert(eltTy->isBuiltinIntegerType(1) && "expected Tensor<i1>");
    cond = getSingleValueResult(createTensorToInt1Inst(
        cond, B, getOpLocation(inst->getLoc()), FP.deviceInfo));
  }

  recordClonedInstruction(inst, B.createCondBranch(
                              getOpLocation(inst->getLoc()), cond,
                              getOpBasicBlock(inst->getTrueBB()), TrueArgs,
                              getOpBasicBlock(inst->getFalseBB()), FalseArgs,
                              inst->getTrueBBCount(), inst->getFalseBBCount()));
}

void PartitionCloner::visitGraphOperationInst(GraphOperationInst *inst) {
  // Handle special case "ops".
  if (inst->getName().is("tfc.scalarToTensor,i")) {
    assert(inst->getNumOperands() == 1 && "invalid tfc.scalarToTensor!");
    // We just lower the result as the input, since the scalar input will have
    // been promoted to a tensor already.  It is possible that the input will
    // have been lowered to something like TensorHandle<Builtin.Int32> and we
    // need a TensorHandle<Int32>. If that is the case, we insert an
    // UncheckedRefCast to get it to the right type.  These are treated as noops
    // by GraphGen.
    //
    // For this special op, we ignore any device placement. Logically, this
    // means this op is always placed on ALL devices.
    auto result = remapValue(inst->getOperand(0));
    auto S2TResult = inst->getResult(0);
    if (!S2TResult->getType().getASTType()->isEqual(
            result->getType().getASTType())) {
      auto &B = getBuilder();
      auto loc = remapLocation(getUserSourceLocation(inst->getDebugLocation()));
      result = B.createUncheckedRefCast(loc, result, S2TResult->getType());
    }

    ValueMap[S2TResult] = result;
    return;
  }

  return SILClonerWithScopes<PartitionCloner>::visitGraphOperationInst(inst);
}

/// Create the attributes of a const tensor inst in accelerator code, where
/// `constVal` and `valTy` respectively specify the value and data type.  Extend
/// `attributes` with the attributes of that const tensor inst, which caller
/// will then use to finish creating the inst.
///
/// For an integer, `constVal` is an APInt. For a float, `constVal` is an
/// APFloat. See convertValuesToTensor() in graph lowering on how the
/// constructed const tensor inst get lowered.
static void createConstTensorAttrsOnAccel(
    SymbolicValue constVal, SILType valTy, ASTContext &ctx,
    GraphOperationBuilder *opBuilder) {
  // Literals take attributes specifying the dtype and value.
  opBuilder->addAttribute(
      {ctx.getIdentifier("dtype$dtype"),
       convertSwiftTypeToConstantTFDataType(valTy.getASTType())});
  opBuilder->addAttribute({ctx.getIdentifier("value$tensor"), constVal});
}

static void
addScalarShapeArrayAttr(ASTContext &ctx, GraphOperationBuilder *opBuilder) {
  // For TPU graph, set a scalar shape array, with attr name "__shapes".
  auto attrName = std::string(TF_SHAPE_ARRAY_ATTR);
  // A shape is an array of ints -- empty array means it's a scalar (0d) shape.
  auto scalarShape = SymbolicValue::getArray(
      {}, ctx.getInt32Decl()->getDeclaredType()->getCanonicalType(),
      ctx.getAllocator());
  opBuilder->addAttribute(
      {ctx.getIdentifier(attrName),
       SymbolicValue::getArray(
           {scalarShape},
           ctx.getTensorShapeDecl()->getDeclaredType()->getCanonicalType(),
           ctx.getAllocator())});
}

/// Given a primitive scalar instruction like a literal or an LLVM IR
/// instruction (represented as a builtin), promote it to a tensor op in the
/// graph function.
void PartitionCloner::visitScalarInst(SingleValueInstruction *inst) {
  auto loc = remapLocation(getUserSourceLocation(inst->getDebugLocation()));

  // Determine which kind of scalar operation this is, since different forms get
  // different sorts of processing applied to them.
  auto opInfo = classifyPromotedScalarOp(inst);
  assert(opInfo.first != nullptr && "Invalid scalar operation to promote");
  auto opKind = opInfo.second;

  // The TensorToScalar operation is promoted by just dropping the operation
  // and using the incoming tensor value.
  if (opKind == PromotedScalarKind::TensorToScalar) {
    ValueMap[inst] = remapValue(inst->getOperand(1));
    return;
  }

  GraphOperationBuilder opBuilder(opInfo.first);

  // Start remapping the operand list.
  auto operandRange = inst->getAllOperands();

  // Overflow-checking integer ops have a "should check" bit as their last
  // parameter, which we drop.
  if (opKind == PromotedScalarKind::OverflowingBinary)
    operandRange = operandRange.drop_back();

  for (auto &op : operandRange)
    opBuilder.addArgument(remapValue(op.get()));

  // The type of the new builtin is usually the same as the input type, but
  // "remapped", which turns Float into TensorHandle<Float>.
  auto resultType = inst->getType();
  if (opKind == PromotedScalarKind::OverflowingBinary) {
    // In the input program, overflow checking instructions return something
    // like (Int64, i1).  During the marking process, we've determined that the
    // overflow bit is dead, so we only produce the normal result.
    resultType = resultType.getTupleElementType(0);
  }

  auto &B = getBuilder();
  auto &ctx = B.getASTContext();

  // Handle opKind-specific issues.
  switch (opKind) {
  case PromotedScalarKind::Invalid:
    assert(0 && "Rejected ealier");
  case PromotedScalarKind::TensorToScalar:
    assert(0 && "Handled above");
  case PromotedScalarKind::Binary:
  case PromotedScalarKind::OverflowingBinary:
    break;
  case PromotedScalarKind::Literal: {
    SymbolicValue constVal;
    unsigned bitWidth = 0;
    if (auto *ILI = dyn_cast<IntegerLiteralInst>(inst)) {
      auto intVal = ILI->getValue();
      constVal = SymbolicValue::getInteger(intVal, ctx.getAllocator());
      bitWidth = intVal.getBitWidth(); // For sanity-check only.
    } else if (auto *FLI = dyn_cast<FloatLiteralInst>(inst)) {
      auto floatVal = FLI->getValue();
      constVal = SymbolicValue::getFloat(floatVal, ctx.getAllocator());
      bitWidth = floatVal.bitcastToAPInt().getBitWidth();
    } else {
      llvm_unreachable("Cannot promoted the above inst to tensor");
    }
#ifndef NDEBUG
    auto dtype = convertSwiftTypeToTF(inst->getType().getASTType());
    assert(dtype);
    auto dtypeSize = TF_DataTypeSize((TF_DataType)dtype);
    // Swift/LLVM uses a 1-bit APInt to represent a bool, but TF_BOOL is 1 byte
    // or more.
    assert((TF_DataType)dtype == TF_BOOL || bitWidth == dtypeSize * 8);
#endif // NDEBUG
    (void)bitWidth;
    createConstTensorAttrsOnAccel(constVal, inst->getType(), ctx, &opBuilder);
    break;
  }
  case PromotedScalarKind::Conversion: {
    // Conversions get an attribute specifying the result dtype, named "DstT".
    opBuilder.addAttribute(
        {ctx.getIdentifier("DstT$dtype"),
         convertSwiftTypeToConstantTFDataType(inst->getType().getASTType())});
    break;
  }
  }
  // To minimize sends/recvs, place promoted scalars on all devices. If they are
  // consumed only on some devices, the promoted scalars on those other devices
  // should get pruned away in the later graph lowering pass.
  FP.deviceInfo.handleDevicePlacement(
      opInfo.first, /*opDevice*/ getDeviceString(DeviceType::ALL),
      B.getModule().getASTContext(), &opBuilder);

  if (FP.deviceInfo.primaryDeviceType == DeviceType::TPU)
    addScalarShapeArrayAttr(ctx, &opBuilder);

  auto *result = opBuilder.build(B, ctx, loc, {remapType(resultType)});

  ValueMap[inst] = getSingleValueResult(result);
}

void PartitionCloner::visitTupleExtractInst(TupleExtractInst *inst) {
  // This case corresponds to PromotedScalarKind::OverflowingBinary above. We
  // clone over tuple_extract(x, 0) into x's value.
  if (classifyInst(inst->getOperand()->getDefiningInstruction()) ==
      PartitioningClass::OverflowCheckingInst) {
    assert(inst->getFieldNo() == 0 && "Unexpected extract to remap");
    ValueMap[inst] = remapValue(inst->getOperand());
    return;
  }

  // Note that tuple_exract from a tuple of TensorFlow values should not be
  // marked and cloned over.
  inst->dump();
  llvm_unreachable("Cannot partition this tuple_extract inst!");
}

/// We clone over struct_extract(x, 0) into x's value.  The only time we mark
/// a struct_extract is when this is safe.
void PartitionCloner::visitStructExtractInst(StructExtractInst *inst) {
  assert(inst->getFieldNo() == 0 && "Unexpected extract to remap");
  ValueMap[inst] = remapValue(inst->getOperand());
}

/// Wrap a value in a simple struct wrapper, these are common in the standard
/// library.
static SILValue wrapInStruct(SILValue v, NominalTypeDecl *decl, SILBuilder &B,
                             SILLocation loc) {
  auto type = decl->getDeclaredInterfaceType()->getCanonicalType();
  auto silType = SILType::getPrimitiveObjectType(type);
  return B.createStruct(loc, silType, v);
}

/// `decl` is an stdlib numeric type represented by a struct wrapping an LLVM
/// builtin type, such as $Bool, $Float and $Int64. An example returned type is
/// $Builtin.Int1, in the case of $Bool as the input.
/// See getStdlibNumericTypeDeclFromBuiltinType() on the reverse type
/// conversion.
static SILType extractBuiltinTypeFromStdlibNumericType(NominalTypeDecl *decl) {
  auto type = getSingleElementDeclFieldType(decl);
  return SILType::getPrimitiveObjectType(type);
}

/// Create a value of some standard library integer type, as specified by
/// integerDecl.
static SILValue createSomeIntegerValue(intmax_t value, SILBuilder &B,
                                       SILLocation loc,
                                       NominalTypeDecl *integerDecl,
                                       IntegerLiteralInst **ILI = nullptr) {
  auto intFieldSILType = extractBuiltinTypeFromStdlibNumericType(integerDecl);

  auto literal = B.createIntegerLiteral(loc, intFieldSILType, value);

  // If the caller wanted the integer_literal instruction, return it too.
  if (ILI)
    *ILI = literal;

  return wrapInStruct(literal, integerDecl, B, loc);
}

/// Create a value of 'Swift.Int' type holding the specified value.  It is a bit
/// trickier to create than some types because its contents are target platform
/// specific: Builtin.Int32 or Builtin.Int64.
static SILValue createIntValue(intmax_t value, SILBuilder &B, SILLocation loc,
                               IntegerLiteralInst **ILI = nullptr) {
  // Int should have one field, a Builtin.Int32/64.
  auto intDecl = B.getASTContext().getIntDecl();
  return createSomeIntegerValue(value, B, loc, intDecl, ILI);
}

/// Add a RecvFromHost graph_op to the accelerator function, as a placeholder for
/// the subsequent graph lowering pass to generate the appropriate TF graph
/// nodes to receive tensors from Swift host.
/// `isScalar` specifies whether the value being received is a promoted scalar.
///
/// recvFromHost has type <T> (tensorId$int, device$string) -> (T)
//
// TODO: Remove this function after migration to GraphOpInst.
static SILValue createAcceleratorReceive(SILBuilder &B, SILLocation loc,
                                         SILType valueTy, bool isScalar,
                                         unsigned idNumber,
                                         GraphFunctionDeviceInfo &deviceInfo) {
  auto &ctx = B.getASTContext();
  auto opType = "tfc.RecvFromHost";
  GraphOperationBuilder opBuilder(opType);

  opBuilder.addAttribute(
      {ctx.getIdentifier("tensorId"), SymbolicValue::getInteger(idNumber, 32)});
  deviceInfo.handleDevicePlacement(opType, /*opDevice*/ "",
                                   B.getModule().getASTContext(), &opBuilder);

  // For XLA compilation, and when receiving a promoted scalar from the host,
  // add a __shapes pseudo-attr to assist the XLA compiler. This special case is
  // handled within the compiler instead of Swift code (e.g. within TensorFlow
  // stdlib or a test util such as _scalarTensorWithShape()), because here
  // compiler is synthesizing a tensor.
  //
  // TODO: Do this also for XLA-based GPU (and CPU) execution. Consider
  // extending `deviceInfo` with a bool indicating if we are targeting XLA.
  if (deviceInfo.primaryDeviceType == DeviceType::TPU && isScalar)
    addScalarShapeArrayAttr(ctx, &opBuilder);
  return getSingleValueResult(opBuilder.build(B, ctx, loc, {valueTy}));
}

static void
verifyTensorComputationArgAsGuaranteedParam(FunctionRefInst *fnRef) {
  const auto &conventions = fnRef->getConventions();
  auto argConvention = conventions.getSILArgumentConvention(0);
  assert(argConvention == SILArgumentConvention::Direct_Guaranteed);
}

// Create a runtime call for the host program to receive a tensor from device
// based on `tensorComputation` and a tensor ID given by `idNumber`.
static SILValue createHostReceive(SILBuilder &B, SILLocation loc,
                                  SILType valueTy, SILValue tensorComputation,
                                  int idNumber, ModuleDecl &tensorFlowModule,
                                  SILModule &userModule,
                                  SILFunction *receiveFn) {
  assert(receiveFn);
  // The function signature is:
  // public static func receiveFromDevice(_ computation: _TensorComputation,
  //                                      _ tensorId: Int
  // ) -> TensorHandle<Scalar> {...}
  // In SIL:
  // function_ref @... : $@convention(method)
  //   <T where T : _TensorFlowDataTypeCompatible> (@owned _TensorComputation,
  //                                                Int,
  //                                                @thick TensorHandle<T>.Type
  // ) -> @owned TensorHandle<T>

  // Example SIL code to generate:
  // %0 = integer_literal $Builtin.Int64, 0
  // %1 = struct $Int (%0 : $Builtin.Int64)
  // %2 = function_ref @...
  // %3 = metatype $@thick TensorHandle<Float>.Type
  // %4 = apply %2<Float>(..., %1, %3)
  auto tensorId = createIntValue(idNumber, B, loc);

  LLVM_DEBUG(llvm::dbgs() << "Creating host receive with valueTy: " << valueTy
                          << "\n");

  auto scalarType = getTensorHandleElementType(valueTy.getASTType());
  SubstitutionMap subMap;
  if (scalarType) {
    // This is for TensorHandle<Scalar>, so the submap has one entry for Scalar.
    subMap =
        getSingleSubstitutionMapForElementType(scalarType, B.getASTContext());
  }
  // Otherwise, this is for VariantHandle, which uses an empty submap.

  // Generate an instruction like:
  // %3 = metatype $@thick TensorHandle<Float>.Type
  // The type can also be VariantHandle.
  auto tensorflowValueType =
      convertElementTypeToTensorValueType(valueTy).getASTType();
  LLVM_DEBUG(llvm::dbgs() << "The created tensor type is: "
                          << tensorflowValueType << "\n");

  auto metatypeType =
      MetatypeType::get(tensorflowValueType, MetatypeRepresentation::Thick)
          ->getCanonicalType();
  auto *metaTypeInst =
      B.createMetatype(loc, SILType::getPrimitiveObjectType(metatypeType));

  auto receiveFnRef = B.createFunctionRef(loc, receiveFn);
  // Verify that tensorComputation is passed in as a guaranteed parameter below.
  verifyTensorComputationArgAsGuaranteedParam(receiveFnRef);
  return B.createApply(loc, receiveFnRef, subMap,
                       /*args*/ {tensorComputation, tensorId, metaTypeInst},
                       /*isNonThrowing*/ false);
}

/// Add a SendToHost graph_op to the accelerator function, as a placeholder for
/// the subsequent graph lowering pass to generate the appropriate TF graph
/// nodes to send tensors to Swift host.
///
// SendToHost has type <T> (input$T, tensorId$int, device$string) -> ()
void createAcceleratorSend(SILBuilder &B, SILLocation loc, SILValue value,
                           unsigned idNumber,
                           GraphFunctionDeviceInfo &deviceInfo) {
  auto &ctx = B.getASTContext();
  auto voidTy = B.getModule().Types.getEmptyTupleType();
  auto opType = "tfc.SendToHost";
  GraphOperationBuilder opBuilder(opType);
  opBuilder.addArgument({value});
  opBuilder.addAttribute(
      {ctx.getIdentifier("tensorId"), SymbolicValue::getInteger(idNumber, 32)});
  deviceInfo.handleDevicePlacement(opType, /*opDevice*/ "",
                                   B.getModule().getASTContext(), &opBuilder);
  opBuilder.build(B, ctx, loc, {voidTy});
}

template <typename T> static T *castWithDebugInfo(SILInstruction *inst) {
#ifndef NDEBUG
  if (!isa<T>(inst)) {
    inst->dump();
    llvm_unreachable("Unexpected instruction type!");
  }
#endif // NDEBUG
  return cast<T>(inst);
}

/// Given an input type such as $Builtin.Int1, return the type decl of the
/// corresponding stdlib numeric type, such as $Bool.
/// See extractBuiltinTypeFromStdlibNumericType() on the reverse type
/// conversion.
/// This function only supports element data types that are accelerable by
/// tensorflow (e.g. BuiltinFloatType::IEEE80 and Float80 are not).
static NominalTypeDecl *
getStdlibNumericTypeDeclFromBuiltinType(Type ty, ASTContext &ctx) {
  // BuiltinIntegerType doesn't carry sign information, which TensorFlow needs,
  // so we can't rely on getting type information from the builtin types
  // themselves.  For now we'll just use signed types.
  if (auto *BII = ty->getAs<BuiltinIntegerType>()) {
    switch (BII->getFixedWidth()) {
    case 1:
      return ctx.getBoolDecl();
    case 8:
      return ctx.getInt8Decl();
    case 16:
      return ctx.getInt16Decl();
    case 32:
      return ctx.getInt32Decl();
    case 64:
      return ctx.getInt64Decl();
    }
  }

  if (auto *BIF = ty->getAs<BuiltinFloatType>()) {
    switch (BIF->getFPKind()) {
    case BuiltinFloatType::IEEE32:
      return ctx.getFloatDecl();
    case BuiltinFloatType::IEEE64:
      return ctx.getDoubleDecl();
    case BuiltinFloatType::IEEE16:
    case BuiltinFloatType::IEEE80:
    case BuiltinFloatType::IEEE128:
    case BuiltinFloatType::PPC128:
      return nullptr;
    }
  }
  return nullptr;
}

// Create a call to runtime API @_swift_tfc_SendTensorHandle() for the host
// program to receive a tensor from a TF-managed Fifo queue.
//
// `value` can be a scalar type like $Builtin.FPIEEE32, or a TensorHandle type
// like TensorHandle<Float>. In the former case, the scalar is converted to a
// tensor before it's sent to accelerator.
static SILValue createHostSend(SILBuilder &B, SILLocation loc, SILValue value,
                               SILValue tensorComputation, int idNumber,
                               ModuleDecl &tensorFlowModule,
                               SILFunction *createScalarTensorFn,
                               SILFunction *sendFn) {
  assert(sendFn);
  // Function signature:
  // public static func sendToAccelerator(_ computation: _TensorComputation,
  //                                      _ tensorId: Int,
  //                                      _ tensor: TensorHandle<Scalar>
  // ) {...}
  // SIL:
  // function_ref @... : $@convention(method)
  //   <T where T : _TensorFlowDataTypeCompatible> (@owned _TensorComputation,
  //                                                Int,
  //                                                @thick TensorHandle<T>.Type
  // ) -> @owned TensorHandle<T>
  auto tensorId = createIntValue(idNumber, B, loc);

  auto &ctx = B.getASTContext();
  SubstitutionMap subMap;
  if (isOpaqueHandle(value->getType().getASTType())) {
    LLVM_DEBUG(llvm::dbgs()
               << "Sending a variant typed tensor: " << value << "\n");
    // `subMap` should remain empty.
  } else if (isTensorHandle(value->getType().getASTType())) {
    LLVM_DEBUG(llvm::dbgs()
               << "Sending a tensor handle typed tensor: " << value << "\n");

    auto tensorValueTy = value->getType().getASTType();
    auto scalarValueTy = getTensorHandleElementType(tensorValueTy);
    subMap = getSingleSubstitutionMapForElementType(scalarValueTy,
                                                    B.getASTContext());
  } else {
    LLVM_DEBUG(llvm::dbgs() << "Sending a scalar tensor: " << value << "\n");

    assert(createScalarTensorFn);
    // Here scalar type is something like $Builtin.FPIEEE32 -- convert it to a
    // _TensorFlowDataTypeCompatible conforming type like Float first, and then
    // create a scalar tensor to send that value.
    auto scalarValueTy = value->getType().getASTType();
    if (!scalarValueTy->getAs<StructType>()) {
      assert(value->getDefiningInstruction());
      auto *typeDecl =
          getStdlibNumericTypeDeclFromBuiltinType(scalarValueTy, ctx);
      if (!typeDecl) {
        value->dump();
        llvm_unreachable("Unexpected data type when sending a scalar!");
      }
      value = wrapInStruct(value, typeDecl, B, loc);
      scalarValueTy = value->getType().getASTType();
    }
    assert(scalarValueTy->getAs<StructType>());
    assert(isTensorFlowDType(value->getType().getASTType()));
    auto tensorValueTy =
        convertElementTypeToTensorValueType(scalarValueTy, ctx).getASTType();

    // Convert the scalar to a tensor value.
    auto metatypeType =
        MetatypeType::get(tensorValueTy, MetatypeRepresentation::Thick)
            ->getCanonicalType();
    auto *metaTypeInst =
        B.createMetatype(loc, SILType::getPrimitiveObjectType(metatypeType));

    // This is a generic function over the value type, so we have to pass it in
    // by address on the stack.
    // Some code below is adapted from convertScalarToHostTensorHandle(). Given
    // the small amount of code duplication, it is not yet worth unifying.
    auto stackAlloc = B.createAllocStack(loc, value->getType());

    auto storeOwnership = B.getFunction().hasQualifiedOwnership()
                              ? StoreOwnershipQualifier::Trivial
                              : StoreOwnershipQualifier::Unqualified;
    B.createStore(loc, value, stackAlloc, storeOwnership);

    auto access = B.createBeginAccess(loc, stackAlloc, SILAccessKind::Read,
                                      SILAccessEnforcement::Static,
                                      /*noNestedConflict=*/false,
                                      /*fromBuiltin=*/false);

    auto createScalarTensorFnRef =
        B.createFunctionRef(loc, createScalarTensorFn);
    SubstitutionMap scalarSubMap = getSingleSubstitutionMapForElementType(
        scalarValueTy, B.getASTContext());
    value = B.createApply(loc, createScalarTensorFnRef, scalarSubMap,
                          /*args*/ {access, metaTypeInst},
                          /*isNonThrowing*/ false);

    // Finish our read access and free the stack memory.
    B.createEndAccess(loc, access, /*aborted*/ false);
    B.createDeallocStack(loc, stackAlloc);

    subMap = getSingleSubstitutionMapForElementType(scalarValueTy,
                                                    B.getASTContext());
  }

  auto sendFnRef = B.createFunctionRef(loc, sendFn);
  // Verify that tensorComputation is passed in as a guaranteed parameter below.
  verifyTensorComputationArgAsGuaranteedParam(sendFnRef);
  // This is a member method of the class of `value`, so we add it as the last
  // parameter.
  return B.createApply(loc, sendFnRef, subMap,
                       /*args*/ {tensorComputation, tensorId, value},
                       /*isNonThrowing*/ false);
}

SILFunction *PartitionCloner::lookupSendReceiveFunction(StringRef fnName,
                                                        SILType type,
                                                        SILLocation loc) {
  LLVM_DEBUG(llvm::dbgs() << "Looking up send/recv func " << fnName
                          << " with type " << type << "\n");

  auto &ctx = FP.hostFn.getASTContext();
  // If `value` is not receivable, reject the program with diagnostics.
  auto proto = ctx.getProtocol(KnownProtocolKind::TensorSendableReceivable);
  SmallVector<ProtocolConformance *, 1> conformances;
  auto tensorValueTy = convertElementTypeToTensorValueType(type);
  auto nominal = tensorValueTy.getASTType()->getAnyNominal();
  auto lookup =
      nominal->lookupConformance(&tensorFlowModule, proto, conformances);
  if (!lookup) {
    type.dump();
    llvm_unreachable("No conformance to TensorSendableReceivable found.");
  }
  assert(conformances.size() == 1 && "Found multiple conformance candidates.");
  DeclName fnDeclName(ctx.getIdentifier(fnName));
  auto *fn = findSILFunctionForRequiredProtocolMember(
      nominal, proto, fnDeclName, &tensorFlowModule, FP.hostFn.getModule());
  assert(fn);
  return fn;
}

static SILBasicBlock *getLastSuccessorBlock(TermInst *inst) {
  auto lastIt = inst->getSuccessorBlocks().end();
  return *--lastIt;
}

/// Handle the send/recv between host and accelerator for TermInst.
/// For example, the partitioned code for switch_enum is illustrated
/// by the snippet below:
///
/// Host side:
/// bb0:
///   ...
///   swich_enum %X, case ..: bb1, case ..: bb2, .., case ..: bbN
///
/// bb1:
///   sendToAccelerator(0) // 0 is the case id here
///   <other host code>
///   br bb_merge
///
/// <same for bb2 through bbN, sending a different case_id each>
///
/// bb_merge:
///   ...
///
/// Accelerator side:
/// bb0:
///   ...
///   %case_id = recvFromHost()
///   if %case_id == 0 goto bb1
///   else if %case_id == 1 goto bb2
///   else ...
///
void PartitionCloner::handleSendRecvForTerminator(TermInst *inst) {
  auto &ctx = FP.hostFn.getASTContext();
  auto int32Decl = ctx.getInt32Decl();
  auto int32SILType = extractBuiltinTypeFromStdlibNumericType(int32Decl);

  auto hostLoc = getUserSourceLocation(inst);
  auto *createScalarTensorFn =
      lookupSendReceiveFunction("scalar", int32SILType, hostLoc);
  // If `result` were not a send'able data type like String, compiler
  // would have rejected the program before reaching here.
  assert(createScalarTensorFn &&
         "At this point of compilation, scalar value must be send'able "
         "from Swift to TensorFlow.");

  SILFunction *sendFn =
      lookupSendReceiveFunction("sendToAccelerator", int32SILType, hostLoc);
  assert(sendFn && "At this point of compilation, the value must be send'able "
                   "from Swift to TensorFlow.");

  // Create the host->accelerator sends in the host code.
  unsigned caseId = 0;
  for (auto *caseBB : inst->getSuccessorBlocks()) {
    SILBuilder BH(caseBB); // Builder for host.

    // The send is at the beginning of each caseBB.
    BH.setInsertionPoint(&caseBB->front());
    // The first inst of `caseBB` could originate from a return statement of a
    // function that's been inlined here, in which case its LocationKind can be
    // ReturnKind. But SILVerifier requires that we use a RegularKind
    // for the SILLocation of the synthesized host code here (such as
    // `caseIdInst`). This is achieved via getAsRegularLocationWithOverwrite().
    auto loc = caseBB->front().getLoc().getAsRegularLocationWithOverwrite();
    // `caseId` must be of a type conforming to `_TensorFlowDataTypeCompatible`,
    // so we chose Int32 here.
    auto caseIdInst = createSomeIntegerValue(caseId /*caseEntry.index()*/, BH,
                                             loc, int32Decl);

    createHostSend(BH, loc, caseIdInst, tensorComputation, nextSendID,
                   tensorFlowModule, createScalarTensorFn, sendFn);
    ++caseId;
  }

  // In accelerator, first recv the caseId from host, and then dispatch to the
  // right BB. An example SIL snippet is:
  //
  // %caseId = graph_op "tfc.RecvFromHost"() {tensorId: i32 0...
  // %zeroScalarTensor = graph_op "Const"...
  // Both operands have type $TensorHandle<Builtin.Int32>
  // %eq = graph_op "Equal,i,i"(%caseId, %zeroScalarTensor)
  // %cond = graph_op "tf_tensor_to_i1"(%eq : $TensorHandle<Builtin.Int1>)
  // cond_br %cond, bb1, bb2
  auto BA = getBuilder(); // Builder for accelerator.
  auto loc = remapLocation(getUserSourceLocation(inst->getDebugLocation()));
  auto receivedCaseId =
      createAcceleratorReceive(BA, loc, remapType(int32SILType),
                               /*isScalar*/ true, nextSendID, FP.deviceInfo);

  // Let K be the total # cases (including the optional default case). Create a
  // chain of K-1 BB's with cond_br, to dispatch to the K cases.
  caseId = 0;
  for (auto *caseBB : inst->getSuccessorBlocks()) {
    // Create a scalar const tensor i, to compare with case id.
    SILValue constTensorWithCaseId;
    {
      GraphOperationBuilder constOpBuilder("Const");
      createConstTensorAttrsOnAccel(SymbolicValue::getInteger(caseId, 32),
                                    int32SILType, ctx, &constOpBuilder);
      FP.deviceInfo.handleDevicePlacement(
          "Const", /*opDevice*/ getDeviceString(DeviceType::ALL),
          BA.getModule().getASTContext(), &constOpBuilder);
      auto tensorHandleInt32Ty =
          convertElementTypeToTensorValueType(int32SILType);
      auto *caseIdInst = constOpBuilder.build(BA, ctx, loc,
                                              {tensorHandleInt32Ty});
      constTensorWithCaseId = getSingleValueResult(caseIdInst);
    }

    // Compare caseId to a constant and get a Tensorhandle<Builtin.Int1> value.
    SILValue condBrOperand;
    {
      // Omit the metatype attr T for simplicity, and TF graphDef compiler can
      // infer the type.
      GraphOperationBuilder equalOpBuilder("Equal");
      equalOpBuilder.addArgument(receivedCaseId);
      equalOpBuilder.addArgument(constTensorWithCaseId);

      auto boolFieldSILType =
          extractBuiltinTypeFromStdlibNumericType(ctx.getBoolDecl());
      auto tensorHandleI1Ty =
          convertElementTypeToTensorValueType(boolFieldSILType);

      FP.deviceInfo.handleDevicePlacement(
          "Equal", /*opDevice*/ getDeviceString(DeviceType::ALL),
          BA.getModule().getASTContext(), &equalOpBuilder);

      auto *equalComparisonInst = equalOpBuilder.build(BA, ctx, loc,
                                                       {tensorHandleI1Ty});
      auto *condBrInst = createTensorToInt1Inst(
          getSingleValueResult(equalComparisonInst), BA, loc, FP.deviceInfo);
      condBrOperand = getSingleValueResult(condBrInst);
    }

    // For case `caseId`, the true branch dispatches to the corresponding enum
    // caseBB. For the false branch:
    // - When `caseId` is the second to last case, the false branch dispatches
    // to the last case.
    // - Otherwise, we create a new BB for it, which evaluates the next cond_br
    // on case `caseId` + 1
    auto isSecondToLastBlock = (caseId == inst->getNumSuccessors() - 2);
    auto *falseBB = isSecondToLastBlock
                        ? getOpBasicBlock(getLastSuccessorBlock(inst))
                        : BA.getFunction().createBasicBlock();
    //
    // The args for the true and false BBs are always sent from host to
    // accelerator, so no BB args for them here.
    BA.createCondBranch(loc, condBrOperand, getOpBasicBlock(caseBB),
                        /*TrueArgs*/ OperandValueArrayRef(), falseBB,
                        /*FalseArgs*/ OperandValueArrayRef(), ProfileCounter(),
                        ProfileCounter());

    // We've covered all cases that we want to  dispatch to.
    if (isSecondToLastBlock)
      break;

    BA.setInsertionPoint(falseBB);
    ++caseId;
  }

  ++nextSendID;
}

/// Insert a send of values from the specified instruction result(s) to the
/// accelerator, and insert receives in it.
void PartitionCloner::insertHostToAccelTransfer(SILInstruction &inst) {
  if (auto *TI = dyn_cast<TermInst>(&inst)) {
    if (shouldMarkSend(TI)) {
      handleSendRecvForTerminator(TI);
      return;
    }
    TI->dump();
    llvm_unreachable("Cannot handle the above terminator with Marking::Send");
  }

  SILBuilder BH(++SILBasicBlock::iterator(&inst)); // Builder for host.
  auto BA = getBuilder();                          // Builder for accelerator.

  auto loc = getUserSourceLocation(&inst);

  for (auto result : inst.getResults()) {
    bool isScalar = !isTensorHandle(result->getType().getASTType());
    // Create the receive in the accelerator code.  Each send/receive pair gets
    // a unique ID to associate one with the other.
    this->ValueMap[result] =
        createAcceleratorReceive(BA, loc, remapType(result->getType()),
                                 isScalar, nextSendID, FP.deviceInfo);

    // if `result` is a scalar, we need to convert it to TensorHandle<T>,
    // before sending that value to accelerator. We pass such a function
    // reference to createHostSend() below for this scalar->tensor conversion.
    SILFunction *createScalarTensorFn = nullptr;
    if (isScalar) {
      createScalarTensorFn =
          lookupSendReceiveFunction("scalar", result->getType(), loc);
      // If `result` were not a send'able data type like String, compiler
      // would have rejected the program before reaching here.
      assert(createScalarTensorFn &&
             "At this point of compilation, scalar value must be send'able "
             "from Swift to TensorFlow.");
    }
    SILFunction *sendFn =
        lookupSendReceiveFunction("sendToAccelerator", result->getType(), loc);
    assert(sendFn &&
           "At this point of compilation, the value must be send'able "
           "from Swift to TensorFlow.");
    // Create the send in the host code.
    createHostSend(BH, loc, result, tensorComputation, nextSendID,
                   tensorFlowModule, createScalarTensorFn, sendFn);
    nextSendID++;
  }
}

bool PartitionCloner::insertAccelToHostTransfer(SILValue value, SILLocation loc) {
  assert(value->getDefiningInstruction() ||
         isa<SILArgument>(value) && "Don't know how to receive this value");
  SILFunction *receiveFn = lookupSendReceiveFunction("receiveFromAccelerator",
                                                     value->getType(), loc);
  // If `value` is not receivable on Swift host from TensorFlow (e.g. it is a
  // tensor resource handle), reject the program.
  if (!receiveFn)
    return false;

  // Diagnose implicit data transfers if they are implicit.
  FP.diagnoseUsesFromHost(value, loc);

  SILBuilder BH(FP.hostFn); // Builder for the host.
  auto BA = getBuilder();   // Builder for accelerator.

  if (auto *inst = value->getDefiningInstruction()) {
    assert(!isa<TermInst>(inst) && "Cannot move a terminator");
    BH.setInsertionPoint(++SILBasicBlock::iterator(inst));

    auto otherInst = ValueMap[value]->getDefiningInstruction();
    BA.setInsertionPoint(++SILBasicBlock::iterator(otherInst));

  } else {
    auto *arg = cast<SILArgument>(value);
    BH.setInsertionPoint(&arg->getParent()->front());

    auto otherBB = remapBasicBlock(arg->getParent());
    BA.setInsertionPoint(&otherBB->front());
  }

  // Create the send in the accelerator code.  Each send/receive pair gets
  // a unique ID to associate one with the other.
  createAcceleratorSend(BA, loc, remapValue(value), nextSendID, FP.deviceInfo);

  // Create the receive in the host code.
  auto newVal = createHostReceive(BH, loc, value->getType(), tensorComputation,
                                  nextSendID, tensorFlowModule,
                                  FP.hostFn.getModule(), receiveFn);
  value->replaceAllUsesWith(newVal);
  nextSendID++;
  return true;
}

bool PartitionCloner::handleHostReferencesOfMovedValue(SILValue value,
                                                       SILLocation loc) {
  // If the argument has any non-debug-non-retain/release instructions using
  // it, then we need to insert a copy.
  SmallVector<SILInstruction *, 4> instToRemove;
  bool needsCopy = false;
  for (auto use : value->getUses()) {
    auto user = use->getUser();
    if (isUserIgnoredByPartitioning(user)) {
      instToRemove.push_back(user);
      continue;
    }
    needsCopy = true;
    break;
  }

  if (needsCopy) {
    // If we need the value on the host, then keep the retain/release ops.
    return insertAccelToHostTransfer(value, loc);
  } else {
    // Otherwise, drop them.
    for (auto *inst : instToRemove)
      inst->eraseFromParent();
    return true;
  }
}

/// Move and clone code over from the input block into this block, inserting
/// transfers between the host and destination code as necessary.
void PartitionCloner::cloneBlock(SILBasicBlock *BB) {
  if (!FP.markedBlocks.count(BB))
    return; // Ignore blocks that aren't in accelerator code.
  auto newBB = BBMap[BB];

  Builder.setInsertionPoint(newBB);
  for (auto &inst : *BB) {
    // If the specified instruction is used by the accelerator program somehow,
    // we have to copy the instruction over, copy it, or send the result.
    auto it = FP.markedInstructions.find(&inst);
    if (it == FP.markedInstructions.end())
      continue;

    switch (it->second) {
    case Marking::Move:
      instructionsToRemove.push_back(&inst);
      LLVM_FALLTHROUGH;
    case Marking::Copy:
      visit(&inst);
      break;
    case Marking::Send:
      insertHostToAccelTransfer(inst);
      break;
    case Marking::Argument:
      // Already handled.
      break;
    case Marking::Delete:
      instructionsToRemove.push_back(&inst);
      break;
    }
  }

  // If the terminator of this block wasn't live then it was either an
  // unconditional branch (which never matter for control dependence analysis)
  // or some kind of conditional branch that wasn't important because control
  // eventually flowed to a post dominator that didn't care about the direction
  // of this branch.  In either case, we can provide a terminator by introducing
  // an unconditional branch to the post dominator of the block.
  if (newBB->empty() || !isa<TermInst>(newBB->back())) {
    auto PDI = FP.tensorCodeBlocks.getPostIDom(BB);
    auto destBB = PDI ? remapBasicBlock(PDI) : exitBB;
    Builder.createBranch(BB->getTerminator()->getLoc(), destBB);
  }
}

void PartitionCloner::cloneFunction(ArrayRef<SILValue> resultValues) {
  // Go through and create all the blocks before we start cloning the
  // instructions over.  This allows us to remap instructions when we clone
  // them over.
  initBlock(FP.tensorStartPoint->getParent()); // First block first.

  for (auto &BB : FP.hostFn) {
    // If the BB is unmarked, we don't need it for the accelerator.
    if (!FP.markedBlocks.count(&BB) || &BB == FP.tensorStartPoint->getParent())
      continue;

    initBlock(&BB);
  }

  // Create a block for the exit node in case we need it.
  exitBB = Builder.getFunction().createBasicBlock();

  // Now that all the basic blocks and BBArguments are created, we can walk the
  // function in depth first order copying the code over.  Because we're working
  // in depth first order and have BB Arguments resolved, we're guaranteed to
  // see all definitions before uses.
  for (auto *BB : llvm::depth_first(&FP.hostFn)) {
    cloneBlock(BB);
  }

  // We can end up with to-be-deleted instructions (like debug_value's) outside
  // of the marked region.  These won't be seen in the cloneBlock walk we just
  // did, but we do want to remove them.  Check to see if we have any of these,
  // and arrange for them to be removed.
  for (auto ip : FP.markedInstructions) {
    if (ip.second == Marking::Delete &&
        !FP.markedBlocks.count(ip.first->getParent()))
      instructionsToRemove.push_back(ip.first);
  }

  // Okay at this point we're done except for setting up the exitBB.  Check to
  // see if it is unused.  If so, we nuke it, otherwise we add a return.
  if (exitBB->pred_empty()) {
    exitBB->eraseFromParent();
  } else {
    Builder.setInsertionPoint(exitBB);

    // Create a return of N values, producing a tuple if necessary.
    SILValue result;
    if (resultValues.size() == 1)
      result = remapValue(resultValues[0]);
    else {
      SmallVector<SILValue, 4> results;
      for (auto r : resultValues)
        results.push_back(remapValue(r));

      result = Builder.createTuple(FP.hostFn.getLocation(), results);
    }

    Builder.createReturn(FP.hostFn.getLocation(), result);
  }
}

/// Now that all of the interesting instructions are cloned over, we need to
/// clean up the input function by removing the instructions, and inserting
/// sends of results from the accelerator code back to the host code.
/// We can still reject the program here, if the values to send do not conform
/// to the TensorSendableReceivable protocol (e.g. a ResourceHandle)
///
bool PartitionCloner::finalizeOriginal() {

  // Build a set of the instructions we're going to remove so we can add new
  // values without fear of adding duplicates.
  SmallPtrSet<SILInstruction *, 16> instsToRemoveSet(
      instructionsToRemove.begin(), instructionsToRemove.end());
  assert(instsToRemoveSet.size() == instructionsToRemove.size() &&
         "duplicates shouldn't exist in the instsToRemove list");

  // Start by dropping interdependent references so we don't get confused by
  // uses that have moved over.  We are adding values to the vector as we
  // process it, so iterate indexes into it to avoid invalidating iterators.
  for (unsigned idx = 0; idx != instructionsToRemove.size(); ++idx) {
    auto inst = instructionsToRemove[idx];

#ifndef NDEBUG
    // When removing a graph_op or ApplyInst tensor op instruction TO, for each
    // tensor argument TA of it, since we know TA is taken at +0, we need not
    // rebalance the retain/release count of TA.
    if (isa<BuiltinInst>(inst)) {
      // The tfop inst takes tensor operands as +0, but we know of no API that
      // we can use to verify that here.
    } else if (auto *ai = dyn_cast<ApplyInst>(inst)) {
      // Sanity check: Verify that `inst` takes all tensor operands at +0.
      assert(ai->getNumOperands() > 0);
      auto firstOper = ai->getOperand(0);
      auto *fri = castWithDebugInfo<FunctionRefInst>(
          firstOper->getDefiningInstruction());
      const auto &conventions = fri->getConventions();
      const auto &operands = inst->getAllOperands();
      assert(operands.size() == conventions.getNumParameters() + 1);
      for (unsigned i = 1, e = operands.size(); i != e; ++i) {
        auto op = operands[i].get();
        if (!isTensorFlowValue(op->getType()))
          continue;

        auto argConvention = conventions.getSILArgumentConvention(i - 1);
        if (argConvention != SILArgumentConvention::Direct_Guaranteed) {
          inst->print(llvm::errs());
          llvm_unreachable("Apply inst takes a tensor operand NOT at +0!");
        }
      }
    }
#endif // NDEBUG

    // Loop through and drop all the operand references.  If we strand any
    // primitive instructions, clean them up now so we get cleaner IR.
    for (auto &op : inst->getAllOperands()) {
      auto v = op.get();
      op.drop();

      // If we just dropped the last use of a trivial instruction (which are how
      // we represent attributes for instructions), clean it up now.
      if (auto opInst = v->getDefiningInstruction())
        if (!opInst->hasUsesOfAnyResult() &&
            (isa<LiteralInst>(opInst) || isa<MetatypeInst>(opInst)) &&
            instsToRemoveSet.insert(opInst).second)
          instructionsToRemove.push_back(opInst);
    }
  }

  // As we're removing branch arguments, we remove all the bb args for a given
  // block at a time.  This is important because otherwise we invalidate our our
  // bb arg -> branch argument mappings.
  SmallPtrSet<SILBasicBlock *, 4> argsRemovedBlocks;

  // For BB arguments, we remove the uses from the branches first (which breaks
  // interdependent references) and delete the actual arguments later.
  for (auto argInfo : FP.markedBBArguments) {
    // We delete arguments that are moved over to the accelerator.
    auto marking = argInfo.second.first;
    if (marking != Marking::Move && marking != Marking::Delete) {
      assert((marking == Marking::Copy || marking == Marking::Argument) &&
             "Only move/copy/argument supported for arguments right now");
      continue;
    }

    // Check to see if we already processed the block this argument lives in.
    auto arg = argInfo.first;
    auto *bb = arg->getParent();
    if (!argsRemovedBlocks.insert(bb).second)
      continue;

    // Ok, this is the first time we're processing this block.  It could have
    // lots of bb arguments, some of which are being removed.  We will remove
    // the arguments in a separate loop, but here we want to remove all of the
    // values passed in by terminators in predecessor blocks.  Start by
    // constructing a bitvector so we know which args are going to be removed.
    llvm::BitVector argsToRemoveMask(bb->getNumArguments());
    unsigned argNo = 0;
    for (auto arg : bb->getArguments()) {
      auto it = FP.markedBBArguments.find(arg);
      if (it != FP.markedBBArguments.end() &&
          (it->second.first == Marking::Move ||
           it->second.first == Marking::Delete)) {
        argsToRemoveMask[argNo] = true;
      }
      ++argNo;
    };

    // Now remove the formal values provided by any branches that jump to that
    // block, as indicated by the BitVector.
    //
    // Collect all predecessor blocks before removing values from branches as
    // pred_iterators get invalidated when removing formal values.
    SmallPtrSet<SILBasicBlock*, 8> predBlocks(bb->pred_begin(), bb->pred_end());
    for (auto pi : predBlocks) {
      auto *br = cast<BranchInst>(pi->getTerminator());
      SmallVector<SILValue, 8> operands;
      for (unsigned i = 0, e = br->getNumOperands(); i != e; ++i)
        if (!argsToRemoveMask[i])
          operands.push_back(br->getOperand(i));
      SILBuilder(br).createBranch(br->getLoc(), br->getDestBB(), operands);
      br->eraseFromParent();
    }
  }

  // Ok, now all interdependent references have been dropped.  If there are any
  // uses of values that we moved over to the accelerator, then we must insert
  // a transfer from the accelerator of the computed value.  Regardless, we can
  // now delete the defining instruction/argument.
  for (auto argInfo : FP.markedBBArguments) {
    auto marking = argInfo.second.first;
    if (marking != Marking::Move && marking != Marking::Delete)
      continue;

    auto arg = argInfo.first;
    if (!handleHostReferencesOfMovedValue(arg, argInfo.second.second))
      return false;

    // Remove it from the block that it lives in.
    arg->getParent()->eraseArgument(arg->getIndex());
  }

  // Next, add sends back of any values that are used by the host code, and
  // remove the original instruction.
  for (auto inst : instructionsToRemove) {
    if (isUserIgnoredByPartitioning(inst)) {
      // TODO: If we are removing retain/release insts over a tensor value `a`,
      // and `a` has uses in the host code after the tensor program region, we
      // need to rebalance its retain/release count.
      inst->eraseFromParent();
      continue;
    }
    // These insts cannot contain types like unconditional branch, which do
    // not have getResults() defined.
    if (isa<NonValueInstruction>(inst)) {
      inst->dump();
      llvm_unreachable("Cannot remove this inst from host code!");
    }
    for (auto result : inst->getResults())
      if (!handleHostReferencesOfMovedValue(result,
                                            getUserSourceLocation(inst)))
        return false;

    inst->eraseFromParent();
  }

  // The copy-in/out markers should be removed now.  They are noops which serve
  // no purpose now that we have emitted diagnostics.
  for (auto ecm : FP.explicitCopyMarkers) {
    assert(isa<ApplyInst>(ecm) && ecm->getResults().size() == 1 &&
           ecm->getNumOperands() == 2 && "unknown copy in/out instruction");
    auto callee = ecm->getOperand(1);
    // `ecm` takes `callee` at +0 and returns a result at +1, so we need to
    // issue a retain to balance the removal of `ecm`.
    SILBuilder B(ecm);
    B.createStrongRetain(ecm->getLoc(), callee, Atomicity::Atomic);
    ecm->getResults()[0]->replaceAllUsesWith(callee);
    ecm->eraseFromParent();

    if (callee->use_empty()) // Remove the function_ref too.
      cast<SingleValueInstruction>(callee)->eraseFromParent();
  }
  return true;
}

/// Convert the specified scalar value (e.g. an i32) into a 0d CTensorHandle
/// value using the TensorFlow runtime utilities.
/// To create the function call "_swift_tfc_CreateCTensorHandle", load that
/// function definition into `userModule` first, so that we can get the generic
/// substitution map as needed to issue the call.
static SILValue convertScalarToHostTensorHandle(SILValue value, SILBuilder &B,
                                                SILLocation loc) {
  // We need to create a dtype value.  It is an imported C enum value, so it is
  // modeled as a struct that wraps an integer value (itself a struct).
  auto dtypeVal = convertSwiftTypeToTF(value->getType().getASTType());
  assert(dtypeVal && "Can only convert TF compatible types to Tensors");

  // Get a reference to the CreateCTensorHandle function, which is defined like
  // this:
  // @_silgen_name("_swift_tfc_CreateCTensorHandle")
  // func _TFCCreateCTensorHandle<T>(_ value : T,
  //                                 _ dtype: TF_DataType) -> CTensorHandle
  auto &userModule = B.getModule();
  auto *createFn = userModule.findFunction("_swift_tfc_CreateCTensorHandle",
                                           SILLinkage::PublicExternal);
  auto *fnRef = B.createFunctionRef(loc, createFn);

  auto dtypeType = fnRef->getFunctionType()->getParameters()[1].getType();
  auto dtypeDecl = dtypeType->getAnyNominal();
  auto dtypeFieldType = getSingleElementDeclFieldType(dtypeDecl);

  // The internal type is something like UInt32.  Create it now.
  auto dtype =
      createSomeIntegerValue(dtypeVal, B, loc, dtypeFieldType->getAnyNominal());
  // Then wrap it in the dtype enum.
  dtype = wrapInStruct(dtype, dtypeDecl, B, loc);

  // This is a generic function over the value type, so we have to pass it in
  // by address on the stack.
  auto stackAlloc = B.createAllocStack(loc, value->getType());

  auto storeOwnership = B.getFunction().hasQualifiedOwnership()
                            ? StoreOwnershipQualifier::Trivial
                            : StoreOwnershipQualifier::Unqualified;
  B.createStore(loc, value, stackAlloc, storeOwnership);

  auto access = B.createBeginAccess(loc, stackAlloc, SILAccessKind::Read,
                                    SILAccessEnforcement::Static,
                                    /*noNestedConflict=*/false,
                                    /*fromBuiltin=*/false);
  auto result =
      B.createApply(loc, fnRef,
                    getSingleSubstitutionMapForElementType(
                        value->getType().getASTType(), B.getASTContext()),
                    {access, dtype}, /*isNonThrowing*/ false);
  // Finish our read access and free the stack memory.
  B.createEndAccess(loc, access, /*aborted*/ false);
  B.createDeallocStack(loc, stackAlloc);

  return result;
}

// If there exists a CFG path from `srcBlock` to `destBlock`, picks an abitrary
// one, appends the blocks on that path to `path`, and returns true. Otherwise
// returns false without modifying `path`.
// `visitedBlocks` is updated with the blocks visited in all cases.
static bool
gatherBBsOnCFGPathHelper(SILBasicBlock *srcBlock, SILBasicBlock *destBlock,
                         llvm::SmallPtrSetImpl<SILBasicBlock *> &visitedBlocks,
                         llvm::SmallVectorImpl<SILBasicBlock *> &path) {
  visitedBlocks.insert(srcBlock);

  // Optimistically assume `srcBlock` is on a path to `destBlock`. If such a
  // path is not found, this update to `path` will be reverted.
  path.push_back(srcBlock);

  if (srcBlock == destBlock)
    return true; // Done with DFS!

  for (SILBasicBlock *childBB : srcBlock->getSuccessors()) {
    // When the CFG has loops, `childBB` may have been visited, in which case we
    // do not visit it again.
    if (!visitedBlocks.count(childBB) &&
        gatherBBsOnCFGPathHelper(childBB, destBlock, visitedBlocks, path))
      return true;
  }

  path.pop_back();
  return false;
}

// Finds an arbitrary CFG path from `srcBlock` to `destBlock` (requires that
// such a path exists), and sets `pathset` to the blocks on that path.
static void
gatherBBsOnCFGPath(SILBasicBlock *srcBlock, SILBasicBlock *destBlock,
                   llvm::SmallPtrSetImpl<SILBasicBlock *> &pathSet) {
  llvm::SmallPtrSet<SILBasicBlock *, 8> visitedBlocks;
  llvm::SmallVector<SILBasicBlock *, 4> path;
  bool foundPath =
      gatherBBsOnCFGPathHelper(srcBlock, destBlock, visitedBlocks, path);
  assert(foundPath);
  for (auto *block : path) {
    pathSet.insert(block);
  }
}

// This function balances the retain/release count of `newResult` in the
// rewritten host program at the tensor end point. The algorithm is:
//
// 1. Find a CFG path from basic block that defines `oldResult` (referred to as
// `defBlock`) to the basic block that contains the tensor end point (referred
// to as `endBlock`).
// 2. Look at all uses of `oldResult` along that path, and compute the
// retain/release balance, denoted by K, of those uses.
// 3. Emit K strong_retain's right after the tensor end point.
//
// For example, let the CFG be:
//   bb0 -> bb1; bb1 -> {bb2, bb3}; bb2 -> bb1
// Let `defBlock` be bb0, and `endBlock` be bb3. In this case, we only need to
// look at uses along the path of bb0 -> bb1 -> bb3. Ignoring bb2 is fine, since
// the retain/release count at the entry of bb2 is guaranteed to equal the count
// at the entry of bb3 (as bb2 and bb3 are both successors of bb1), and the
// count at the exit of bb2 is guaranteed to equal the count at the end of bb0
// (as both bb0 and bb2 are predecessors of bb1).
void TFFunctionPartition::balanceRetainReleaseCount(SILValue oldResult,
                                                    SILValue newResult,
                                                    SILBuilder &B) {
  auto *defBlock = oldResult->getDefiningInstruction()->getParent();
  auto *endBlock = B.getInsertionBB();
  llvm::SmallPtrSet<SILBasicBlock *, 4> relevantBlocks;
  gatherBBsOnCFGPath(defBlock, endBlock, relevantBlocks);

  int retainReleaseBalance = 0;
  for (auto UI = oldResult->use_begin(), UE = oldResult->use_end(); UI != UE;) {
    auto *operand = *UI++;
    auto user = operand->getUser();

    // Ignore uses outside the CFG path from `defBlock` to `endBlock`.
    if (!relevantBlocks.count(user->getParent()))
      continue;

    // Ignore uses outside the tensor program region.
    if (DI.dominates(tensorEndPoint, user))
      continue;

    if (isa<DebugValueInst>(user))
      continue;

    // Recall that tensor ops take tensor arguments as +0 values, so no adjust
    // is needed.
    if (tensorOpsSet.count(user))
      continue;

    if (isa<StrongRetainInst>(user)) {
      ++retainReleaseBalance;
      continue;
    }

    if (isa<StrongReleaseInst>(user)) {
      --retainReleaseBalance;
      continue;
    }

    user->dump();
    oldResult->dump();
    llvm_unreachable(
        "Unhandled instruction type, as a use of a result tensor above");
  }

  LLVM_DEBUG(llvm::dbgs() << "Creating " << retainReleaseBalance
                          << " retains for result tensor :\n");
  LLVM_DEBUG(oldResult->print(llvm::dbgs()));
  assert(retainReleaseBalance >= 0);
  for (int i = 0; i < retainReleaseBalance; ++i) {
    B.createStrongRetain(oldResult.getLoc(), newResult, Atomicity::Atomic);
  }
}

void TFFunctionPartition::insertTensorComputationStartEndTerminate(
    ArrayRef<SILValue> resultValues) {
  // Sanity check that all result values are tensor handles. Note SIL
  // accelerator functions under the "tensorflow" convention can also return
  // variant and resource tensors (in addition to tensor handles), but such
  // functions will not generate all host-side code, and thus this method will
  // not be called.
  for (auto resultValue : resultValues) {
    assert(TFSendRecvOpaqueHandle || isTensorHandle(resultValue->getType()) &&
           "Cannot return a non-TensorHandle value to host in the TF program "
           "-- should this function use tensorflow convention?");
  }

  auto &ctx = hostFn.getASTContext();
  auto loc = hostFn.getLocation();

  // We are going to create a call to this function to kick off the tensor
  // computation:
  //
  // The C type is TF_TensorHandle*
  // public typealias CTensorHandle = OpaquePointer
  //
  // @_silgen_name("_swift_tfc_StartTensorComputation")
  // public func _TFCStartTensorComputation(
  //   _ programByteAddress: UnsafeRawPointer,
  //   _ programByteCount: Int,
  //   _ entryFunctionBaseName: UnsafePointer<Int8>,
  //   _ tensorArgumentAddress: UnsafePointer<CTensorHandle>,
  //   _ tensorArgumentCount: Int,
  //   _ helperFunctionCount: Int,
  //   _ resultCount: Int
  // ) -> TensorComputation {
  auto startComputationFn = hostFn.getModule().findFunction(
      "_swift_tfc_StartTensorComputation", SILLinkage::PublicExternal);

  // We are going to create a call to this function to syncronize with the
  // completed tensor computation and collect the results.
  //
  // @_silgen_name("_swift_tfc_FinishTensorComputation") public
  // func _TFCFinishTensonComputation(
  //    _ computation: TensorComputation,
  //    _ resultAddress: UnsafeMutablePointer<CTensorHandle>,
  //    _ tensorResultCount: Int) {...}
  auto finishComputationFn = hostFn.getModule().findFunction(
      "_swift_tfc_FinishTensorComputation", SILLinkage::PublicExternal);

  // We may also generate a call to this function to kill the tensor computation
  // if the host execution goes away:
  //
  // @_silgen_name("_swift_tfc_TerminateTensorComputation")
  // public func _TFCTerminateTensorComputation(
  //   _ computation: TensorComputation
  // ) {...}
  //
  auto terminateComputationFn = hostFn.getModule().findFunction(
      "_swift_tfc_TerminateTensorComputation", SILLinkage::PublicExternal);

  if (!startComputationFn || !finishComputationFn || !terminateComputationFn) {
    diagnose(ctx, hostFn.getLocation().getSourceLoc(), diag::tf_internal_error,
             "'_swift_tfc_' entrypoints not found in TensorFlow module");
    return;
  }

  // Create various types and SIL types that we'll be using below.
  auto cTensorHandleTy = ctx.getOpaquePointerDecl()->getDeclaredType();
  auto cTensorHandleSILTy =
      SILType::getPrimitiveObjectType(cTensorHandleTy->getCanonicalType());
  auto unsafePointerType = BoundGenericType::get(
      ctx.getUnsafePointerDecl(), /*parent*/ Type(), cTensorHandleTy);
  auto unsafePointerSILType =
      SILType::getPrimitiveObjectType(unsafePointerType->getCanonicalType());
  auto unsafeMutPointerType = BoundGenericType::get(
      ctx.getUnsafeMutablePointerDecl(), /*parent*/ Type(), cTensorHandleTy);
  auto unsafeMutPointerSILType =
      SILType::getPrimitiveObjectType(unsafeMutPointerType->getCanonicalType());
  auto int8PointerType =
      BoundGenericType::get(ctx.getUnsafePointerDecl(), /*parent*/ Type(),
                            ctx.getInt8Decl()->getDeclaredType());
  auto int8PointerSILType =
      SILType::getPrimitiveObjectType(int8PointerType->getCanonicalType());

  // This assumes that the first member of TensorHandle is the CTensorHandle.
  auto tensorHandleDecl = ctx.getTensorHandleDecl();
  assert(getSingleElementDeclFieldType(tensorHandleDecl) &&
         "TensorHandle should have exactly one field");
  auto *anyTensorHandleClass =
      tensorHandleDecl->getSuperclass()->getAnyNominal();
  auto anyTensorHandleSILTy = SILType::getPrimitiveObjectType(
      anyTensorHandleClass->getDeclaredType()->getCanonicalType());
  assert(anyTensorHandleClass);
  auto *tensorHandleMember =
      *anyTensorHandleClass->getStoredProperties().begin();

  // Ownership markers for CTensorHandle accesses.
  auto loadOwnership = hostFn.hasQualifiedOwnership()
                           ? LoadOwnershipQualifier::Trivial
                           : LoadOwnershipQualifier::Unqualified;
  auto storeOwnership = hostFn.hasQualifiedOwnership()
                            ? StoreOwnershipQualifier::Trivial
                            : StoreOwnershipQualifier::Unqualified;

  SILBuilder B(tensorStartPoint);

  // Create a string literal to hold the  serialized protobuf for the tensor
  // program.  We haven't actually created that yet, so we create a placeholder
  // and RAUW it later.
  auto programPlaceholder = B.createStringLiteral(
      loc, StringRef(), StringLiteralInst::Encoding::Bytes);
  auto program =
      wrapInStruct(programPlaceholder, ctx.getUnsafeRawPointerDecl(), B, loc);
  auto entryFunctionBaseNamePlaceholder = B.createStringLiteral(
      loc, StringRef(), StringLiteralInst::Encoding::UTF8);
  auto entryFunctionBaseName = B.createStruct(
      loc, int8PointerSILType, {entryFunctionBaseNamePlaceholder});

  // Pass zero as a place holder for now; they will be filled in later.
  IntegerLiteralInst *programLengthPlaceholder = nullptr;
  auto programLength = createIntValue(0, B, loc, &programLengthPlaceholder);
  // We cannot yet use `deviceInfo.usedDeviceTypes` to set
  // `helperFunctionCount`, because Swift<->TF tensor transfers are handled in
  // the later PartitionCloner logic, which could be adding a TF CPU device as
  // the impl (queueing) mechanism for such tensor transfers.
  IntegerLiteralInst *helperFunctionCountPlaceholder = nullptr;
  auto helperFunctionCount =
      createIntValue(0, B, loc, &helperFunctionCountPlaceholder);

  // We pass the list of N tensor arguments as a pointer + length of
  // CTensorHandle values, i.e.:
  //   (..., _ inputs: UnsafePointer<CTensorHandle>, _ numInputs: Int)
  // to get this, we create an N-ary tuple on the stack and pass the address of
  // the first element.

  // Note that the allocation becomes a scalar value when it has one value,
  // and we make sure to allocate space for at least one element to avoid
  // passing an undef into the runtime.
  size_t startAllocSize = std::max(tensorFnArguments.size(), size_t(1));
  SmallVector<TupleTypeElt, 8> tupleEltTypes(startAllocSize,
                                             TupleTypeElt(cTensorHandleTy));
  auto tupleType = TupleType::get(tupleEltTypes, ctx)->getCanonicalType();
  // %0 = alloc_stack $(CTensorHandle, CTensorHandle)
  auto stackAlloc =
      B.createAllocStack(loc, SILType::getPrimitiveObjectType(tupleType));

  // Emit a store into the tuple for each parameter, giving it a copy of the
  // OpaquePointer that is within the TensorHandle<T> value we have.
  for (size_t i = 0, numArgs = tensorFnArguments.size(); i != numArgs; ++i) {
    auto tensorValue = tensorFnArguments[i];

    // The argument is either a TensorHandle<T> or a scalar value that we've
    // closed over.  If it is a TensorHandle<T>, load the CTensorHandle out of
    // it.  If it is a scalar, then we need to box the scalar in a
    // CTensorHandle.
    if ((TFSendRecvOpaqueHandle &&
         isTensorFlowValue(tensorValue->getType().getASTType())) ||
        isTensorHandle(tensorValue->getType().getASTType())) {
      // Upcast to _AnyTensorHandle.
      tensorValue = B.createUpcast(loc, tensorValue, anyTensorHandleSILTy);
      auto fieldAddress =
          B.createRefElementAddr(loc, tensorValue, tensorHandleMember);
      tensorValue = B.createLoad(loc, fieldAddress, loadOwnership);
    } else {
      tensorValue = convertScalarToHostTensorHandle(tensorValue, B, loc);
    }
    SILValue eltAddr = stackAlloc;
    if (numArgs >= 2)
      eltAddr = B.createTupleElementAddr(loc, stackAlloc, i,
                                         cTensorHandleSILTy.getAddressType());
    B.createStore(loc, tensorValue, eltAddr, storeOwnership);
  }

  // Ok, now we have our array on the stack.  Start a read access, and get the
  // address of the first element as an UnsafePointer<CTensorHandle>.
  // %1 = begin_access [read] [static] %0 : $*(CTensorHandle, CTensorHandle)
  auto access = B.createBeginAccess(loc, stackAlloc, SILAccessKind::Read,
                                    SILAccessEnforcement::Static,
                                    /*noNestedConflict=*/false,
                                    /*fromBuiltin=*/false);

  // %2 = tuple_element_addr %1 : $*(CTensorHandle, CTensorHandle), 0
  SILValue firstPtr = access;
  if (tensorFnArguments.size() >= 2)
    firstPtr = B.createTupleElementAddr(loc, firstPtr, 0,
                                        cTensorHandleSILTy.getAddressType());
  // %3 = address_to_pointer %2 : $*CTensorHandle to $Builtin.RawPointer
  firstPtr =
      B.createAddressToPointer(loc, firstPtr, SILType::getRawPointerType(ctx));

  // %4 = struct $UnsafePointer<CTensorHandle>(%3 : $Builtin.RawPointer)
  firstPtr = B.createStruct(loc, unsafePointerSILType, firstPtr);

  auto numTensorArguments = createIntValue(tensorFnArguments.size(), B, loc);

  // TODO: When the runtime matures, we shouldn't have to pass in the # results.
  auto numTensorResults = createIntValue(resultValues.size(), B, loc);

  // The first two arguments are the program, the rest of the arguments are the
  // parameters passed in.
  SILValue startArgs[] = {
      program,               // programByteAddress: UnsafeRawPointer
      programLength,         // programByteCount: Int
      entryFunctionBaseName, // entryFunctionBaseName: UnsafePointer<Int8>
      firstPtr,           // tensorArgumentAddress: UnsafePointer<CTensorHandle>
      numTensorArguments, // tensorArgumentCount: Int
      helperFunctionCount, // helperFunctionCount,: Int
      numTensorResults     // resultCount: Int
  };

  // Now that we have our argument list, create a call.
  auto startProgramFnRef = B.createFunctionRef(loc, startComputationFn);

  // Create the runtime call in the host program that kicks off the tensor
  // program, setting the argument values we provide as the tensor params.
  auto tensorComputation =
      B.createApply(loc, startProgramFnRef, /*no substitutions*/ {}, startArgs,
                    /*isNonThrowing*/ false);

  // Finish our read access and free the stack memory.
  // end_access %1 : $*(CTensorHandle, CTensorHandle)
  B.createEndAccess(loc, access, /*aborted*/ false);
  // dealloc_stack %0 : $*(CTensorHandle, CTensorHandle)
  B.createDeallocStack(loc, stackAlloc);

  // Create the runtime call in the host program that rendezvous with the tensor
  // program and returns the results.

  // Start by creating an uninitialized buffer to receive the values into.
  B.setInsertionPoint(tensorEndPoint);

  // Note that the allocation becomes a scalar value when it has one value,
  // and we avoid allocating zero elements, because we don't want to pass in
  // undef into the runtime.
  size_t resultAllocSize = std::max(resultValues.size(), size_t(1));
  tupleEltTypes = SmallVector<TupleTypeElt, 8>(resultAllocSize,
                                               TupleTypeElt(cTensorHandleTy));
  tupleType = TupleType::get(tupleEltTypes, ctx)->getCanonicalType();

  // %0 = alloc_stack $(CTensorHandle, CTensorHandle)
  stackAlloc =
      B.createAllocStack(loc, SILType::getPrimitiveObjectType(tupleType));

  // Ok, now we have our uninitialized array on the stack.  Start a write
  // access, and get the address of the first element as an
  // UnsafePointer<CTensorHandle>.
  // %1 = begin_access [write] [static] %0 : $*(CTensorHandle, CTensorHandle)
  access = B.createBeginAccess(loc, stackAlloc, SILAccessKind::Modify,
                               SILAccessEnforcement::Static,
                               /*noNestedConflict=*/false,
                               /*fromBuiltin=*/false);

  // %2 = tuple_element_addr %1 : $*(CTensorHandle, CTensorHandle), 0
  firstPtr = access;
  if (resultValues.size() >= 2)
    firstPtr = B.createTupleElementAddr(loc, firstPtr, 0,
                                        cTensorHandleSILTy.getAddressType());
  // %3 = address_to_pointer %2 : $*CTensorHandle to $Builtin.RawPointer
  firstPtr =
      B.createAddressToPointer(loc, firstPtr, SILType::getRawPointerType(ctx));

  // %4 = struct $UnsafeMutablePointer<CTensorHandle>(%3: $Builtin.RawPointer)
  firstPtr = B.createStruct(loc, unsafeMutPointerSILType, firstPtr);

  auto finishComputationFnRef = B.createFunctionRef(loc, finishComputationFn);

  // Create the builtin in the host program that kicks off the tensor program,
  // setting the argument values.
  B.createApply(loc, finishComputationFnRef, /*no substitutions*/ {},
                /*args*/ {tensorComputation, firstPtr, numTensorResults},
                /*isNonThrowing*/ false);

  // end_access %1 : $*(CTensorHandle, CTensorHandle)
  B.createEndAccess(loc, access, /*aborted*/ false);

  // After the call, we have a buffer filled in with CTensorHandle values, load
  // them, taking ownership and RAUW'ing uses of the old value to the newly
  // loaded value.
  for (unsigned resultNumber = 0, e = resultValues.size(); resultNumber != e;
       ++resultNumber) {
    SILValue result = resultValues[resultNumber];
    SILValue eltAddress = stackAlloc;
    if (resultValues.size() > 1) {
      eltAddress =
          B.createTupleElementAddr(result.getLoc(), eltAddress, resultNumber,
                                   cTensorHandleSILTy.getAddressType());
    }

    // The load takes ownership from the buffer, leaving the buffer
    // uninitialized again.
    SILValue newValue =
        B.createLoad(result.getLoc(), eltAddress, loadOwnership);

    // Create a new TensorHandle<T> type to take ownership.
    auto newTH = B.createAllocRef(result.getLoc(), result->getType(),
                                  /*objc*/ false, /*canAllocOnStack*/ false,
                                  /*elementTypes*/ {},
                                  /*elementCountOperands*/ {});
    auto baseTH = B.createUpcast(loc, newTH, anyTensorHandleSILTy);
    auto fieldAddress =
        B.createRefElementAddr(result.getLoc(), baseTH, tensorHandleMember);

    B.createStore(result.getLoc(), newValue, fieldAddress, storeOwnership);

    // After moving the program region between tensor start and end points to
    // the accelerator, we want to restore the retain/release balance of
    // `result` in the host program at the tensor end point.
    balanceRetainReleaseCount(result, newTH, B);

    // Manually walk the use list in a custom way to avoid invalidating the
    // iterator as we potentially change it.
    for (auto UI = result->use_begin(), UE = result->use_end(); UI != UE;) {
      auto *operand = *UI++;
      auto user = operand->getUser();

      // Users may be either inside (e.g. another tensor op, or a non-tensor op
      // that causes a copy back to the host) or outside the tensor program.  If
      // it is after the tensor op, we can replace the use with the
      // corresponding result value.  If inside, we'll track its retain/release
      // balance, and then nuke it later.
      if (DI.dominates(tensorEndPoint, user)) {
        operand->set(newTH);
        continue;
      }
    }
  }

  // Now that we are done with the buffer of results, get rid of it.  It is
  // uninitialized at this point, so it should not be destroyed.

  // dealloc_stack %0 : $*(CTensorHandle, CTensorHandle)
  B.createDeallocStack(loc, stackAlloc);

  // tensorComputation is the return value of _TFCStartTensorComputation.  By
  // construction, it is known to dominate all abort points and the finish point
  // point of the program.
  //
  // If the host program reaches any of the tensorKillBlocks, it should abort
  // execution of the tensor program.
  for (auto *killBB : tensorKillBlocks) {
    B.setInsertionPoint(&killBB->front());

    auto terminateComputationFnRef =
        B.createFunctionRef(loc, terminateComputationFn);

    // Create the builtin in the host program that kicks off the tensor program,
    // setting the argument values.
    B.createApply(loc, terminateComputationFnRef, /*no substitutions*/ {},
                  /*args*/ {tensorComputation}, /*isNonThrowing*/ false);
  }

  tensorProgram = {programPlaceholder, programLengthPlaceholder,
                   entryFunctionBaseNamePlaceholder,
                   helperFunctionCountPlaceholder, tensorComputation};
}

bool TFFunctionPartition::partitionAndLowerGraph(bool isTest) {
  // If partition() returns true, short circuit the execution and error out.
  return partition(isTest) || lowerGraph(isTest);
}

/// Return true if a user instruction is returning or forming a return value.
static bool isReturning(SILInstruction *user) {
  if (isa<ReturnInst>(user)) return true;
  if (auto *TI = dyn_cast<TupleInst>(user)) {
    return llvm::all_of(TI->getUses(), [&](Operand *use) {
      return isReturning(use->getUser());
    });
  }
  return false;
}

/// Run the TensorFlow partitioning pass.  This pass is a very close relative to
/// the standard "Aggressive Dead Code Elimination" (ADCE) optimization which is
/// implemented using post-dominance frontiers and control dependence
/// information, but instead of determining live code, we're determining
/// operations and a subset of the CFG that is profitable and interesting to
/// move to the accelerator.
///
/// If it returns true, an error has occurred, and diagnostics has been
/// issued. Otherwise, it sets `acceleratorFn` to an accelerator function
/// generated by this call, to be lowered to a TF graph.
///
bool TFFunctionPartition::partition(bool isTest) {
  assert(!markedBlocks.empty() &&
         "Shouldn't run on functions with no tensor ops");

  // Start by determining the result values produced by the tensor program.
  // These will be returned by the tensor end point instead of being sent back
  // from the accelerator to the host.
  SmallVector<SILValue, 4> resultValues;

  // Walk the function in a determinstic order to populate resultValues.
  for (auto *block : llvm::depth_first(&hostFn)) {
    for (auto &inst : *block) {
      auto it = markedInstructions.find(&inst);

      // We only care about values that are being moved to the accelerator.  If
      // they are being copied over, we can just use the original value computed
      // on the host.
      if (it == markedInstructions.end() || it->second != Marking::Move)
        continue;

      // Scan all the users of return values of the instructions moved over.  If
      // any of the results cannot be handled with a result, then we just send
      // the whole value.
      bool hasAnyNonResultUse = false, hasAnyUse = false;
      for (auto result : inst.getResults()) {
        assert(!isa<TupleType>(result->getType().getASTType()) &&
               "No result value of a graph_op inst should be a tuple!");

        for (auto *operand : result->getUses()) {
          auto user = operand->getUser();

          // If the user is to be deleted or moved, then we can safely ignore
          // it.
          auto it = markedInstructions.find(user);
          if (it != markedInstructions.end() &&
              (it->second == Marking::Delete || it->second == Marking::Move))
            continue;

          // Ignore retain/release and debugging instructions.  They can all
          // be removed if the other users are.
          if (isUserIgnoredByPartitioning(user))
            continue;

          // If the user is an unconditional branch instruction, and the value
          // is feeding a basic block argument that is being moved or deleted,
          // then we can ignore it.
          if (auto *branch = dyn_cast<BranchInst>(user)) {
            auto opNumber = operand->getOperandNumber();
            auto arg = branch->getDestBB()->getArgument(opNumber);
            auto argInfo = markedBBArguments.find(arg);
            if (argInfo != markedBBArguments.end() &&
                (argInfo->second.first == Marking::Move ||
                 argInfo->second.first == Marking::Delete))
              continue;
          }

          // If it's an opaque handle such as VariantHandle or ResourceHandle,
          // it cannot be a result except when it's being returned in an
          // accelerator-only function.
          if (!TFSendRecvOpaqueHandle && isOpaqueHandle(result->getType()) &&
              !(isAcceleratorOnly(hostFn) && isReturning(user))) {
            diagnoseOpaqueHandleCopy(result, user);
            return true;
          }

          // Remember if the instruction has any use.  If not, then it never
          // needs to be sent or returned.
          hasAnyUse = true;

          // If the end point dominates the out-of-model use, then we can
          // represent it with the return value of the tensor program.
          // Otherwise, it will turn into a send of data back to the host.
          if (!sinkValueAfterEndPoint(user, tensorEndPoint, DI)) {
            hasAnyNonResultUse = true;
            break;
          }
        }
      }
      // If all of the results can be handled with return values, then handle
      // them that way by appending them to our result list.
      if (hasAnyUse && !hasAnyNonResultUse)
        resultValues.append(inst.getResults().begin(), inst.getResults().end());
    }
  }

  if (auto *outs = getTFDumpIntermediateStream()) {
    *outs << "\n---- PARTITION STATE FOR FUNCTION " << hostFn.getName()
          << " ----------\n";
    *outs << "(Possibly updated) tensor end point: ";
    tensorEndPoint->print(*outs);
    *outs << "There are " << resultValues.size() << " result values:\n";
    for (auto &resultValue : resultValues) {
      resultValue->print(*outs);
    }
    *outs << "---- END OF PARTITION STATE FOR FUNCTION ----------\n\n";
    outs->flush();
  }

  if (!isAcceleratorOnly(hostFn)) {
    // TODO(b/111123797): Lift this restriction.
    if (resultValues.empty() &&
        deviceInfo.primaryDeviceType == DeviceType::TPU) {
      diagnose(hostFn.getASTContext(), hostFn.getLocation().getSourceLoc(),
               diag::tfop_incorrect_definition,
               "TPU execution cannot yet handle a graph that produces no "
               "result tensors.");
      return true;
    }

    // Insert the start/finish and any terminate runtime calls.
    insertTensorComputationStartEndTerminate(resultValues);

    // If the TensorFlow module is malformed, bail out without breaking the
    // code.
    if (!tensorProgram.theTensorComputation)
      return true;
  }

  // Calculate the parameter list for the new function.
  SmallVector<SILParameterInfo, 4> params;
  for (auto v : tensorFnArguments) {
    auto argTy = convertElementTypeToTensorValueType(v->getType());
    params.push_back(SILParameterInfo(argTy.getASTType(),
                                      ParameterConvention::Direct_Unowned));
  }

  SmallVector<SILResultInfo, 4> results;
  for (auto r : resultValues)
    results.push_back(
        SILResultInfo(r->getType().getASTType(), ResultConvention::Unowned));

  // Create the partitioned function, which never has arguments or result
  // values, since they get sent and received back and forth.
  auto newFnType = SILFunctionType::get(
      /*genericSig*/ nullptr, SILFunctionType::ExtInfo(),
      SILCoroutineKind::None, ParameterConvention::Direct_Owned, params,
      /*interfaceYields*/ {}, results, /*interfaceErrorResult*/ None,
      hostFn.getModule().getASTContext());
  SILOptFunctionBuilder FB(transform);
  auto resultFn = FB.getOrCreateFunction(
      hostFn.getLocation(), hostFn.getName().str() + ".tf", SILLinkage::Private,
      newFnType,
      /*What's this*/ IsBare, IsNotTransparent, IsNotSerialized);
  SWIFT_DEFER {
    // If we error out before assigning `resultFn` to the member field
    // `acceleratorFn`, we should make sure this synthensized function is
    // removed, to avoid it going through the normal compiler flow.
    if (resultFn) {
      transform.getPassManager()->notifyWillDeleteFunction(resultFn);
      resultFn->getModule().eraseFunction(resultFn);
    }
  };

  PartitionCloner PC(*this, *resultFn, tensorProgram.theTensorComputation,
                     tensorFlowModule);

  // Fill in the cloned function body.
  PC.cloneFunction(resultValues);

  // Clean up the source function, removing the tensor code.
  if (!isAcceleratorOnly(hostFn) && !PC.finalizeOriginal())
    return true;

  // Success!
  acceleratorFn = resultFn;
  resultFn = nullptr;

  // Our partitioning can leave around lots of unconditional branches between
  // blocks that formerly had control edges.  Go through and merge those to
  // make later passes simpler.
  contractUncondBranches(acceleratorFn, /*DI*/ nullptr, /*LI*/ nullptr);

  if (auto outs = getTFDumpIntermediateStream()) {
    *outs << "--- TFPartition Accelerator Result: " << acceleratorFn->getName()
          << "\n";
    acceleratorFn->print(*outs);
    *outs << "----\n";
    outs->flush();
  } else if (isTest) {
    llvm::outs() << "--- TFPartition Accelerator Result: "
                 << acceleratorFn->getName() << "\n";
    acceleratorFn->print(llvm::outs());
    llvm::outs() << "----\n";
    llvm::outs().flush();
  }

#ifndef NDEBUG
  // Verify that the generated function is ok.
  acceleratorFn->verify();
#endif

  return false;
}

bool TFFunctionPartition::lowerGraph(bool isTest) {
  assert(acceleratorFn);
  if (isAcceleratorOnly(hostFn)) {
    assert(deviceInfo.numUsedDeviceTypes == 1 &&
           "An accelerator-only SIL function must be lowered to a single TF "
           "device.");
    if (graphLowering->lowerTFFunction(hostFn.getName(), acceleratorFn,
                                       deviceInfo))
      return true;

    // Remove the host function so it doesn't go through the normal
    // compiler flow.
    transform.getPassManager()->notifyWillDeleteFunction(&hostFn);
    hostFn.getModule().eraseFunction(&hostFn);
  } else {
    if (graphLowering->lowerTFGraph(hostFn.getName(), acceleratorFn,
                                    deviceInfo))
      return true;
  }

  return false;
}

void TFFunctionPartition::finalizeHostFunction(const std::vector<char> &bytes,
                                               StringRef entryFnBaseName) {
  assert(!entryFnBaseName.startswith("$"));

  if (isAcceleratorOnly(hostFn))
    return;

  // Now that we know what the tensor program actually is, we can replace
  // the placeholder instructions for the data + length with the actual bits
  // we want to use.
  // This effectively emits the encoded graph as a global symbol.
  SILBuilder B(tensorProgram.programPlaceholder);
  auto data = B.createStringLiteral(hostFn.getLocation(),
                                    StringRef(bytes.data(), bytes.size()),
                                    StringLiteralInst::Encoding::Bytes);
  auto len = B.createIntegerLiteral(
      hostFn.getLocation(), tensorProgram.programLengthPlaceholder->getType(),
      bytes.size());

  auto name =
      B.createStringLiteral(hostFn.getLocation(), StringRef(entryFnBaseName),
                            StringLiteralInst::Encoding::UTF8);
  // Set the number of helper functions here based on the # devices involved
  // in this TF program.
  auto helperFunctionCount = B.createIntegerLiteral(
      hostFn.getLocation(),
      tensorProgram.helperFunctionCountPlaceholder->getType(),
      deviceInfo.numUsedDeviceTypes - 1);
  tensorProgram.programPlaceholder->replaceAllUsesWith(data);
  tensorProgram.programPlaceholder->eraseFromParent();
  tensorProgram.programLengthPlaceholder->replaceAllUsesWith(len);
  tensorProgram.programLengthPlaceholder->eraseFromParent();
  tensorProgram.entryFunctionBaseNamePlaceholder->replaceAllUsesWith(name);
  tensorProgram.entryFunctionBaseNamePlaceholder->eraseFromParent();
  tensorProgram.helperFunctionCountPlaceholder->replaceAllUsesWith(
      helperFunctionCount);
  tensorProgram.helperFunctionCountPlaceholder->eraseFromParent();

  if (auto outs = getTFDumpIntermediateStream()) {
    *outs << "--- TFPartition Host Result: " << hostFn.getName() << "\n";
    hostFn.print(*outs);
    *outs << "---\n";
    outs->flush();
  }
}

//===----------------------------------------------------------------------===//
//                              Top Level Driver
//===----------------------------------------------------------------------===//

namespace {
class TFPartition : public SILModuleTransform {
  ModuleDecl *tfModule = nullptr;

  bool isTest = false;
  TensorFunctionClassifier tfc;
  /// For partitionable functions, map the SIL host function names to the
  /// lowered TF graph artifacts.
  ///
  /// Impl note: The value type is a unique_ptr to avoid having to moving the
  /// underlying `LoweredGraphFunction` object once it's constructed, because
  /// the map key is backed by the string field
  /// `LoweredGraphFunction::silHostFnName` in that object.
  DenseMap<StringRef, std::unique_ptr<LoweredGraphFunction>> graphFunctions;

  // Mutually exclusive, based on flag value TFModuleLevelGraph.
  TFGraphLowering moduleGraphLowering; // Used when the flag is true.
  SmallVector<TFGraphLowering, 16> testGraphLowerings; // Used when false.

  using HostPartitionContext =
      std::pair<SILFunction *, std::unique_ptr<TFFunctionPartition>>;

public:
  TFPartition(bool isTest)
      : isTest(isTest),
        moduleGraphLowering(TFGraphLowering(*this, graphFunctions)) {}

  /// The entry point to the transformation.
  void run() override;

private:
  /// Partition and lower `hostFn` if it's eligible. Add an entry to
  /// `hostPartitionContexts`, if it references some other function F as a
  /// function-typed attribute, but the graph function definition of F is not
  /// yet available.
  bool
  processFunction(SILFunction *hostFn,
                  SmallVectorImpl<HostPartitionContext> &hostPartitionContexts);

  /// Partition and lower `hostFn` into a graph if it's eligible. In that case
  /// set `partitioner` to the created partitioner instance to continue with
  /// graph serialization later, or otherwise set `partitioner` to NULL.  Return
  /// true on error.
  bool partitionFunction(SILFunction *hostFn,
                         std::unique_ptr<TFFunctionPartition> &partitioner);
};
} // end anonymous namespace

void TFPartition::run() {
  SILModule *module = getModule();
  auto &ctx = module->getASTContext();
  // TODO(clattner): This logic will eventually be subsumed by the
  // corresponding logic in the TFDeabstraction pass.  Until deabstraction
  // is done, we have some amount of top-level redundancy here because we have
  // to run partitioning after the optimization passes.

  // If the TensorFlow module hasn't been imported by the program, don't do
  // anything.  This avoids impacting compile time for non-TensorFlow using
  // Swift programs by doing extraneous analysis.
  tfModule = ctx.getLoadedModule(ctx.Id_TensorFlow);
  if (!tfModule)
    return;

  // We use a two-pass design below. The first pass partitions and lowers those
  // partitionable functions into graphs. When we process a function that
  // references in function-typed attributes other functions that are not yet
  // lowered, keep track of them in `hostPartitionContexts`, to be processed in
  // the second pass.
  //
  // Loop over all of the functions defined in the current module. Since we are
  // synthesizing new functions into the module (those accelerator functions),
  // we make a snapshot of the functions to be processed into `fns`, and then
  // start the processing.
  SmallVector<SILFunction *, 16> fns;
  for (auto &fn : *module) {
    if (!fn.isDefinition())
      continue;
    fns.push_back(&fn);
  }

  // Track the set of functions that reference some function-typed attributes
  // whose graph function definitions are not yet available.
  SmallVector<HostPartitionContext, 16> hostPartitionContexts;
  for (auto *fn : fns)
    if (processFunction(fn, hostPartitionContexts))
      continue; // error already emitted, but continue processing other
                // functions.

  if (hostPartitionContexts.empty() || !TFModuleLevelGraph)
    return;

  // In the second pass, we know all partitionable functions have had their
  // graph function bodies generated. When a function foo() references another
  // function bar() as a function-typed attribute, but bar()'s graph function
  // body is not avilable during the first pass, we will be copying the graph
  // function body of bar() into the graph function body of foo() in this pass.

  std::vector<char> bytes;
  auto errorLoc = hostPartitionContexts.front().first->getLocation();
  if (TFModuleLevelGraph &&
      moduleGraphLowering.serializeGraphProtoBuf(ctx, errorLoc, bytes))
    return; // error already emitted

  for (auto &context : hostPartitionContexts) {
    auto *hostFn = context.first;
    auto *partitioner = context.second.get();
    assert(graphFunctions.count(hostFn->getName()));
    auto &thisGraphFunc = *graphFunctions[hostFn->getName()];

    // TODO: Currently the byte buffer is local to the host function, so
    // multiple partitionable functions that share the same graphDef could have
    // replicated byte buffers. Can move to a global byte buffer if this helps
    // with performance.
    partitioner->finalizeHostFunction(bytes, thisGraphFunc.graphFnName);
  }
}

bool TFPartition::processFunction(
    SILFunction *hostFn,
    SmallVectorImpl<HostPartitionContext> &hostPartitionContexts) {
  // If something crashes, make sure the pretty stack trace says what we
  // were doing.
  llvm::PrettyStackTraceFormat X("TFPartition on function %s",
                                 hostFn->getName().str().c_str());

  std::unique_ptr<TFFunctionPartition> partitioner;
  if (partitionFunction(hostFn, partitioner)) {
    return true; // error already emitted.
  } else if (partitioner) {
    if (!TFModuleLevelGraph) {
      // In this test mode, for code simplicity we eagerly serialize the graph
      // def, without first copying over any graph functions that this graph
      // function depends on (in its function-typed attributes), so this test
      // mode is only robust when there are no function-typed attributes.
      assert(graphFunctions.count(hostFn->getName()));
      auto &thisGraphFunc = *graphFunctions[hostFn->getName()];

      std::vector<char> bytes;
      auto errorLoc = hostFn->getLocation();
      if (partitioner->getGraphLoweringTest()->serializeGraphProtoBuf(
              getModule()->getASTContext(), errorLoc, bytes))
        return true; // error already emitted
      partitioner->finalizeHostFunction(bytes, thisGraphFunc.graphFnName);

      // Reset the tensor id counter for host-TF sends/recvs, so that each test
      // starts with a nice 0 as the id.
      nextSendID = 0;
    } else {
      // The lowered graph references some other graph functions whose
      // bodies are not yet available -- track it for continued processing
      // in the second pass.
      hostPartitionContexts.push_back(
          std::make_pair(hostFn, std::move(partitioner)));
    }
  }

  // If this is called from sil-opt, we currently just print out the results
  // and quit.  This allows writing regression tests for the tf-partition
  // pass in isolation.  This is pretty unconventional for a SIL pass, but
  // this is an unconventional pass!
  if (isTest) {
    llvm::outs() << "--- TFPartition Host Result: " << hostFn->getName()
                 << "\n";
    hostFn->print(llvm::outs());
    llvm::outs() << "---\n";
    llvm::outs().flush();
  }
  return false;
}

bool TFPartition::partitionFunction(
    SILFunction *hostFn, std::unique_ptr<TFFunctionPartition> &partitioner) {
  if (isAcceleratorOnly(*hostFn)) {
    LLVM_DEBUG(llvm::dbgs() << "SIL function " << hostFn->getName()
                            << " is accelerator-only.\n");
  }

  // If this function is a building block of larger tensor programs (e.g.
  // the ops defined in the TensorFlow module), then don't transform it in
  // isolation.
  LLVM_DEBUG(llvm::dbgs() << "Processing SIL function " << hostFn->getName()
                          << " in TFPartition::partitionFunction().\n");

  if (!tfc.shouldBePartitioned(hostFn, /*forceTFFunctions=*/true))
    return false;
  
  if (llvm::TFDynamicCompilation && !isAcceleratorOnly(*hostFn))
    return false;

  LLVM_DEBUG(llvm::dbgs() << "  " << hostFn->getName()
                          << " should be partitioned.\n");
  TFGraphLowering *graphLowering = &moduleGraphLowering;
  if (!TFModuleLevelGraph) {
    testGraphLowerings.emplace_back(TFGraphLowering(*this, graphFunctions));
    graphLowering = &testGraphLowerings.back();
  }
  partitioner = llvm::make_unique<TFFunctionPartition>(
      *this, *hostFn, PM, *tfModule, graphLowering, graphFunctions);

  bool hasTensorOps = false;
  if (partitioner->markFunction(hasTensorOps))
    return true; // We've encountered an error.
  if (!hasTensorOps) {
    partitioner = nullptr;
    return false;
  }

  LLVM_DEBUG(llvm::dbgs() << "  " << hostFn->getName()
                          << " contains tensor op(s).\n");

  // Check to see if we cannot transform the function but should.  In this
  // case we emit a compiler error.  This is a limitation of the compiler that
  // will need to be resolved in the future (possibly through a model change),
  // it's not clear if we should allow partitioning to work on unspecialized
  // generics.
  if (hostFn->getLoweredFunctionType()->isPolymorphic()) {
    auto &ctx = hostFn->getASTContext();
    diagnose(ctx, hostFn->getLocation().getSourceLoc(), diag::tf_internal_error,
             "TensorFlow graph program extraction does not work on generic "
             "functions yet");
    return true;
  }

  // Because we're in active development, it is common to do something wrong
  // in the TensorFlow module.  Detect and reject things here.
  if (hostFn->getModule().getSwiftModule() == tfModule) {
    auto &ctx = hostFn->getASTContext();
    diagnose(ctx, hostFn->getLocation().getSourceLoc(), diag::tf_internal_error,
             "nothing in the TensorFlow module should require partitioning, "
             "did you forget @inlinable on '" +
                 hostFn->getName().str() + "'?");
    return true;
  }

  // Actually do the partitioning transformation, splitting out a new SIL
  // function for the tensor program body.
  // If we've encountered an error, exit without breaking the SIL:
  // an error has already been emitted.
  return partitioner->partitionAndLowerGraph(isTest);
}

SILTransform *swift::createTFPartition() {
  return new TFPartition(/*isTest*/ false);
}

/// Create a version of the tf-partition pass that is used by sil-opt for
/// testcases.  TF-Partition is not a normal pass, so we need an unconventional
/// approach here.
SILTransform *swift::createTFPartitionTest() {
  return new TFPartition(/*isTest*/ true);
}
