//===--- TFPartition.cpp - Split Tensor ops out of mainline flow ----------===//
//
// This source file is part of the Swift.org open source project
//
// Copyright (c) 2014 - 2017 Apple Inc. and the Swift project authors
// Licensed under Apache License v2.0 with Runtime Library Exception
//
// See https://swift.org/LICENSE.txt for license information
// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors
//
//===----------------------------------------------------------------------===//
//
// This pass splits tensor operations out into seperate functions - one per
// TensorFlow graph that is generated by the TFLowerGraph functionality.
//
//===----------------------------------------------------------------------===//

#define DEBUG_TYPE "tf-partition"
#include "TFUtilities.h"
#include "swift/SILOptimizer/PassManager/Passes.h"
#include "swift/SILOptimizer/PassManager/Transforms.h"
#include "swift/SILOptimizer/Analysis/DominanceAnalysis.h"
#include "swift/SIL/CFG.h"
#include "swift/SIL/DebugUtils.h"
#include "swift/SIL/SILArgument.h"
#include "swift/SIL/SILCloner.h"
#include "swift/AST/DiagnosticsSIL.h"
#include "swift/AST/Expr.h"
#include "llvm/ADT/BitVector.h"
#include "llvm/ADT/DepthFirstIterator.h"
#undef DEBUG_TYPE
#include "llvm/Support/GenericDomTreeConstruction.h"
#define DEBUG_TYPE "tf-partition"
using namespace swift;
using namespace tf;
using llvm::DenseMap;


template<typename...T, typename...U>
static InFlightDiagnostic
diagnose(ASTContext &Context, SourceLoc loc, Diag<T...> diag, U &&...args) {
  return Context.Diags.diagnose(loc, diag, std::forward<U>(args)...);
}

/// Returns true if the partitioning pass should ignore this user.
static bool isUserIgnoredByPartitioning(SILInstruction *inst) {
  return isa<DebugValueInst>(inst) || isa<RefCountingInst>(inst);
}

/// Given a decl for a struct that has a single field (typically because it is
/// known to be a standard library type like Int or Float), return the canonical
/// type of the single member, asserting and aborting if we get something
/// unexpected.
static CanType getSingleElementDeclFieldType(NominalTypeDecl *decl) {
  auto fieldIt = decl->getStoredProperties().begin();
  assert(fieldIt != decl->getStoredProperties().end() &&
         "Struct should have one member");
  auto fieldType = (*fieldIt++)->getType()->getCanonicalType();
  assert(fieldIt == decl->getStoredProperties().end() &&
         "Struct should have one member");
  return fieldType;
}

/// Classification of instructions that are interesting to the partitioning
/// pass for various reasons.
enum class PartitioningClass {
  Unknown,

  /// This is an apply instruction of the __tf_get_scalar_or_die family.
  /// Its result is a scalar, operand #0 is the callee function_ref, operand #1
  /// is a TensorHandle, and operand #2 is a metatype because these are defined
  /// as methods.
  GetScalarOrDie,

  /// This is an apply instruction of the __tf_hoistable family.  While it may
  /// appear that these functions have side effects, it is well known that they
  /// may be hoisted above the start of the function safely ignoring them.
  Hoistable,

  /// Scalar instructions that check for overflow like "sadd.with.overflow" and
  /// friends.
  OverflowCheckingInst,
};

static PartitioningClass classifyInst(SILInstruction *inst) {
  if (auto *BI = dyn_cast<BuiltinInst>(inst)) {
    switch (BI->getBuiltinInfo().ID) {
    default: return PartitioningClass::Unknown;
    case BuiltinValueKind::UAddOver:
    case BuiltinValueKind::SAddOver:
    case BuiltinValueKind::USubOver:
    case BuiltinValueKind::SSubOver:
    case BuiltinValueKind::UMulOver:
    case BuiltinValueKind::SMulOver:
      return PartitioningClass::OverflowCheckingInst;
    }
  }

  // Classify well-known functions defined in the TensorFlow module.
  if (auto *apply = dyn_cast<ApplyInst>(inst)) {
    if (auto fn = apply->getCalleeFunction()) {
      if (fn->getName().startswith("__tf_get_scalar_or_die_"))
        return PartitioningClass::GetScalarOrDie;
      if (fn->getName().startswith("__tf_hoistable_"))
        return PartitioningClass::Hoistable;
    }
  }

  return PartitioningClass::Unknown;
}


/// Given an overflow-checking integer operation, return true if the overflow
/// result will be unused in the tensor program.  This could be because the
/// overflow result is already dead, because it is extracted but the extract
/// isn't used, or if the result is only used by code that provably won't make
/// it into the tensor program.
static bool canPromoteOverflowCheckingInstToTensorProgram(BuiltinInst *BI) {
  // Annoyingly, these builtins are modeled as returning a tuple, which then
  // has tuple extracts hanging off of it.
  for (auto *use : BI->getUses()) {
    auto *extract = dyn_cast<TupleExtractInst>(use->getUser());
    if (!extract) return false;

    // If this is a use of the normal result of the call, then we can ignore
    // it - this can be moved to the tensor program.
    if (extract->getFieldNo() == 0)
      continue;
    assert(extract->getFieldNo() == 1 && "Overflowing ops only have 2 results");

    // Check all uses of the TupleExtract.  If any of them are going to end up
    // in the tensor program, then we cannot move it.
    for (auto *overflowUse : extract->getUses()) {
      auto overflowUser = overflowUse->getUser();
      // CondFail won't get moved over.
      if (isa<CondFailInst>(overflowUser))
        continue;
      return false;
    }
  }
  return true;
}


/// When classifying a builtin operation as a scalar to be promoted to a tensor,
/// this returns the kind of operation it is.
enum class PromotedScalarKind {
  Invalid,
  Literal,
  TensorToScalar,
  Conversion,
  Binary,
  OverflowingBinary,
};


/// If the specified scalar operation can be partitioned and run on the
/// accelerator, return the name of the op to use to implement it.
static std::pair<const char*, PromotedScalarKind>
classifyPromotedScalarOp(SILInstruction *inst) {
  // We can turn integer and FP literals into constant nodes if their type is
  // compatible.
  if (isa<IntegerLiteralInst>(inst) || isa<FloatLiteralInst>(inst)) {
    auto resultTy = inst->getResults()[0]->getType();
    if (isValidTensorFlowElementType(resultTy.getSwiftRValueType()))
      return { "Const", PromotedScalarKind::Literal };
  }

  auto instClass = classifyInst(inst);

  // __tf_get_scalar_or_die cannot be marked as having no side effects
  // because it takes a +1 value as its argument.  That said, it is safe to
  // hoist and sink it.
  if (instClass == PartitioningClass::GetScalarOrDie)
    return { "tfc.getScalarOrDie", PromotedScalarKind::TensorToScalar };

  auto *builtin = dyn_cast<BuiltinInst>(inst);
  if (!builtin) return { nullptr, PromotedScalarKind::Invalid };

  // Verify that the instruction is processing dtypes that are supported by
  // TensorFlow nodes - we don't want to handle SIMD vectors or other exotic
  // types.
  if (builtin->getNumOperands() != 0) {
    auto opTy = builtin->getOperand(0)->getType();
    if (!isValidTensorFlowElementType(opTy.getSwiftRValueType()))
      return { nullptr, PromotedScalarKind::Invalid };
  }
  // Verify the result is a valid tensorflow type, or a 2-element tuple that
  // starts with one (used by the overflowing ops).
  if (!isValidTensorFlowElementType(builtin->getType().getSwiftRValueType())) {
    auto *tt = builtin->getType().getSwiftRValueType()->getAs<TupleType>();
    if (!tt || tt->getNumElements() != 2 ||
        !isValidTensorFlowElementType(tt->getElementType(0)))
      return { nullptr, PromotedScalarKind::Invalid };
  }


  auto binary = [&](const char *name)
       -> std::pair<const char*, PromotedScalarKind> {
    return { name, PromotedScalarKind::Binary };
  };

  auto overflowingBinary = [&](const char *name)
       -> std::pair<const char*, PromotedScalarKind> {
    if (!canPromoteOverflowCheckingInstToTensorProgram(builtin))
      return { nullptr, PromotedScalarKind::Invalid };
    return { name, PromotedScalarKind::OverflowingBinary };
  };

  auto conversion = [&]() -> std::pair<const char*, PromotedScalarKind> {
    return { "Cast", PromotedScalarKind::Conversion };
  };

  switch (builtin->getBuiltinInfo().ID) {
  default: return { nullptr, PromotedScalarKind::Invalid };
  // TODO: Unsigned comparisons: ICMP_UGT, ICMP_ULE, ICMP_UGE
  // TODO: FP Comparisons.
  case BuiltinValueKind::ICMP_EQ:  return binary("Equal");
  case BuiltinValueKind::ICMP_NE:  return binary("NotEqual");
  case BuiltinValueKind::ICMP_SLT: return binary("Less");
  case BuiltinValueKind::ICMP_SGT: return binary("Greater");
  case BuiltinValueKind::ICMP_SLE: return binary("LessEqual");
  case BuiltinValueKind::ICMP_SGE: return binary("GreaterEqual");
  case BuiltinValueKind::Add:      return binary("Add");
  case BuiltinValueKind::Sub:      return binary("Sub");
  case BuiltinValueKind::Mul:      return binary("Mul");
  case BuiltinValueKind::FAdd:     return binary("Add");
  case BuiltinValueKind::FSub:     return binary("Sub");
  case BuiltinValueKind::FMul:     return binary("Mul");
  // TODO: UAddOver, USubOver, UMulOver:
  case BuiltinValueKind::SAddOver: return overflowingBinary("Add");
  case BuiltinValueKind::SSubOver: return overflowingBinary("Sub");
  case BuiltinValueKind::SMulOver: return overflowingBinary("Mul");

  // TODO: Unsigned conversions.
  case BuiltinValueKind::SIToFP:
  case BuiltinValueKind::FPToSI:
  case BuiltinValueKind::Trunc:
  case BuiltinValueKind::TruncOrBitCast:
  case BuiltinValueKind::SExt:
  case BuiltinValueKind::SExtOrBitCast:
     return conversion();
  }
}

/// If the specified type is a TensorHandle<T> type, return it.  Otherwise, it
/// must be a primitive type T.  In that case, wrap it to form TensorHandle<T>.
static SILType convertToTensorHandleType(SILType ty) {
  // If this is already TensorHandle<T>, return it.
  if (isTensorHandle(ty.getSwiftRValueType()))
    return ty;

  // Otherwise, this is a valid TensorFlow element type "T", like Builtin.Int32,
  // turn it into a TensorHandle<T> type.
  assert(isValidTensorFlowElementType(ty.getSwiftRValueType()));
  auto decl = ty.getASTContext().getTensorHandleDecl();
  auto tensorType =
    BoundGenericClassType::get(decl, /*parent*/Type(),
                               ty.getSwiftRValueType());

  return SILType::getPrimitiveObjectType(tensorType->getCanonicalType());
}



//===----------------------------------------------------------------------===//
//                  BlocksReachingTensorCode CFG Subset
//===----------------------------------------------------------------------===//

/// These nodes mirrors a CFG subset of the function being partitioned, which
/// makes it easy to reuse the dominator algorithms in LLVM.  While it seems
/// silly to make a clone of a graph to produce a filtered view on it, getting
/// the right view projecting iterators to work is more complicated than it is
/// worth at this point.
///
/// As such, this is a very simple representation, which can be optimized later
/// if it ever becomes a performance problem (doubtful).
///
namespace {
class BlocksReachingTensorCode;
struct SILBBSubsetNode {
  // We care about the addresses of these nodes, so disable these to avoid
  // accidental copies.
  SILBBSubsetNode() = delete;
  SILBBSubsetNode(const SILBBSubsetNode&) = delete;
public:
  SILBasicBlock *BB;
  BlocksReachingTensorCode *Parent;
  SILBBSubsetNode(SILBasicBlock *BB, BlocksReachingTensorCode *Parent)
    : BB(BB), Parent(Parent) {}

  SILBBSubsetNode(SILBBSubsetNode &&rhs) {
    BB = rhs.BB;
    Parent = rhs.Parent;
  }

  // These are predecessors and successors of the CFG subset.
  typedef std::vector<SILBBSubsetNode*> BBListTy;
  BBListTy Predecessors, Successors;


  // Requirements of the domtree implementation.
  BlocksReachingTensorCode *getParent() const { return Parent; }
  void printAsOperand(llvm::raw_ostream &O, bool printType = true) {
    BB->printAsOperand(O, printType);
  }
};
} // end anonymous namespace


namespace llvm {
  template <> struct GraphTraits<SILBBSubsetNode*> {
    using ChildIteratorType = SILBBSubsetNode::BBListTy::iterator;
    using Node = SILBBSubsetNode;
    using NodeRef = SILBBSubsetNode*;

    static ChildIteratorType child_begin(NodeRef N) {
      return N->Successors.begin();
    }
    static ChildIteratorType child_end(NodeRef N) {
      return N->Successors.end();
    }
  };

  template <> struct GraphTraits<Inverse<SILBBSubsetNode*>> {
    using ChildIteratorType = SILBBSubsetNode::BBListTy::iterator;
    using Node = SILBBSubsetNode;
    using NodeRef = SILBBSubsetNode*;
    static ChildIteratorType child_begin(NodeRef N) {
      return N->Predecessors.begin();
    }
    static ChildIteratorType child_end(NodeRef N) {
      return N->Predecessors.end();
    }
  };
} // end namespace llvm



namespace {
/// Represent a subset of a function in a way that we can fulfill the model of
/// the LLVM Graph abstractions, allowing us to get post-dominators for a subset
/// of the nodes and edges in a function.
///
/// A Swift function that contains tensor operations will often have a number
/// of blocks "hanging off it" that represent failure paths: integer overflow
/// checks, precondition failures, etc.  Also, the function may have a
/// significant amount of general computation that happens after the meat of
/// tensor computation happens.
///
/// Unfortunately, these exits blocks in particular will generally pessimize
/// PostDominator information, because the extraneous edges go to blocks that
/// end with unreachable.  This means that we frequently get degenerate blocks
/// where the only post dominator of two related blocks is the exit node for
/// the function, which unifies the blocks that end with 'return' and the
/// blocks that end with 'unreachable'.
///
/// To solve this, we compute the set of blocks that we need to represent in
/// the tensor slice because they can reach tensor computation or a direct use
/// of that computation.  This subset of the function is what we need to
/// generate accurate post dominator info.
///
class BlocksReachingTensorCode {
  friend struct llvm::GraphTraits<BlocksReachingTensorCode*>;
  friend struct llvm::GraphTraits<llvm::Inverse<BlocksReachingTensorCode*>>;

  /// The function this slice is a subset of.
  SILFunction &fn;

  /// These are all of the nodes themselves.
  std::vector<SILBBSubsetNode> nodes;

  /// This map contains all of the SILBBSubsetNode's that make up the subset
  /// graph.
  DenseMap<SILBasicBlock*, SILBBSubsetNode*> nodeMap;

  /// This is the post dominator tree built over our node subset.
  llvm::DominatorTreeBase<SILBBSubsetNode, true> PDI;
public:
  BlocksReachingTensorCode(SILFunction &fn) : fn(fn) {}
  void compute(ArrayRef<SILInstruction*> ops);

  SILBBSubsetNode *getNode(SILBasicBlock *BB) const {
    auto i = nodeMap.find(BB);
    assert(i != nodeMap.end() && i->second && "BasicBlock not in our subset");
    return i->second;
  }

  SILBBSubsetNode *getEntryBlock() const {
    return getNode(fn.getEntryBlock());
  }

  /// Return true if the specified block is in our subset of the function.
  bool contains(SILBasicBlock *bb) const {
    return nodeMap.count(bb);
  }

  bool postDominates(SILBasicBlock *dominator, SILBasicBlock *dominatee) {
    return PDI.dominates(getNode(dominator), getNode(dominatee));
  }

  SILBasicBlock *findNearestCommonPostDominator(SILBasicBlock *B1,
                                                SILBasicBlock *B2) {
    auto res = PDI.findNearestCommonDominator(getNode(B1), getNode(B2));
    return res ? res->BB : nullptr;
  }


  SILBasicBlock *getPostIDom(SILBasicBlock *BB) {
    auto PDINode = PDI[getNode(BB)]->getIDom();
    return PDINode ? PDINode->getBlock()->BB : nullptr;
  }

  void dump();

public:
  // Random stuff used by DominatorTree internals.  Don't use generally.
  SILBBSubsetNode &front() const { return *getEntryBlock(); }
};
} // end anonymous namespace

/// \brief An iterator type that allows iterating over the address of the
/// elements returned via some other iterator.
///
/// \code
///   using iterator = address_iterator<SmallVectorImpl<T>::iterator>;
/// \endcode
template <typename WrappedIteratorT,
          typename T = decltype(&*std::declval<WrappedIteratorT>())>
struct address_iterator
  : llvm::iterator_adaptor_base<
          address_iterator<WrappedIteratorT>, WrappedIteratorT,
          typename std::iterator_traits<WrappedIteratorT>::iterator_category,
          T> {
  address_iterator() = default;
  template <typename U>
  address_iterator(U &&u)
      : address_iterator::iterator_adaptor_base(std::forward<U &&>(u)) {}

  T operator*() const { return &*this->I; }
};

namespace llvm {
  template<>
  struct GraphTraits<BlocksReachingTensorCode*>
                            : public GraphTraits<SILBBSubsetNode*> {
    using GraphType = BlocksReachingTensorCode*;
    using NodeRef = SILBBSubsetNode*;

    static NodeRef getEntryNode(GraphType F) {
      return F->getEntryBlock();
    }

    typedef address_iterator<std::vector<SILBBSubsetNode>::iterator>
                               nodes_iterator;
    static nodes_iterator nodes_begin(GraphType F) {
      return nodes_iterator(F->nodes.begin());
    }
    static nodes_iterator nodes_end(GraphType F) {
      return nodes_iterator(F->nodes.end());
    }

    static unsigned size(GraphType F) { return F->nodes.size(); }
  };

  template<> struct GraphTraits<Inverse<BlocksReachingTensorCode*>>
  : public GraphTraits<Inverse<swift::SILBasicBlock*>> {
    using GraphType = Inverse<BlocksReachingTensorCode*>;
    using NodeRef = SILBBSubsetNode*;

    typedef address_iterator<std::vector<SILBBSubsetNode>::iterator>
                              nodes_iterator;
    static nodes_iterator nodes_begin(GraphType F) {
      return nodes_iterator(F.Graph->nodes.begin());
    }
    static nodes_iterator nodes_end(GraphType F) {
      return nodes_iterator(F.Graph->nodes.end());
    }
    static unsigned size(GraphType F) { return F.Graph->nodes.size(); }
  };
} // end namespace llvm

// This template requires explicit instantiation.
template class llvm::DominatorTreeBase<SILBBSubsetNode, true>;

/// Compute the set of blocks that can reach the specified operations, and
/// uses of them.
void BlocksReachingTensorCode::compute(ArrayRef<SILInstruction*> ops) {
  assert(!ops.empty() && "Cannot compute empty CFG subset");
  SmallVector<SILBasicBlock*, 8> worklist;

  // We seed the worklist with the blocks that the operations occur in, along
  // with the blocks containing all uses of the operands.  These uses may not
  // themselves be tensor results that we partition: but they may be the send
  // operations that cause copy out or function results.
  for (auto *i : ops) {
    auto instBB = i->getParent();
    worklist.push_back(instBB);

    // Add the blocks that any users live in.
    for (auto result : i->getResults()) {
      for (auto user : result->getUses()) {
        if (user->getUser()->getParent() != instBB &&
            !isUserIgnoredByPartitioning(user->getUser()))
          worklist.push_back(user->getUser()->getParent());
      }
    }
  }

  // In addition to blocks with ops in them, we also add any block that has a
  // return, since that will be the head of the post dominator tree, and it is
  // possible that there are no ops in that block.
  for (auto &bb : fn.getBlocks())
    if (isa<ReturnInst>(bb.getTerminator()))
      worklist.push_back(&bb);

  // Make sure the nodes are never reallocated out from under us.
  nodes.reserve(fn.getBlocks().size());

  // Figure out all of the blocks that should be included.
  while (!worklist.empty()) {
    auto *bb = worklist.pop_back_val();

    // If we already visited this block, we're done.  Otherwise insert it,
    // creating the SILBBSubsetNode for this BB.
    auto &entry = nodeMap[bb];
    if (entry != nullptr) continue;

    nodes.emplace_back(SILBBSubsetNode(bb, this));
    entry = &nodes.back();

    // Its predecessors can also reach a tensor op.
    worklist.append(bb->pred_begin(), bb->pred_end());
  }

  assert(nodeMap.count(fn.getEntryBlock()) &&
         "Entry block should be reachable from Tensor work");

  // Now that all of the nodes are created, we can wire up the predecessor and
  // successor lists.
  for (auto &node : nodes) {
    for (auto *succ : node.BB->getSuccessorBlocks()) {
      auto it = nodeMap.find(succ);
      if (it != nodeMap.end())
        node.Successors.push_back(it->second);
    }
    for (auto *pred : node.BB->getPredecessorBlocks()) {
      auto it = nodeMap.find(pred);
      if (it != nodeMap.end())
        node.Predecessors.push_back(it->second);
    }
  }

  // Now that we have our CFG subset, compute the post dominator tree from it.
  PDI.recalculate(*this);
}

void BlocksReachingTensorCode::dump() {
  PDI.print(llvm::errs());
}


//===----------------------------------------------------------------------===//
//                             FunctionPartitioner
//===----------------------------------------------------------------------===//

namespace {
/// Marking values in the host program need to either be moved, copied, or have
/// their results sent over to the accelerator.
enum class Marking {
  Copy,      // This instruction is run on both the host and accelerator.
  Move,      // This instruction is run on the accelerator, not the host.
  Send,      // The value produced by this instruction is copied to accelerator.
  Argument,  // The value is passed as an argument to the tensor function.
  Delete,    // This instruction is simply deleted (e.g. debug_value)
};

class TFFunctionPartition {
public:
  SILFunction &fn;
  ModuleDecl &tensorFlowModule;  // TensorFlow standard library.
  DominanceInfo &DI;
  BlocksReachingTensorCode tensorCodeBlocks;

  /// These are all the tensor ops found in the initial scan over the function.
  SmallPtrSet<SILInstruction*, 8> tensorOpsSet;

  /// This keeps track of the set of blocks that are marked as needing to be
  /// partitioned out to the accelerator.  If the block is in this set, then
  /// some instruction in the block has to run on the accelerator.
  SmallPtrSet<SILBasicBlock*, 8> markedBlocks;

  /// This contains a set of all basic blocks that are immediate successors of
  /// TensorOp blocks, but which are outside of the tensor program (typically
  /// these are error edges).  These blocks need to kill the tensor program if
  /// reached.
  SmallPtrSet<SILBasicBlock*, 8> tensorKillBlocks;

  /// As the primary tensor operations are marked, the nearest common ancestor
  /// (NCA) in the dominator tree of the tensor operations is found.  This will
  /// be the entry block of the tensor computation, and marks the point where
  /// the tensor computation is started (and where arguments are passed).
  ///
  /// Among other things, this trims off the front matter that often ends up at
  /// the beginning of functions.
  ///
  /// The instruction pointed to here is either the first Tensor operation (if
  /// there is one which dominates all other ops) or the terminator of the block
  /// that dominates all of the operations.
  SILInstruction *tensorStartPoint = nullptr;

  /// Similar to the start point, this indicates the first instruction after
  /// the last Tensor operation, which is when the computation should be
  /// completed and results are returned.  If there are no tensor ops in the
  /// final block, then this will be the entry instruction.
  SILInstruction *tensorEndPoint = nullptr;

  /// The values passed as arguments to the tensor function.
  SmallVector<SILValue, 4> tensorFnArguments;

  /// The instructions that are to be run on the accelerator.
  DenseMap<SILInstruction*, Marking> markedInstructions;

  /// BB Arguments that are marked as being moved, copied, or used as arguments.
  /// If a marked argument is moved over, it is deleted from the host program.
  /// If the host also uses the argument, then a copy will have to be inserted
  /// back from the accelerator to the host.
  ///
  /// We capture source location information for BB arguments during the marking
  /// phase, because once we start chopping up instructions we can't reliably
  /// get it.
  DenseMap<SILArgument*, std::pair<Marking, SILLocation>> markedBBArguments;

  /// The set of values that must be sent to the accelerator.
  SmallPtrSet<SILValue, 8> valuesToSend;

  /// Set of all of the __tf_send calls that silence copy-in warnings.
  SmallPtrSet<SILInstruction*, 8> explicitCopyMarkers;
public:
  TFFunctionPartition(SILFunction &Fn, SILPassManager *PM,
                      ModuleDecl &tensorFlowModule)
    : fn(Fn), tensorFlowModule(tensorFlowModule),
      DI(*PM->getAnalysis<DominanceAnalysis>()->get(&Fn)),
      tensorCodeBlocks(Fn) {
  }

  bool markFunction();

  struct PartitionedTensorProgram {
    SILFunction *fn;  // The function representing the tensor program.

    /// These are placeholder instructions inserted during partitioning to
    /// represent the tensor program itself.  These will be replaced when the
    /// tensor function is lowered to a TF graph.
    StringLiteralInst *programPlaceholder;
    IntegerLiteralInst *programLengthPlaceholder;
    StringLiteralInst *entryFunctionNamePlaceholder;

    /// This is the "TensorFlow.TensorComputation" object returned by the
    /// '_swift_tfc_StartTensorComputation' runtime API entrypoint.  This is
    /// returned as null if the tensorflow module is invalid and no
    /// transformation has been made.
    SILValue theTensorComputation;
  };
  PartitionedTensorProgram partition();

  void diagnoseCopyToAccelerator(SILValue value, SILInstruction *user);
  bool diagnoseCopyToHost(SILValue value, SILInstruction *user,
                          SILLocation loc);

private:
  // Marking.
  void markBlock(SILBasicBlock *BB);
  void markInstruction(SILInstruction &inst, Marking mark);
  void markArgument(SILArgument *arg, SILInstruction *user);
  void markValue(SILValue value, SILInstruction *user);
  void sinkValueIntoRegionForPromotion(SILInstruction *&inst);

  void promoteCopyToMove();

  // Rewriting the host function.
  PartitionedTensorProgram
  insertTensorComputationStartEndTerminate(ArrayRef<SILValue> resultValues);
};
} // end anonymous namespace

/// Check to see if the specified value being copied into a partition for the
/// accelerator is our designated "send" operation.  If so, we're fine,
/// otherwise emit a warning to tell the programmer that they are doing
/// something that induces an implicit data transfer into their code.
void TFFunctionPartition::
diagnoseCopyToAccelerator(SILValue value, SILInstruction *user) {
  // If it isn't the result of a "send" operation, then produce a warning about
  // an implicit copy to the accelerator.
  if (auto *apply = dyn_cast<ApplyInst>((SILNode*)value))
    if (auto *callee = dyn_cast<FunctionRefInst>(apply->getCallee()))
      if (callee->getReferencedFunction()->getName().contains("__tf_send")) {
        explicitCopyMarkers.insert(apply);
        return;
      }

  // Try to determine a good source location to report.
  auto loc = getUserSourceLocation(value);

  // Try to make a useful description of the value being copied to help
  // disambiguate.
  std::string description = "value";
  Diag<StringRef> diagID = diag::tf_value_implicitly_copied_to_accel;

  if (auto expr = loc.getAsASTNode<Expr>()) {
    expr = expr->getSemanticsProvidingExpr();

    // Look through LoadExpr's, since they are just part of accesses to mutable
    // values.
    if (auto *load = dyn_cast<LoadExpr>(expr))
      expr = load->getSubExpr()->getSemanticsProvidingExpr();

    // If this is a reference to a class/existential property, diagnose it
    // specifically, as dynamic accesses are never deabstracted.
    if (auto *mre = dyn_cast<MemberRefExpr>(expr)) {
      if (mre->getBase()->getType()->isAnyClassReferenceType()) {
        description = "properties in classes";
        diagID = diag::tf_value_implicitly_copied_to_accel_always;
      } else if (mre->getBase()->getType()->isExistentialType()) {
        description = "properties in dynamic protocols";
        diagID = diag::tf_value_implicitly_copied_to_accel_always;
      }
    }

    if (auto *ae = dyn_cast<ApplyExpr>(expr->getSemanticsProvidingExpr())) {
      if (auto dsc = dyn_cast<DotSyntaxCallExpr>(ae->getFn())) {
        // If this is a call to a class/existential method, diagnose it
        // specifically, as dynamic accesses are never deabstracted.
        if (dsc->getBase()->getType()->isAnyClassReferenceType()) {
          description = "class methods";
          diagID = diag::tf_value_implicitly_copied_to_accel_always;
        } else if (dsc->getBase()->getType()->isExistentialType()) {
          description = "dynamic protocol methods";
          diagID = diag::tf_value_implicitly_copied_to_accel_always;
        } else {
          description = "method result";
        }
      } else if (isa<ConstructorRefCallExpr>(ae->getFn())) {
        description = "'" + expr->getType()->getString() + "'";
      }
    }
  } else if (auto decl = loc.getAsASTNode<Decl>()) {
    if (auto pd = dyn_cast<ParamDecl>(decl))
      description = "'" + pd->getName().str().str() + "'";
  }

  // Emit the warning.
  diagnose(fn.getModule().getASTContext(), loc.getSourceLoc(), diagID,
           description)
    .highlight(loc.getSourceRange());

  // If the use is on a different line, emit a note showing where it is.
  auto userLoc = getUserSourceLocation(user);
  auto &SM = fn.getModule().getASTContext().SourceMgr;

  if (SM.findBufferContainingLoc(loc.getSourceLoc()) !=
        SM.findBufferContainingLoc(userLoc.getSourceLoc()) ||
      SM.getLineNumber(loc.getSourceLoc()) !=
        SM.getLineNumber(userLoc.getSourceLoc())) {
    diagnose(fn.getModule().getASTContext(), userLoc.getSourceLoc(),
             diag::tf_value_implicitly_copied_to_host_computed_used_here)
    .highlight(userLoc.getSourceRange());
  }
}

/// Check to see if the specified value being copied into a partition for the
/// accelerator is our designated "receive" operation.  If so, we're fine,
/// otherwise emit a warning to tell the programmer that they are doing
/// something that induces an implicit data transfer into their code.
bool TFFunctionPartition::
diagnoseCopyToHost(SILValue value, SILInstruction *user, SILLocation loc) {
  // If it isn't the result of a "send" operation, then produce a warning about
  // an implicit copy to the accelerator.
  if (auto *apply = dyn_cast<ApplyInst>(user))
    if (auto *fn = dyn_cast<FunctionRefInst>(apply->getCallee()))
      if (fn->getReferencedFunction()->getName().contains("__tf_receive")) {
        explicitCopyMarkers.insert(apply);
        return false;
      }

  auto &ctx = fn.getModule().getASTContext();

  // Emit the warning.
  diagnose(ctx, loc.getSourceLoc(), diag::tf_value_implicitly_copied_to_host)
    .highlight(loc.getSourceRange());

  // If the use is at a different position, emit a note showing where it is.
  auto userLoc = getUserSourceLocation(user);
  if (loc.getSourceLoc() != userLoc.getSourceLoc()) {
    diagnose(ctx, userLoc.getSourceLoc(),
             diag::tf_value_implicitly_copied_to_host_computed_used_here)
    .highlight(userLoc.getSourceRange());
  }
  return true;
}


/// Some instruction in the specified block needs to be split out to the
/// accelerator, so we mark it (and its control dependencies) as to-be-moved
/// over.
void TFFunctionPartition::markBlock(SILBasicBlock *BB) {
  // "tensorStartPoint" marks the start of the extracted function, so it must
  // dominate all blocks that are to be extracted.
  assert(DI.dominates(tensorStartPoint->getParent(), BB) &&
         "Marking instructions out of the tensor region?");

  // Insert the block into our set - if the block is already there, we have
  // nothing more to do.
  if (!markedBlocks.insert(BB).second)
    return;

  // Walk predecessors until we find marked blocks or other blocks we are
  // control-dependent on.  We only scan the region post dominated by BB.
  //
  // Note that though this is bounded, that it isn't a very efficient algorithm
  // since each block marking can walk the entire function's CFG, but it is good
  // enough for now.  It would probably make more sense to walk the pdom tree
  // instead of walking the CFG.
  SmallVector<SILBasicBlock*, 8> worklist;
  worklist.push_back(BB);

  // The visited set keeps track of blocks that have been added to the worklist,
  // to ensure we don't process something more than once.
  SmallPtrSet<SILBasicBlock*, 32> visited;
  visited.insert(BB);

  // Walk up the CFG looking for terminators we are control-dependent on.
  while (!worklist.empty()) {
    auto thisBB = worklist.pop_back_val();
    assert(tensorCodeBlocks.postDominates(BB, thisBB) &&
           "Should only be scanning the region pdom'd by BB");

    // If we found the start of the region we are extracting, then stop there.
    if (thisBB == tensorStartPoint->getParent())
      continue;

    // Check the predecessors of this block.  If any of them have multiple
    // successors, then we may be control-dependent on that conditional.
    for (auto pred : thisBB->getPredecessorBlocks()) {
      // Count the number of successors of this block which are tensor related.
      // If we see successors that are not tensor related, we'll remember that
      // so we can insert a kill of the tensor program.
      unsigned numTensorSuccs = 0;
      for (auto succ : pred->getSuccessorBlocks()) {
        if (tensorCodeBlocks.contains(succ))
          ++numTensorSuccs;
        else {
          assert(succ->getSinglePredecessorBlock() &&
                 "Need to split critical edges??");
          tensorKillBlocks.insert(succ);
        }
      }

      // Check to see if pred is already visited or if its terminator is already
      // marked then we don't need to reprocess it.
      if (visited.count(pred) ||
          markedInstructions.count(pred->getTerminator()))
        continue;

      // If the predecessor has a single tensor-related successor (us) then we
      // aren't control-dependent on it.  It may be control-dependent on
      // something though.
      if (numTensorSuccs == 1) {
        worklist.push_back(pred);
        continue;
      }

      // Otherwise, check to see if BB is post-dominated by thisBB, but not by
      // pred - in other words that it is the post dominance frontier for BB.
      // If so, that means that the terminator in pred controls whether BB is
      // executed, so it must be marked.
      if (!tensorCodeBlocks.postDominates(BB, pred)) {
        auto predTerm = pred->getTerminator();
        if (isa<BranchInst>(predTerm) || isa<CondBranchInst>(predTerm)) {
          markInstruction(*predTerm, Marking::Copy);
          continue;
        }

        predTerm->dump();
        assert(0 && "FIXME: Handle non-branch terminators like try_apply, which"
               "cannot (in general) be moved to the accelerator");
      }

      // Otherwise, the predecessor is just another block post-dominated by BB,
      // continue walking it.
      worklist.push_back(pred);
      visited.insert(pred);
    }
  }
}


/// When considering whether we should promote a scalar operation to a tensor
/// op in the graph, we have several cases.
namespace {
enum class ScalarPromoteClass {
  NeverPromote,  ///< Do not promote this operation.
  CanPromote,    ///< We can promote this operation, but an argument is also ok.
  ShouldPromote, ///< This is cheaper to run in graph than on the host.
};
} // end anonymous namespace.

/// Determine whether we can promote the specified scalar instruction to a
/// tensor operation in the graph.
static ScalarPromoteClass shouldPromoteToTensorOp(SILInstruction *inst) {
  // We can handle (tuple_extract x, 0) if x is an overflow-checking integer
  // operation.
  if (auto *TE = dyn_cast<TupleExtractInst>(inst)) {
    auto *op = dyn_cast<SILInstruction>((SILNode*)TE->getOperand());
    if (!op || TE->getFieldNo() != 0 ||
        classifyInst(op) != PartitioningClass::OverflowCheckingInst)
      return ScalarPromoteClass::NeverPromote;

    // We can only handle this tuple_extract if the underlying instruction can
    // be handled.  This can depend on dtype support, whether the overflow
    // flag is used, etc.
    return shouldPromoteToTensorOp(op);
  }

  // We can handle (struct_extract x, 0) if x is __tf_get_scalar_or_die.
  if (auto *SE = dyn_cast<StructExtractInst>(inst)) {
    auto *op = dyn_cast<SILInstruction>((SILNode*)SE->getOperand());
    if (op && SE->getFieldNo() == 0 &&
        classifyInst(op) == PartitioningClass::GetScalarOrDie)
      return ScalarPromoteClass::ShouldPromote;
  }

  // Check to see if we know how to promote this to a tensor operation.  If not,
  // we reject it.
  auto scalarClass = classifyPromotedScalarOp(inst).second;
  if (scalarClass == PromotedScalarKind::Invalid)
    return ScalarPromoteClass::NeverPromote;

  // Determine which scalar operations make sense to pull into the graph, even
  // if they are defined above the current start point.  We can always sink them
  // down into the tensor region if we want to, but that's pointless if it just
  // puts scalar computation onto the device for no reason.
  //
  // Cases that are handled here should be handled by
  // sinkValueIntoRegionForPromotion.

  // We prefer to put integer and floating point literals in graph because this
  // allows TensorFlow to do constant propagation.  We prefer to put
  // TensorToScalar into the graph because that avoids a pointless conversion
  // which forces the tensor data itself onto the host.
  if (scalarClass == PromotedScalarKind::Literal ||
      scalarClass == PromotedScalarKind::TensorToScalar)
    return ScalarPromoteClass::ShouldPromote;

  // If this is a conversion from an instruction that should be promoted (like
  // a literal), then try hard to promote the conversion.
  if (scalarClass == PromotedScalarKind::Conversion)
    if (auto *op = dyn_cast<SILInstruction>((SILNode*)inst->getOperand(0)))
      if (shouldPromoteToTensorOp(op) == ScalarPromoteClass::ShouldPromote)
        return ScalarPromoteClass::ShouldPromote;

  // Otherwise, we can promote this if desired.
  return ScalarPromoteClass::CanPromote;
}

void TFFunctionPartition::markInstruction(SILInstruction &inst, Marking mark) {
  // Insert the specified instruction into the marked set.  If it is already
  // there then we have nothing more to do.
  if (!markedInstructions.insert({&inst, mark}).second)
    return;

  // If we're moving a computation to the accelerator, we can remove any
  // debug_value and retain/release instructions using this one.
  if (mark == Marking::Move) {
    for (auto result : inst.getResults())
      for (auto *operand : result->getUses()) {
        auto user = operand->getUser();
        if (isUserIgnoredByPartitioning(user))
          markInstruction(*user, Marking::Delete);
      }
  }

  // If we are simply deleting this instruction, then we're done.
  if (mark == Marking::Delete)
    return;

  // Make sure the instruction's block is marked as being copied over to the
  // tensor program.
  markBlock(inst.getParent());

  // If we have an uncond branch with basic block arguments, don't add operands
  // as used values.  Instead, we'll use more careful conditional liveness
  // based on whether the BB args in the successor are live.
  if (isa<BranchInst>(&inst))
    return;

  // If we are moving the instruction over, then we know it is a tensor op, and
  // it get special attention.  Otherwise, we just recursively mark the
  // operands in question.
  if (mark != Marking::Move) {
    auto operandRange = inst.getAllOperands();

    // Some instructions require special handling during marking.
    switch (classifyInst(&inst)) {
    default: break;  // No special handling.

    case PartitioningClass::OverflowCheckingInst:
      // Overflow-checking integer ops have a "should check" bit as their last
      // parameter, we don't remap it, so don't mark it.
      assert(operandRange.back().get()->getType().is<BuiltinIntegerType>());
      operandRange = operandRange.drop_back();
      break;

    case PartitioningClass::GetScalarOrDie:
      // The __tf_get_scalar_or_die has a callee and a metatype to ignore.
      operandRange = operandRange.drop_front().drop_back();
      break;
    }

    // Scan the operands to make sure they are available: either by moving the
    // computation over to the accelerator, by copying the value over, or by
    // passing as an argument to the tensor computation.
    for (auto &op : operandRange)
      markValue(op.get(), &inst);
    return;
  }

  // Okay, we know that the instruction is a tensor op.  Decode its argument
  // list so we know how to handle the operands.
  SILTensorOpInfo tfopInfo = SILTensorOpInfo::decode(&inst).getValue();
  for (unsigned i = 0, e = inst.getNumOperands(); i != e; ++i) {
    // Tensor and scalar input operands are recursively marked.
    if (tfopInfo.isInput(i) &&
        // Don't mark the array designator.
        !inst.getOperand(i)->getType().is<MetatypeType>())
      markValue(inst.getOperand(i), &inst);
  }
}

void TFFunctionPartition::markArgument(SILArgument *arg, SILInstruction *user) {
  // If we've already marked this argument, there is nothing more to do.
  if (markedBBArguments.count(arg))
    return;

  // Make sure the argument's block is marked as being copied.
  markBlock(arg->getParent());

  // If this BB argument is outside the region dominated by the start point,
  // then we pass its value in as an argument to the tensor function.
  if (!DI.properlyDominates(tensorStartPoint->getParent(), arg->getParent())) {
    markedBBArguments.insert({
      arg, { Marking::Argument, getUserSourceLocation(arg) }
    });
    tensorFnArguments.push_back(SILValue(arg));
    diagnoseCopyToAccelerator(arg, user);
    return;
  }

  // Otherwise, if this is a value of TensorHandle type, then we move it to the
  // accelerator.  If it is also used on the host, it will be copied back.
  if (isTensorHandle(arg->getType().getSwiftRValueType())) {
    // We cannot move over function arguments, but they should never be in the
    // dominated region anyway.
    assert(!isa<SILFunctionArgument>(arg) &&
           "Cannot move function parameters!");
    markedBBArguments.insert({
      arg, {Marking::Move, getUserSourceLocation(arg)}
    });
  } else {
    markedBBArguments.insert({
      arg, { Marking::Copy, getUserSourceLocation(arg) }
    });
  }

  // Otherwise, we mark the branches that contribute values to it, then mark
  // their formal BB argument values that correspond to this argument.
  bool hasUncondBr = false;
  for (auto *pred : arg->getParent()->getPredecessorBlocks()) {
    auto predTerm = pred->getTerminator();
    hasUncondBr |= isa<BranchInst>(predTerm);
    markInstruction(*predTerm, Marking::Copy);
  }

  // We handle conditional liveness of "phi node" like arguments which are set
  // by unconditional branches.  Arguments resulting from other terminators are
  // handled eagerly.
  if (hasUncondBr) {
    SmallVector<SILValue, 4> incomingValues;
    arg->getIncomingValues(incomingValues);
    for (auto v : incomingValues)
      markValue(v, user);
  }
}

/// Determine whether we are able to move the specified instruction across
/// arbitrary other instructions.  This is basically "side effect free" in the
/// most liberal sense.
static bool canMoveInstruction(SILInstruction *inst) {
  // Instructions that SIL knows are always side-effect-free can generally be
  // moved.
  if (inst->getMemoryBehavior() == SILInstruction::MemoryBehavior::None) {
    if (isa<TermInst>(inst))
      return false;
    // Can't hoist allocation and dealloc stacks.
    if (isa<AllocationInst>(inst) || isa<DeallocStackInst>(inst))
      return false;
    return true;
  }

  // Some other instructions have side effects because they take their argument
  // as +1, but we can still move them around.
  if (isa<PartialApplyInst>(inst))
    return true;

  switch (classifyInst(inst)) {
  case PartitioningClass::GetScalarOrDie:
  case PartitioningClass::Hoistable:
    return true;
  default:
    return false;
  }
}

/// The specified instruction is in the region dominated by the start point of
/// the tensor computation and needs to be copied into it.  Try to hoist above
/// the start point, since we prefer arguments to the tensor program rather than
/// send and receives.  This returns true if it successfully hoists the
/// computation or if the value is already above the start point.
static bool hoistValueAboveStartPoint(SILInstruction *inst,
                                      SILInstruction *tensorStartPoint,
                                      DominanceInfo &DI) {
  // If this instruction already dominates the start point, then we're good to
  // go.  Don't move anything.
  if (DI.properlyDominates(inst, tensorStartPoint))
    return true;

  // In general, we need to check to see if we have a chain of side-effect free
  // instructions whose ultimate inputs dominate the start point.
  if (canMoveInstruction(inst)) {
    // We can hoist one of these instructions if all of their operands are
    // hoistable.
    for (auto &op : inst->getAllOperands()) {
      if (auto *opInst = dyn_cast<SILInstruction>((SILNode*)op.get())) {
        if (!hoistValueAboveStartPoint(opInst, tensorStartPoint, DI))
          return false;
      } else if (!DI.properlyDominates(op.get(), tensorStartPoint))
        return false;
    }

    // If all of the operands are hoisted above the start point, then this
    // instruction can be too.
    inst->moveBefore(tensorStartPoint);
    return true;
  }

  // Otherwise, we can't handle this instruction.
  return false;
}

/// The specified instruction is using a value defined in the tensor program.
/// Try to sink it below the end point of the program, since we prefer result
/// values from the tensor program rather than send and receives.  This returns
/// true if it successfully sinks the computation or if the value is already
/// below the end point.
static bool sinkValueAfterEndPoint(SILInstruction *inst,
                                   SILInstruction *&tensorEndPoint,
                                   DominanceInfo &DI) {
  // If this instruction is already dominated by the end point, then we're good
  // to go.  Don't move anything.
  if (DI.dominates(tensorEndPoint, inst))
    return true;

  // In general, we need to check to see if we have a chain of side-effect free
  // instructions whose ultimate results can all be sunk after the endpoint.
  if (canMoveInstruction(inst)) {
    // Make sure the end point is dominated by any operands.
    //
    // TODO: We could make this more aggressive through various techniques, e.g.
    // hoisting instructions to dominate the start point (which is known to also
    // dominate the end points).
    for (auto &op : inst->getAllOperands())
      if (!DI.properlyDominates(op.get(), tensorEndPoint))
        return false;

    for (auto result : inst->getResults())
      for (auto use : result->getUses())
        if (!sinkValueAfterEndPoint(use->getUser(), tensorEndPoint, DI))
          return false;

    // If all of the uses are sunk after the end point, then this
    // instruction can be too.

    // The tensorEndPoint is the first non-tensor instruction in the program.
    // Insert our sunk instruction immediately before it, and this instruction
    // becomes the new end point.
    inst->moveBefore(tensorEndPoint);
    tensorEndPoint = inst;
    return true;
  }

  // Otherwise, we can't handle this instruction.
  return false;
}

/// The specified instruction is known to dominate the start point for the
/// program, and is known to be promotable to a tensor op.   Try to sink it down
/// to be part of the tensor program and return true if successful.
///
void TFFunctionPartition::
sinkValueIntoRegionForPromotion(SILInstruction *&inst) {
  // Determine whether the instruction is only used by things after the
  // tensorStartPoint.  If so, we can move it into place.
  bool hasProblematicUsers = false;
  for (auto result : inst->getResults())
    for (auto *use : result->getUses())
      if (!DI.dominates(tensorStartPoint, use->getUser())) {
        hasProblematicUsers = true;
        break;
      }

  // If all the users are after the start point, we can just move it into place.
  if (!hasProblematicUsers) {
    inst->moveBefore(tensorStartPoint);
    tensorStartPoint = inst;
    return;
  }

  // This instruction has multiple users, some of which are ahead of the
  // tensorStartPoint.  Instead of moving it, we clone it.
  auto oldInst = inst;
  inst = inst->clone(tensorStartPoint);
  inst->setDebugLocation(oldInst->getDebugLocation());
  tensorStartPoint = inst;

  // Replace uses of the original instruction with the new one, if they are
  // within the tensor program.
  for (auto result : oldInst->getResults()) {
    for (auto it = result->use_begin(), e = result->use_end(); it != e; ) {
      auto *operand = *it++;
      auto user = operand->getUser();

      // If the start point dominates this use, replace it.
      if (DI.dominates(tensorStartPoint, user))
        operand->set(cast<SingleValueInstruction>(inst));
    }
  }
}

/// Indicate that the specified value must be available on the accelerator.
/// This can be done by moving the computation over, or by inserting a data
/// transfer.
void TFFunctionPartition::markValue(SILValue value, SILInstruction *user) {
  // We can safely ignore SILUndef, since SILCloner will just make another
  // one for us.
  if (isa<SILUndef>(value))
    return;

  if (auto *arg = dyn_cast<SILArgument>(value))
    return markArgument(arg, user);

  auto *inst = cast<SILInstruction>((SILNode*)value);
  if (markedInstructions.count(inst))
    return;

  // If this is a reference to a tensor op that we haven't gotten to yet, just
  // ignore it.  The outer marking loop will find it and mark it.
  if (tensorOpsSet.count(inst))
    return;

  // Determine whether the instruction is lexically before the tensor program
  // start point, and whether it is something we can promote into the graph.
  bool isBeforeStartPoint =
    !DI.properlyDominates(tensorStartPoint, inst);
  ScalarPromoteClass promotionClass = shouldPromoteToTensorOp(inst);

  // If this is a scalar operation that we really want to promote to a tensor
  // operation, then try to do so.
  if (promotionClass == ScalarPromoteClass::ShouldPromote) {
    // If the value is defined before the program region, is used by the
    // tensor program, and if it is better to have it in the tensor program than
    // for it to be an argument, sink it into the tensor program and mark it.
    // This is useful for constants.  Consider code like this:
    //
    //   x = <opaque value>
    //   y = tensor_literal 1
    //   z = x+y
    //
    // In this case, the operation "+" will be the start point, but we'd like to
    // sink the constant '1' into the region (it will actually become the new
    // start point).
    if (isBeforeStartPoint)
      sinkValueIntoRegionForPromotion(inst);

    // If the instruction is in the tensor program region (either because it was
    // already or because we just moved it) then we can mark it to be copied in.
    return markInstruction(*inst, Marking::Copy);
  }

  // If the value is defined outside of the region dominated by the tensor
  // operations (or it can be hoisted above it), then it is passed in as an
  // argument to the tensor function.
  if (isBeforeStartPoint ||
      // If we can hoist it above the start point then it can be an argument.
      hoistValueAboveStartPoint(inst, tensorStartPoint, DI)) {
    markedInstructions.insert({inst, Marking::Argument});
    tensorFnArguments.push_back(value);
    diagnoseCopyToAccelerator(value, user);
    return;
  }

  // If this is a scalar operation that can be promoted to a tensor op on the
  // accelerator, mark it as being copied over (this leads to its operands
  // being recursively copied as well).
  if (promotionClass == ScalarPromoteClass::CanPromote)
    return markInstruction(*inst, Marking::Copy);

  // Otherwise, insert a send from the host to the accelerator.
  valuesToSend.insert(value);
  diagnoseCopyToAccelerator(value, user);

  // Instead of cloning over this instruction, we'll add a send after it and
  // insert a receive in the accelerator code.
  markedInstructions.insert({inst, Marking::Send});
  markBlock(inst->getParent());
}

/// Given a list of tensor operations, find the nearest common ancestor of those
/// operations in the [post-]dominator-tree of the CFG.  In addition to finding
/// the NCA, this also returns the list of ops that are in that block (if any).
static SILBasicBlock *
findNCAOfTensorOps(ArrayRef<SILInstruction*> tensorOps,
                   SmallPtrSet<SILInstruction*, 8> &ncaBBOps,
         std::function<SILBasicBlock*(SILBasicBlock*,SILBasicBlock*)> findNCA) {
  assert(!tensorOps.empty() && "expect at least one tensor op");

  auto ncaBlock = tensorOps[0]->getParent(); // Arbitrary starting point.

  for (auto inst : tensorOps) {
    // If this op is in the ncaBlock, just remember it.
    auto instBB = inst->getParent();
    if (instBB == ncaBlock) {
      ncaBBOps.insert(inst);
      continue;
    }

    // Otherwise, it is possible that the startBB already dominates instBB.  If
    // so, the NCA Will not change.
    auto NCA = findNCA(ncaBlock, instBB);
    if (NCA == ncaBlock)
      continue;

    // Otherwise, the instBB dominated startBB, or neither dominated the other
    // one (meaning NCA is some unrelated parent block).  In either case, it
    // becomes our new startBB.
    ncaBlock = NCA;
    ncaBBOps.clear();
    ncaBBOps.insert(inst);
  }

  return ncaBlock;
}

/// Optimize the host code (and avoid copies back to the host in some cases) by
/// changing scalar operations marked as "Copy" into "Move" when all of their
/// users are known to be moved or deleted.
void TFFunctionPartition::promoteCopyToMove() {
  // Keep a worklist of instructions to process, allowing us to mark operands
  // of instructions that are used by other instructions that get moved over.

  // TODO: This should be an optimistic algorithm in general, and should include
  // arguments as well.  This would allow moving over of cyclic references.
  SmallVector<SILInstruction*, 16> worklist;
  for (auto &markInfo : markedInstructions) {
    auto *inst = markInfo.first;
    if (markInfo.second == Marking::Copy)
      worklist.push_back(inst);
  }

  // Iteratively process instructions until we converge.
  SmallVector<SILInstruction*, 4> additionalInstsToDelete;

  while (!worklist.empty()) {
    auto *inst = worklist.pop_back_val();
    auto mii = markedInstructions.find(inst);

    // Ignore terminators or instructions that are not marked as copies.
    if (mii == markedInstructions.end() || mii->second != Marking::Copy ||
        isa<TermInst>(inst))
      continue;

    additionalInstsToDelete.clear();
    bool canMove = true;
    for (auto result : inst->getResults())
      for (auto *use : result->getUses()) {
        auto *user = use->getUser();
        // If this user is marked for deletion or to be moved, then it won't
        // require the instruction to be around.
        auto it = markedInstructions.find(user);
        if (it != markedInstructions.end() &&
            (it->second == Marking::Move || it->second == Marking::Delete))
          continue;

        // If this is a debug_value or retain/release instruction that becomes
        // obsolete if we move the value, then keep track of it, but keep going.
        if (isUserIgnoredByPartitioning(user)) {
          additionalInstsToDelete.push_back(user);
          continue;
        }

        // Otherwise, there are host instructions that need this value.  Leave
        // the computation on the host.
        canMove = false;
        break;
      }

    // If we can't move this instruction, then ignore it but keep processing
    // worklist entries.
    if (!canMove)
      continue;

    // Okay, we can move this instruction.
    markedInstructions[inst] = Marking::Move;

    // Make sure to delete any debug_value or refcounting instructions this
    // uses.
    for (auto inst : additionalInstsToDelete)
      markedInstructions[inst] = Marking::Delete;

    // Moving this over means that any operands of the instruction may have
    // become movable.  Add them (back?) to the worklist for further
    // consideration.
    for (auto &op : inst->getAllOperands()) {
      if (auto opInst = op.get()->getDefiningInstruction())
        worklist.push_back(opInst);
    }
  }
}


/// Scan the function looking for blocks with tensor operations in them.  As
/// we find them, mark them as "to-be-partitioned", which marks (transitive)
/// data and control dependencies.
bool TFFunctionPartition::markFunction() {
  // We walk the function in depth first order so that we only visit reachable
  // blocks and to slightly improve compile time performance of the 'marking'
  // operation.
  SmallVector<SILInstruction*, 32> tensorOps;
  bool invalidOpFound = false;
  for (auto *BB : llvm::depth_first(&fn)) {
    for (auto I = BB->begin(), E = BB->end(); I != E; ) {
      // Manually move iterator to avoid invalidation if we replace 'inst'.
      auto *inst = &*I++;

      // If this is a well known function that can be transformed into an op, do
      // so first.
      if (auto apply = dyn_cast<ApplyInst>(inst))
        if (auto fn = apply->getCalleeFunction())
          inst = SILTensorOpInfo::decodeApply(apply, fn->getName());

      auto opInfo = SILTensorOpInfo::decode(inst);
      if (!opInfo)
        continue;

      // Check to see if the usage of this op looks ok.  If not, reject it with
      // an error and ignore it.
      auto error = opInfo->checkAndDiagnoseOperands();
      if (!error.empty()) {
        // TODO: improve the diagnostic to talk about the parameter label in the
        // user code, not the internal op attribute.  The bookkeeping for this
        // isn't obvious though.
        auto loc = getUserSourceLocation(inst);
        diagnose(fn.getModule().getASTContext(), loc.getSourceLoc(),
                 diag::tf_op_misuse, error)
          .highlight(loc.getSourceRange());
        invalidOpFound = true;
        continue;
      }

      // Because we don't have deabstraction yet, and because generic
      // deabstraction doesn't know about our builtins, we get scalars and
      // constants passed by reference through a stack allocation.  We support
      // this form on input, but want to canonicalize this away so the
      // partitioning pass and data flow analysis code doesn't have to reason
      // about it.
      // TODO(clattner): Remove this when deabstraction subsumes it.
      inst = opInfo->canonicalizeOperands();

      tensorOps.push_back(inst);
      tensorOpsSet.insert(inst);
    }
  }

  // If there is nothing to do, or the ops in this function are malformed,
  // don't touch this function.
  if (tensorOps.empty() || invalidOpFound)
    return false;

  // Compute the blocksReachingTensorCode set.
  tensorCodeBlocks.compute(tensorOps);

  // Next, compute the NCA of all of the core tensor operations as our "start
  // point".  This is where we will start the tensor computation, sending over
  // argument values defined outside the scope of the computation.
  SmallPtrSet<SILInstruction*, 8> bbOps;
  auto startBB = findNCAOfTensorOps(tensorOps, bbOps,
                                    [&] (SILBasicBlock *B1,
                                         SILBasicBlock *B2) -> SILBasicBlock* {
    return DI.findNearestCommonDominator(B1, B2);
  });

  // Compute the start point by doing a linear scan of the startBB to find the
  // first (of possibly many) tensor operations.
  tensorStartPoint = startBB->getTerminator();
  if (!bbOps.empty()) {
    for (auto &inst : *startBB) {
      if (bbOps.count(&inst)) {
        tensorStartPoint = &inst;
        break;
      }
    }
  }

  // Now that we know the region we're extracting from, mark all of the
  // operations as being moved over to the graph, and recursively mark their
  // operands as appropriate.
  for (auto inst : tensorOps)
    markInstruction(*inst, Marking::Move);

  // Optimize the host code (and avoid copies back to the host in some cases) by
  // changing scalar operations marked as "Copy" into "Move" when all of their
  // users are known to be moved or deleted.
  promoteCopyToMove();


  // Not that we know all of the instructions we'll be moving over, find the end
  // point by finding the nearest common post dominating ancestor of the marked
  // instructions.
  bbOps.clear();
  SmallVector<SILInstruction*, 16> instrsToCheck;
  for (auto markInfo : markedInstructions) {
    // Ignore instructions that will be deleted.
    if (markInfo.second == Marking::Delete) continue;
    auto *inst = markInfo.first;
    instrsToCheck.push_back(inst);

    // If the marked instruction is a terminator, then make sure that the first
    // instruction in each successor is marked as well.  This ensures that we
    // have an "out of loop" use to put the end point outside of a loop if the
    // start point is also outside of it.
    if (auto *ti = dyn_cast<TermInst>(inst))
      for (auto &succ : ti->getSuccessors())
        instrsToCheck.push_back(&succ.getBB()->front());
  }

  auto endBB = findNCAOfTensorOps(instrsToCheck, bbOps,
                                  [&] (SILBasicBlock *B1,
                                         SILBasicBlock *B2) -> SILBasicBlock* {
    return tensorCodeBlocks.findNearestCommonPostDominator(B1, B2);
  });
  assert(endBB && "Didn't find an end point for the tensor program");

  // Compute the end point by doing a backward scan.
  if (bbOps.empty()) {
    tensorEndPoint = &endBB->front();
  } else {
    for (auto &inst : llvm::reverse(*endBB)) {
      if (bbOps.count(&inst))
        break;
      tensorEndPoint = &inst;
    }
  }

  return true;
}




//===----------------------------------------------------------------------===//
//                              PartitionCloner
//===----------------------------------------------------------------------===//

namespace {
class PartitionCloner : public SILClonerWithScopes<PartitionCloner> {
  TFFunctionPartition &FP;

  /// This is a basic block on the newly created function which represents the
  /// exit node of the function.
  SILBasicBlock *exitBB;

  /// This is a counter we use to give each send/receive operation a unique ID.
  unsigned nextSendID = 0;

  /// This is the set of instructions that should be removed from the host code
  /// after the cloning operation is complete.
  SmallVector<SILInstruction*, 8> instructionsToRemove;
public:
  PartitionCloner(TFFunctionPartition &FP, SILFunction &NewFn)
    : SILClonerWithScopes(NewFn), FP(FP) {
  }

  void cloneFunction(ArrayRef<SILValue> resultValues);
  void finalizeOriginal();

  void insertSend(SILInstruction &inst);
  void insertReceive(SILValue value, SILLocation loc);

  // Handle references to blocks from cloned code.
  SILBasicBlock *remapBasicBlock(SILBasicBlock *BB) {
    // If the block is included in the partition, directly reference it.
    auto bbIt = BBMap.find(BB);
    if (bbIt != BBMap.end())
      return bbIt->second;

    // Otherwise, it must be a jump to a block that wasn't included in the
    // partition.  Figure out which post-dominated block is included, and jump
    // to it instead.
    auto pdiBlock = BB;
    while (1) {
      // If the post-idom is in the partition, use it.  Otherwise keep scanning.
      pdiBlock = FP.tensorCodeBlocks.getPostIDom(pdiBlock);

      // If we found the exit node, use our exitBB.
      if (!pdiBlock)
        return BBMap[BB] = exitBB;

      auto bbIt = BBMap.find(pdiBlock);
      if (bbIt != BBMap.end())
        return BBMap[BB] = bbIt->second;
    }
  }

  SILType remapType(SILType ty) {
    return convertToTensorHandleType(ty);
  }

  void visitOpInst(SingleValueInstruction *inst, SILTensorOpInfo &tfopInfo);
  void visitScalarInst(SingleValueInstruction *inst);

  void visitScalarOrOpInst(SingleValueInstruction *inst) {
    // Check to see if this is an op.
    if (auto tfopInfo = SILTensorOpInfo::decode(inst))
      return visitOpInst(inst, tfopInfo.getValue());

    // Otherwise it is a scalar to promote.
    visitScalarInst(inst);
  }

  void visitBuiltinInst(BuiltinInst *inst) {
    visitScalarOrOpInst(inst);
  }
  void visitApplyInst(ApplyInst *inst) {
    visitScalarOrOpInst(inst);
  }
  void visitIntegerLiteralInst(IntegerLiteralInst *inst) {
    visitScalarInst(inst);
  }
  void visitFloatLiteralInst(FloatLiteralInst *inst) {
    visitScalarInst(inst);
  }

  void visitTupleExtractInst(TupleExtractInst *inst);
  void visitStructExtractInst(StructExtractInst *inst);

  void visitBranchInst(BranchInst *inst);
  void visitCondBranchInst(CondBranchInst *inst);

private:
  // Check to see if the argument was marked in a way that indicates we should
  // copy it over to the tensor program.
  bool shouldCloneArgument(SILArgument *arg) const {
    auto it = FP.markedBBArguments.find(arg);
    if (it == FP.markedBBArguments.end()) return false;

    switch (it->second.first) {
    case Marking::Copy:
    case Marking::Move:
      return true;
    case Marking::Send:
    case Marking::Argument:
    case Marking::Delete:
      return false;
    }
  }

  void initBlock(SILBasicBlock *BB);
  void cloneBlock(SILBasicBlock *BB);
};
} // end anonymous namespace


/// We create each block in an initial pass over the function, before cloning
/// over the instructions.  This allows us to know that there is always a block
/// in our block mapping as we start cloning over branch instructions.
void PartitionCloner::initBlock(SILBasicBlock *BB) {
  auto newBB = Builder.getFunction().createBasicBlock();
  BBMap[BB] = newBB;

  // If the basic block has arguments, map over any marked ones.
  for (auto *arg : BB->getArguments()) {
    if (!shouldCloneArgument(arg))
      continue;

    // Create the argument and copy it into the ValueMap so future references
    // use it.
    ValueMap[arg] = newBB->createPHIArgument(remapType(arg->getType()),
                                             ValueOwnershipKind::Trivial,
                                             arg->getDecl());
  }

  // If this is the entry block for our computation, add the parameter BB
  // arguments.
  if (BB == FP.tensorStartPoint->getParent()) {
    for (auto arg : FP.tensorFnArguments) {
      auto argTy = convertToTensorHandleType(arg->getType());
      auto newArg = newBB->createFunctionArgument(argTy);
      ValueMap[arg] = SILValue(newArg);
    }
  }
}

// Provide special handling for unconditional branch instructions: we only
// clone over marked bb arguments, not all of them.
void PartitionCloner::visitBranchInst(BranchInst *inst) {
  SmallVector<SILValue, 4> operands;
  operands.reserve(inst->getNumArgs());
  auto destBB = inst->getDestBB();
  unsigned opNum = 0;
  for (auto &arg : inst->getAllOperands()) {
    if (shouldCloneArgument(destBB->getArgument(opNum++)))
      operands.push_back(remapValue(arg.get()));
  }

  getBuilder().setCurrentDebugScope(getOpScope(inst->getDebugScope()));
  auto br = getBuilder().createBranch(getOpLocation(inst->getLoc()),
                                      getOpBasicBlock(inst->getDestBB()),
                                      operands);
  doPostProcess(inst, br);
}

/// For conditional branches, we do exactly what the normal cloner does, except
/// that if we see a branch on a Tensor<Int1>, we unwrap it into an Int1.  We
/// know (by construction) that this only happens when the Tensor is a 0D value.
void PartitionCloner::visitCondBranchInst(CondBranchInst *inst) {
  auto TrueArgs = getOpValueArray<8>(inst->getTrueArgs());
  auto FalseArgs = getOpValueArray<8>(inst->getFalseArgs());
  auto &B = getBuilder();
  B.setCurrentDebugScope(getOpScope(inst->getDebugScope()));

  auto cond = getOpValue(inst->getCondition());

  if (auto eltTy = isTensorHandle(cond->getType().getSwiftRValueType())) {
    assert(eltTy->isBuiltinIntegerType(1) && "expected Tensor<i1>");

    auto name = B.getASTContext().getIdentifier("tf_tensor_to_i1");
    cond = B.createBuiltin(getOpLocation(inst->getLoc()), name,
                   SILType::getPrimitiveObjectType(eltTy->getCanonicalType()),
                           /*substitutionlist*/{}, cond);
  }

  doPostProcess(inst,
                B.createCondBranch(getOpLocation(inst->getLoc()),
                                   cond,
                                   getOpBasicBlock(inst->getTrueBB()), TrueArgs,
                                   getOpBasicBlock(inst->getFalseBB()),
                                   FalseArgs, inst->getTrueBBCount(),
                                   inst->getFalseBBCount()));
}


// Transform ops builtin instructions to the one we need in the tensor program.
void PartitionCloner::visitOpInst(SingleValueInstruction *inst,
                                  SILTensorOpInfo &tfopInfo) {
  auto &B = getBuilder();
  auto loc = remapLocation(getUserSourceLocation(inst->getDebugLocation()));

  // Handle special case "ops".
  if (tfopInfo.opName == "tfc.scalarToTensor") {
    assert(inst->getNumOperands() == 1 && "invalid tfc.scalarToTensor!");
    // We just lower the result as the input, since the scalar input will have
    // been promoted to a tensor already.  It is possible that the input will
    // have been lowered to something like TensorHandle<Int64> and we need a
    // TensorHandle<Int>. If that is the case, we insert an UncheckedRefCast
    // to get it to the right type.  These are treated as noops by GraphGen.
    auto result = remapValue(inst->getOperand(0));
    if (!inst->getType().getSwiftRValueType()
          ->isEqual(result->getType().getSwiftRValueType())) {
      result = B.createUncheckedRefCast(loc, result, inst->getType());
    }

    ValueMap[inst] = result;
    return;
  }

  SmallVector<SILValue, 4> args;
  auto cloneSingleInst = [&](SingleValueInstruction *inst)
  -> SingleValueInstruction* {
    auto ourInst = inst->clone();
    ourInst->setDebugLocation(B.getSILDebugLocation(loc));
    B.getInsertionBB()->push_back(ourInst);
    return ourInst;
  };

  for (unsigned i = isa<ApplyInst>(inst), e = inst->getNumOperands();
       i != e; ++i) {
    auto opValue = inst->getOperand(i);
    if (isTensorHandle(opValue->getType())) {
      // Tensor operands just become operands.
      args.push_back(remapValue(opValue));
    } else {
      args.push_back(cloneSingleInst(cast<SingleValueInstruction>(opValue)));
    }
  }

  auto name = B.getASTContext().getIdentifier(tfopInfo.builtinName);
  auto result = B.createBuiltin(loc, name, inst->getType(),
                                /*substitutionlist*/{}, args);
  ValueMap[inst] = result;
}


/// Given a primitive scalar instruction like a literal or an LLVM IR
/// instruction (represented as a builtin), promote it to a tensor op in the
/// graph function.
void PartitionCloner::visitScalarInst(SingleValueInstruction *inst) {
  auto loc = remapLocation(inst->getLoc());

  // Determine which kind of scalar operation this is, since different forms get
  // different sorts of processing applied to them.
  auto opInfo = classifyPromotedScalarOp(inst);
  assert(opInfo.first != nullptr && "Invalid scalar operation to promote");
  auto opKind = opInfo.second;

  // The TensorToScalar operation is promoted by just dropping the operation
  // and using the incoming tensor value.
  if (opKind == PromotedScalarKind::TensorToScalar) {
    ValueMap[inst] = remapValue(inst->getOperand(1));
    return;
  }
  
  std::string opName = "__tfop_" + std::string(opInfo.first);

  // Start remapping the operand list.
  auto operandRange = inst->getAllOperands();

  // Overflow-checking integer ops have a "should check" bit as their last
  // parameter, which we drop.
  if (opKind == PromotedScalarKind::OverflowingBinary)
    operandRange = operandRange.drop_back();

  SmallVector<SILValue, 4> operands;
  for (auto &op : operandRange)
    operands.push_back(remapValue(op.get()));

  // The type of the new builtin is usually the same as the input type, but
  // "remapped", which turns Float into TensorHandle<Float>.
  auto resultType = inst->getType();
  if (opKind == PromotedScalarKind::OverflowingBinary) {
    // In the input program, cverflow checking instructions return something
    // like (Int64, i1).  During the marking process, we've determined that the
    // overflow bit is dead, so we only produce the normal result.
    resultType = resultType.getTupleElementType(0);
  }

  auto &B = getBuilder();
  auto &ctx = B.getASTContext();

  // Handle opKind-specific issues.
  switch (opKind) {
  case PromotedScalarKind::Invalid: assert(0 && "Rejected ealier");
  case PromotedScalarKind::TensorToScalar: assert(0 && "Handled above");
  case PromotedScalarKind::Binary:
  case PromotedScalarKind::OverflowingBinary:
    opName += ",$in,$in";
    break;
  case PromotedScalarKind::Literal: {
    // Literals take attributes specifying the dtype and value.
    opName += ",dtype$dtype,value$tensor";

    auto dtype = convertSwiftTypeToTF(inst->getType().getSwiftRValueType());
    auto dtypeCst =
      B.createIntegerLiteral(loc, SILType::getBuiltinIntegerType(32, ctx),
                             dtype);
    operands.push_back(dtypeCst);

    // The value attribute is specified by a clone of the literal itself.
    auto ourCst = inst->clone(dtypeCst);
    ourCst->setDebugLocation(B.getSILDebugLocation(loc));
    operands.push_back(ourCst);
    break;
  }
  case PromotedScalarKind::Conversion: {
    // Conversions get an attribute specifying the result dtype, named "DstT".
    opName += ",$in,DstT$dtype";
    auto dtype = convertSwiftTypeToTF(inst->getType().getSwiftRValueType());
    auto dtypeCst =
      B.createIntegerLiteral(loc, SILType::getBuiltinIntegerType(32, ctx),
                             dtype);
    operands.push_back(dtypeCst);
    break;
  }
  }

  auto result =
    B.createBuiltin(loc, B.getASTContext().getIdentifier(opName),
                    remapType(resultType), /*substitutionlist*/{}, operands);
  ValueMap[inst] = result;
}

/// We clone over tuple_extract(x, 0) into x's value.  The only time
/// tuple_extract instructions get marked is when this is safe.
void PartitionCloner::visitTupleExtractInst(TupleExtractInst *inst) {
  assert(inst->getFieldNo() == 0 && "Unexpected extract to remap");
  ValueMap[inst] = remapValue(inst->getOperand());
}

/// We clone over struct_extract(x, 0) into x's value.  The only time we mark
/// a struct_extract is when this is safe.
void PartitionCloner::visitStructExtractInst(StructExtractInst *inst) {
  assert(inst->getFieldNo() == 0 && "Unexpected extract to remap");
  ValueMap[inst] = remapValue(inst->getOperand());
}

static SILValue createReceive(SILBuilder &B, SILLocation loc,
                              SILType valueTy, unsigned idNumber) {
  auto &ctx = B.getASTContext();

  auto name = ctx.getIdentifier("tensorflowReceive_"+ llvm::utostr(idNumber));

  return // tensorflowReceive has type <T> () -> T
    B.createBuiltin(loc, name, valueTy,
                    Substitution(valueTy.getSwiftRValueType(), {}), {});
}

static void createSend(SILBuilder &B, SILLocation loc,
                       SILValue value, unsigned idNumber) {
  auto &ctx = B.getASTContext();
  auto voidTy = B.getModule().Types.getEmptyTupleType();
  auto name = ctx.getIdentifier("tensorflowSend_"+ llvm::utostr(idNumber));

  // tensorflowSend has type <T> (T) -> ()
  B.createBuiltin(loc, name, voidTy,
                  Substitution(value->getType().getSwiftRValueType(), {}),
                  {value});
}

/// Insert a send of values from the specified instruction result(s) to the
/// accelerator, and insert receives in it.
void PartitionCloner::insertSend(SILInstruction &inst) {
  assert(!isa<TermInst>(inst) && "Cannot insert after a terminator");

  SILBuilder BH(++SILBasicBlock::iterator(&inst)); // Builder for host.
  auto BA = getBuilder();        // Builder for accelerator.

  auto loc = getUserSourceLocation(&inst);

  for (auto result : inst.getResults()) {
    // Create the receive in the accelerator code.  Each send/receive pair gets
    // a unique ID to associate one with the other.
    this->ValueMap[result] =
      createReceive(BA, loc, remapType(result->getType()), nextSendID);

    // Create the send in the host code.
    createSend(BH, loc, result, nextSendID);
    nextSendID++;
  }
}

void PartitionCloner::insertReceive(SILValue value, SILLocation loc) {
  assert(isa<SILInstruction>((SILNode*)value) || isa<SILArgument>(value) &&
         "Don't know how to receive this value");

  SILBuilder BH(FP.fn);          // Builder for the host.
  auto BA = getBuilder();        // Builder for accelerator.

  if (auto *inst = dyn_cast<SILInstruction>((SILNode*)value)) {
    assert(!isa<TermInst>(inst) && "Cannot move a terminator");
    BH.setInsertionPoint(++SILBasicBlock::iterator(inst));

    auto otherInst = cast<SILInstruction>((SILNode*)ValueMap[value]);
    BA.setInsertionPoint(++SILBasicBlock::iterator(otherInst));

  } else {
    auto *arg = cast<SILArgument>(value);
    BH.setInsertionPoint(&arg->getParent()->front());

    auto otherBB = remapBasicBlock(arg->getParent());
    BA.setInsertionPoint(&otherBB->front());
  }

  // Diagnose implicit data transfers.
  for (auto *use : value->getUses()) {
    FP.diagnoseCopyToHost(value, use->getUser(), loc);
  }

  // Create the send in the accelerator code.  Each send/receive pair gets
  // a unique ID to associate one with the other.
  createSend(BA, loc, remapValue(value), nextSendID);

  // Create the receive in the host code.
  auto newVal = createReceive(BH, loc, value->getType(), nextSendID);
  value->replaceAllUsesWith(newVal);
  nextSendID++;
}


/// Move and clone code over from the input block into this block, inserting
/// transfers between the host and destination code as necessary.
void PartitionCloner::cloneBlock(SILBasicBlock *BB) {
  if (!FP.markedBlocks.count(BB))
    return;  // Ignore blocks that aren't in accelerator code.
  auto newBB = BBMap[BB];

  Builder.setInsertionPoint(newBB);
  for (auto &inst : *BB) {
    // If the specified instruction is used by the accelerator program somehow,
    // we have to copy the instruction over, copy it, or send the result.
    auto it = FP.markedInstructions.find(&inst);
    if (it == FP.markedInstructions.end()) continue;

    switch (it->second) {
    case Marking::Move:
      instructionsToRemove.push_back(&inst);
      LLVM_FALLTHROUGH;
    case Marking::Copy:
      visit(&inst);
      break;
    case Marking::Send:
      insertSend(inst);
      break;
    case Marking::Argument:
      // Already handled.
      break;
    case Marking::Delete:
      instructionsToRemove.push_back(&inst);
      break;
    }
  }

  // If the terminator of this block wasn't live then it was either an
  // unconditional branch (which never matter for control dependence analysis)
  // or some kind of conditional branch that wasn't important because control
  // eventually flowed to a post dominator that didn't care about the direction
  // of this branch.  In either case, we can provide a terminator by introducing
  // an unconditional branch to the post dominator of the block.
  if (newBB->empty() || !isa<TermInst>(newBB->back())) {
    auto PDI = FP.tensorCodeBlocks.getPostIDom(BB);
    auto destBB = PDI ? remapBasicBlock(PDI) : exitBB;
    Builder.createBranch(BB->getTerminator()->getLoc(), destBB);
  }
}

void PartitionCloner::cloneFunction(ArrayRef<SILValue> resultValues) {
  // Go through and create all the blocks before we start cloning the
  // instructions over.  This allows us to remap instructions when we clone
  // them over.
  initBlock(FP.tensorStartPoint->getParent());  // First block first.

  for (auto &BB : FP.fn) {
    // If the BB is unmarked, we don't need it for the accelerator.
    if (!FP.markedBlocks.count(&BB) ||
        &BB == FP.tensorStartPoint->getParent())
      continue;

    initBlock(&BB);
  }

  // Create a block for the exit node in case we need it.
  exitBB = Builder.getFunction().createBasicBlock();

  // Now that all the basic blocks and BBArguments are created, we can walk the
  // function in depth first order copying the code over.  Because we're working
  // in depth first order and have BB Arguments resolved, we're guaranteed to
  // see all definitions before uses.
  for (auto *BB : llvm::depth_first(&FP.fn)) {
    cloneBlock(BB);
  }

  // We can end up with to-be-deleted instructions (like debug_value's) outside
  // of the marked region.  These won't be seen in the cloneBlock walk we just
  // did, but we do want to remove them.  Check to see if we have any of these,
  // and arrange for them to be removed.
  for (auto ip : FP.markedInstructions) {
    if (ip.second == Marking::Delete &&
        !FP.markedBlocks.count(ip.first->getParent()))
      instructionsToRemove.push_back(ip.first);
  }

  // Okay at this point we're done except for setting up the exitBB.  Check to
  // see if it is unused.  If so, we nuke it, otherwise we add a return.
  if (exitBB->pred_empty()) {
    exitBB->eraseFromParent();
  } else {
    Builder.setInsertionPoint(exitBB);

    // Create a return of N values, producing a tuple if necessary.
    SILValue result;
    if (resultValues.size() == 1)
      result = remapValue(resultValues[0]);
    else {
      SmallVector<SILValue, 4> results;
      for (auto r : resultValues)
        results.push_back(remapValue(r));

      result = Builder.createTuple(FP.fn.getLocation(), results);
    }

    Builder.createReturn(FP.fn.getLocation(), result);
  }
}


/// Now that all of the interesting instructions are cloned over, we need to
/// clean up the input function by removing the instructions, and inserting
/// sends of results from the accelerator code back to the host code.
///
void PartitionCloner::finalizeOriginal() {

  // Build a set of the instructions we're going to remove so we can add new
  // values without fear of adding duplicates.
  SmallPtrSet<SILInstruction*, 16>
  instsToRemoveSet(instructionsToRemove.begin(), instructionsToRemove.end());
  assert(instsToRemoveSet.size() == instructionsToRemove.size() &&
         "duplicates shouldn't exist in the instsToRemove list");

  // Start by dropping interdependent references so we don't get confused by
  // uses that have moved over.  We are adding values to the vector as we
  // process it, so iterate indexes into it to avoid invalidating iterators.
  for (unsigned idx = 0; idx != instructionsToRemove.size(); ++idx) {
    auto inst = instructionsToRemove[idx];

    // When removing a builtin tensor op instruction TO, for each tensor
    // argument TA of it, if TA is NOT generated by other tensor ops, emit a
    // strong_release to balance the already emitted strong_retain's for
    // TA. Recall the calling convention of TO is that TO consumes TA at +1.
    //
    // For example, before removing instruction
    // %r1 = builtin "__tfop_Add"(%arg1, %arg2),
    // we first emit strong_release'es on %arg1 and %arg2.
    // If there is a subsequent instruction
    // %r2 = builtin "__tfop_Add"(%r1, %arg3),
    // we only emit strong_release on %arg3, and not on %r1, since the
    // strong_retain on %r1 will be removed when all references of the first Add
    // instruction are dropped along with that instruction itself.
    if (isa<ApplyInst>(inst) || isa<BuiltinInst>(inst)) {
      SILBuilder BH(inst);  // Builder for the host.
      for (auto &operand : inst->getAllOperands()) {
        auto op = operand.get();
        if (!isTensorHandle(op->getType().getSwiftRValueType()))
          continue;

        // def could be NULL, say if the operand is produced by argument passed
        // into FP.fn.
        auto def = op->getDefiningInstruction();
        bool isDefinedByLocalTensorOp = def && FP.tensorOpsSet.count(def);
        if (!isDefinedByLocalTensorOp) {
          BH.emitStrongReleaseAndFold(inst->getLoc(), op);
        }
      }
    }

    // Loop through and drop all the operand references.  If we strand any
    // primitive instructions, clean them up now so we get cleaner IR.
    for (auto &op : inst->getAllOperands()) {
      auto v = op.get();
      op.drop();

      // If we just dropped the last use of a trivial instruction (which are how
      // we represent attributes for instructions), clean it up now.
      if (auto opInst = v->getDefiningInstruction())
        if (!opInst->hasUsesOfAnyResult() &&
            (isa<LiteralInst>(opInst) || isa<MetatypeInst>(opInst)) &&
            instsToRemoveSet.insert(opInst).second)
          instructionsToRemove.push_back(opInst);
    }
  }


  // As we're removing branch arguments, we remove all the bb args for a given
  // block at a time.  This is important because otherwise we invalidate our our
  // bb arg -> branch argument mappings.
  SmallPtrSet<SILBasicBlock*, 4> argsRemovedBlocks;

  // We remove arguments with a two-pass approach, first dropping the insts that
  // feed them, then removing the arg (potentially inserting a receive).
  SmallVector<SILArgument*, 4> argsToRemove;

  // For BB arguments, we remove the uses from the branches first (which breaks
  // interdependent references) and delete the actual arguments later.
  for (auto argInfo : FP.markedBBArguments) {
    // We delete arguments that are moved over to the accelerator.
    auto marking = argInfo.second.first;
    if (marking != Marking::Move) {
      assert((marking == Marking::Copy || marking == Marking::Argument) &&
             "Only move/copy/argument supported for arguments right now");
      continue;
    }

    // Check to see if we already processed the block this argument lives in.
    auto arg = argInfo.first;
    auto *bb = arg->getParent();
    if (!argsRemovedBlocks.insert(bb).second)
      continue;

    // Ok, this is the first time we're processing this block.  It could have
    // lots of bb arguments, some of which are being removed.  We will remove
    // the arguments in a separate loop, but here we want to remove all of the
    // values passed in by terminators in predecessor blocks.  Start by
    // constructing a bitvector so we know which args are going to be removed.
    llvm::BitVector argsToRemoveMask(bb->getNumArguments());
    unsigned argNo = 0;
    for (auto arg : bb->getArguments()) {
      auto it = FP.markedBBArguments.find(arg);
      if (it != FP.markedBBArguments.end() &&
          it->second.first == Marking::Move) {
        argsToRemoveMask[argNo] = true;
      }
      ++argNo;
    };

    // Now remove the formal values provided by any branches that jump to that
    // block, as indicated by the BitVector.
    for (auto pi : bb->getPredecessorBlocks()) {
      auto *br = cast<BranchInst>(pi->getTerminator());
      SmallVector<SILValue, 8> operands;
      for (unsigned i = 0, e = br->getNumOperands(); i != e; ++i)
        if (!argsToRemoveMask[i])
          operands.push_back(br->getOperand(i));
      SILBuilder(br).createBranch(br->getLoc(), br->getDestBB(), operands);
      br->eraseFromParent();
    }
  }

  // Ok, now all interdependent references have been dropped.  If there are any
  // uses of values that we moved over to the accelerator, then we must insert
  // a receive from the accelerator of the computed value.  Regardless, we can
  // now delete the defining instruction/argument.
  for (auto argInfo : FP.markedBBArguments) {
    auto marking = argInfo.second.first;
    if (marking != Marking::Move)
      continue;

    auto arg = argInfo.first;
    auto loc = argInfo.second.second;

    // If the argument has any non-debug-non-retain/release instructions using
    // it, then we need to insert a copy.
    SmallVector<SILInstruction*, 2> instToRemove;
    bool needsCopy = false;
    for (auto operand : arg->getUses()) {
      auto user = operand->getUser();
      if (isUserIgnoredByPartitioning(user)) {
        instToRemove.push_back(user);
        continue;
      }
      needsCopy = true;
      break;
    }

    if (needsCopy) {
      // If we need the value on the host, then keep the retain/release ops.
      insertReceive(arg, loc);
    } else {
      // Otherwise, drop them.
      for (auto *inst : instToRemove)
        inst->eraseFromParent();
    }

    // Remove it from the block that it lives in.
    arg->getParent()->eraseArgument(arg->getIndex());
  }

  // Next, add sends back of values that are used by the host code, and remove
  // the original instructions.
  for (auto inst : instructionsToRemove) {
    for (auto result : inst->getResults())
      if (!result->use_empty()) {
        insertReceive(result, getUserSourceLocation(inst));
      }

    inst->eraseFromParent();
  }

  // The copy-in/out markers should be removed now.  They are noops which serve
  // no purpose now that we have emitted diagnostics.
  for (auto ecm : FP.explicitCopyMarkers) {
    assert(isa<ApplyInst>(ecm) && ecm->getResults().size() == 1 &&
           ecm->getNumOperands() == 2 && "unknown copy in/out instruction");
    auto callee = ecm->getOperand(1);
    ecm->getResults()[0]->replaceAllUsesWith(ecm->getOperand(1));
    ecm->eraseFromParent();

    if (callee->use_empty())  // Remove the function_ref too.
      cast<SingleValueInstruction>(callee)->eraseFromParent();
  }
}

/// Wrap a value in a simple struct wrapper, these are common in the standard
/// library.
static SILValue wrapInStruct(SILValue v, NominalTypeDecl *decl, SILBuilder &B,
                             SILLocation loc) {
  auto type = decl->getDeclaredInterfaceType()->getCanonicalType();
  auto silType = SILType::getPrimitiveObjectType(type);
  return B.createStruct(loc, silType, v);
}

/// Create a value of some standard library integer type, as specified by
/// integerDecl.
static SILValue createSomeIntegerValue(intmax_t value, SILBuilder &B,
                                       SILLocation loc,
                                       NominalTypeDecl *integerDecl,
                                       IntegerLiteralInst **ILI = nullptr) {
  auto intFieldType = getSingleElementDeclFieldType(integerDecl);
  auto intFieldSILType = SILType::getPrimitiveObjectType(intFieldType);

  auto literal = B.createIntegerLiteral(loc, intFieldSILType, value);

  // If the caller wanted the integer_literal instruction, return it too.
  if (ILI) *ILI = literal;

  return wrapInStruct(literal, integerDecl, B, loc);
}

/// Create a value of 'Swift.Int' type holding the specified value.  It is a bit
/// trickier to create than some types because its contents are target platform
/// specific: Builtin.Int32 or Builtin.Int64.
static SILValue createIntValue(intmax_t value, SILBuilder &B, SILLocation loc,
                               IntegerLiteralInst **ILI = nullptr) {
  // Int should have one field, a Builtin.Int32/64.
  auto intDecl = B.getASTContext().getIntDecl();
  return createSomeIntegerValue(value, B, loc, intDecl, ILI);
}


/// Convert the specified scalar value (e.g. an i32) into a 0d CTensorHandle
/// value using the TensorFlow runtime utilities.
static SILValue convertScalarToHostTensorHandle(SILValue value, SILBuilder &B,
                                                SILLocation loc) {
  // We need to create a dtype value.  It is an imported C enum value, so it is
  // modeled as a struct that wraps an integer value (itself a struct).
  auto dtypeVal = convertSwiftTypeToTF(value->getType().getSwiftRValueType());
  assert(dtypeVal && "Can only convert TF compatible types to Tensors");

  // Get a reference to the CreateCTensorHandle function, which is defined like
  // this:
  // @_silgen_name("_swift_tfc_CreateCTensorHandle")
  // func _TFCCreateCTensorHandle<T>(_ value : T,
  //                                 _ dtype: TF_DataType) -> CTensorHandle
  auto createFn = B.getModule().findFunction("_swift_tfc_CreateCTensorHandle",
                                             SILLinkage::PublicExternal);
  auto *fnRef = B.createFunctionRef(loc, createFn);

  auto dtypeType = fnRef->getFunctionType()->getParameters()[1].getType();
  auto dtypeDecl = dtypeType->getAnyNominal();
  auto dtypeFieldType = getSingleElementDeclFieldType(dtypeDecl);

  // The internal type is something like UInt32.  Create it now.
  auto dtype = createSomeIntegerValue(dtypeVal, B, loc,
                                      dtypeFieldType->getAnyNominal());
  // Then wrap it in the dtype enum.
  dtype = wrapInStruct(dtype, dtypeDecl, B, loc);

  // This is a generic function over the value type, so we have to pass it in
  // by address on the stack.
  auto stackAlloc = B.createAllocStack(loc, value->getType());

  auto storeOwnership =
    B.getFunction().hasQualifiedOwnership() ? StoreOwnershipQualifier::Trivial :
                                          StoreOwnershipQualifier::Unqualified;
  B.createStore(loc, value, stackAlloc, storeOwnership);

  auto access = B.createBeginAccess(loc, stackAlloc, SILAccessKind::Read,
                                    SILAccessEnforcement::Static);

  Substitution sub(value->getType().getSwiftRValueType(), {});
  auto result = B.createApply(loc, fnRef, {sub}, { access, dtype },
                              /* isNonThrowing */false);
  // Finish our read access and free the stack memory.
  B.createEndAccess(loc, access, /*aborted*/false);
  B.createDeallocStack(loc, stackAlloc);

  return result;
}

/// Rewrite the host program, inserting a call to _TFCStartTensorComputation at
/// the start point of the tensor function, passing in the tensor program
/// itself, input tensor arguments etc.
///
/// The resultValues list is the set of values moved to the accelerator program
/// that should be returned as program results instead of being sent back.
auto TFFunctionPartition::
insertTensorComputationStartEndTerminate(ArrayRef<SILValue> resultValues)
       -> PartitionedTensorProgram {
  auto &ctx = fn.getASTContext();
  auto loc = fn.getLocation();

  // We are going to create a call to this function to kick off the tensor
  // computation:
  //
  // The C type is TF_TensorHandle*
  // public typealias CTensorHandle = OpaquePointer
  //
  // @_silgen_name("_swift_tfc_StartTensorComputation")
  // public func _TFCStartTensorComputation(
  //   _ programByteAddress: UnsafeRawPointer,
  //   _ programByteCount: Int,
  //   _ entryFunctionName: UnsafePointer<Int8>,
  //   _ tensorArgumentAddress: UnsafePointer<CTensorHandle>,
  //   _ tensorArgumentCount: Int,
  //   _ resultCount: Int
  // ) -> TensorComputation {
  auto startComputationFn =
    fn.getModule().findFunction("_swift_tfc_StartTensorComputation",
                                SILLinkage::PublicExternal);

  // We are going to create a call to this function to syncronize with the
  // completed tensor computation and collect the results.
  //
  // @_silgen_name("_swift_tfc_FinishTensorComputation") public
  // func _TFCFinishTensonComputation(
  //    _ computation: TensorComputation,
  //    _ resultAddress: UnsafeMutablePointer<CTensorHandle>,
  //    _ tensorResultCount: Int) {...}
  auto finishComputationFn =
    fn.getModule().findFunction("_swift_tfc_FinishTensorComputation",
                                SILLinkage::PublicExternal);

  // We may also generate a call to this function to kill the tensor computation
  // if the host execution goes away:
  //
  // @_silgen_name("_swift_tfc_TerminateTensorComputation")
  // public func _TFCTerminateTensorComputation(
  //   _ computation: TensorComputation
  // ) {...}
  //
  auto terminateComputationFn =
    fn.getModule().findFunction("_swift_tfc_TerminateTensorComputation",
                                SILLinkage::PublicExternal);

  if (!startComputationFn || !finishComputationFn || !terminateComputationFn) {
    diagnose(ctx, fn.getLocation().getSourceLoc(),
             diag::tf_internal_error,
             "'_swift_tfc_' entrypoints not found in TensorFlow module");
    return { nullptr, nullptr, nullptr, nullptr, SILValue() };
  }

  // Create various types and SIL types that we'll be using below.
  auto cTensorHandleTy = ctx.getOpaquePointerDecl()->getDeclaredType();
  auto cTensorHandleSILTy =
    SILType::getPrimitiveObjectType(cTensorHandleTy->getCanonicalType());
  auto unsafePointerType =
    BoundGenericType::get(ctx.getUnsafePointerDecl(), /*parent*/Type(),
                          cTensorHandleTy);
  auto unsafePointerSILType =
    SILType::getPrimitiveObjectType(unsafePointerType->getCanonicalType());
  auto unsafeMutPointerType =
    BoundGenericType::get(ctx.getUnsafeMutablePointerDecl(), /*parent*/Type(),
                          cTensorHandleTy);
  auto unsafeMutPointerSILType =
    SILType::getPrimitiveObjectType(unsafeMutPointerType->getCanonicalType());
  auto int8PointerType =
    BoundGenericType::get(ctx.getUnsafePointerDecl(), /*parent*/Type(),
                          ctx.getInt8Decl()->getDeclaredType());
  auto int8PointerSILType =
    SILType::getPrimitiveObjectType(int8PointerType->getCanonicalType());

  // This assumes that the first member of TensorHandle is the CTensorHandle.
  auto tensorHandleDecl = ctx.getTensorHandleDecl();
  assert(getSingleElementDeclFieldType(tensorHandleDecl) &&
         "TensorHandle should have exactly one field");
  auto tensorHandleMember = *tensorHandleDecl->getStoredProperties().begin();

  // Ownership markers for CTensorHandle accesses.
  auto loadOwnership =
    fn.hasQualifiedOwnership() ? LoadOwnershipQualifier::Trivial :
                                 LoadOwnershipQualifier::Unqualified;
  auto storeOwnership =
    fn.hasQualifiedOwnership() ? StoreOwnershipQualifier::Trivial :
                                 StoreOwnershipQualifier::Unqualified;


  SILBuilder B(tensorStartPoint);

  // Create a string literal to hold the  serialized protobuf for the tensor
  // program.  We haven't actually created that yet, so we create a placeholder
  // and RAUW it later.
  auto programPlaceholder =
    B.createStringLiteral(loc, StringRef(), StringLiteralInst::Encoding::Bytes);
  auto program = wrapInStruct(programPlaceholder, ctx.getUnsafeRawPointerDecl(),
                              B, loc);
  auto entryFunctionNamePlaceholder =
    B.createStringLiteral(loc, StringRef(), StringLiteralInst::Encoding::UTF8);
  auto entryFunctionName = B.createStruct(loc, int8PointerSILType,
                                          { entryFunctionNamePlaceholder });

  // Pass a length of zero for now, it will be filled in later.
  IntegerLiteralInst *programLengthPlaceholder = nullptr;
  auto programLength = createIntValue(0, B, loc, &programLengthPlaceholder);

  // We pass the list of N tensor arguments as a pointer + length of
  // CTensorHandle values, i.e.:
  //   (..., _ inputs: UnsafePointer<CTensorHandle>, _ numInputs: Int)
  // to get this, we create an N-ary tuple on the stack and pass the address of
  // the first element.

  // Note that the allocation becomes a scalar value when it has one value.
  SmallVector<TupleTypeElt, 8> tupleEltTypes(tensorFnArguments.size(),
                                             TupleTypeElt(cTensorHandleTy));
  auto tupleType = TupleType::get(tupleEltTypes, ctx)->getCanonicalType();
  // %0 = alloc_stack $(CTensorHandle, CTensorHandle)
  auto stackAlloc =
    B.createAllocStack(loc, SILType::getPrimitiveObjectType(tupleType));

  // Emit a store into the tuple for each parameter, giving it a copy of the
  // OpaquePointer that is within the TensorHandle<T> value we have.
  for (size_t i = 0, numArgs = tensorFnArguments.size(); i != numArgs; ++i) {
    auto tensorValue = tensorFnArguments[i];

    // The argument is either a TensorHandle<T> or a scalar value that we've
    // closed over.  If it is a TensorHandle<T>, load the CTensorHandle out of
    // it.  If it is a scalar, then we need to box the scalar in a
    // CTensorHandle.
    if (isTensorHandle(tensorValue->getType().getSwiftRValueType())) {
      auto fieldAddress = B.createRefElementAddr(loc, tensorValue,
                                                 tensorHandleMember);
      tensorValue = B.createLoad(loc, fieldAddress, loadOwnership);
    } else {
      tensorValue = convertScalarToHostTensorHandle(tensorValue, B, loc);
    }
    SILValue eltAddr = stackAlloc;
    if (numArgs >= 2)
      eltAddr = B.createTupleElementAddr(loc, stackAlloc, i,
                                         cTensorHandleSILTy.getAddressType());
    B.createStore(loc, tensorValue, eltAddr, storeOwnership);
  }

  // Ok, now we have our array on the stack.  Start a read access, and get the
  // address of the first element as an UnsafePointer<CTensorHandle>.
  // %1 = begin_access [read] [static] %0 : $*(CTensorHandle, CTensorHandle)
  auto access = B.createBeginAccess(loc, stackAlloc, SILAccessKind::Read,
                                    SILAccessEnforcement::Static);

  // %2 = tuple_element_addr %1 : $*(CTensorHandle, CTensorHandle), 0
  SILValue firstPtr = access;
  if (tensorFnArguments.size() >= 2)
    firstPtr = B.createTupleElementAddr(loc, firstPtr, 0,
                                        cTensorHandleSILTy.getAddressType());
  // %3 = address_to_pointer %2 : $*CTensorHandle to $Builtin.RawPointer
  firstPtr = B.createAddressToPointer(loc, firstPtr,
                                      SILType::getRawPointerType(ctx));

  // %4 = struct $UnsafePointer<CTensorHandle>(%3 : $Builtin.RawPointer)
  firstPtr = B.createStruct(loc, unsafePointerSILType, firstPtr);

  auto numTensorArguments = createIntValue(tensorFnArguments.size(), B, loc);

  // TODO: When the runtime matures, we shouldn't have to pass in the # results.
  auto numTensorResults = createIntValue(resultValues.size(), B, loc);

  // The first two arguments are the program, the rest of the arguments are the
  // parameters passed in.
  SILValue startArgs[] = {
    program,            // programByteAddress: UnsafeRawPointer
    programLength,      // programByteCount: Int
    entryFunctionName,  // entryFunctionName: UnsafePointer<Int8>
    firstPtr,           // tensorArgumentAddress: UnsafePointer<CTensorHandle>
    numTensorArguments, // tensorArgumentCount: Int
    numTensorResults    // resultCount: Int
  };

  // Now that we have our argument list, create a call.
  auto startProgramFnRef = B.createFunctionRef(loc, startComputationFn);

  // Create the runtime call in the host program that kicks off the tensor
  // program, setting the argument values we provide as the tensor params.
  auto tensorComputation =
    B.createApply(loc, startProgramFnRef, /*no substitutions*/{}, startArgs,
                  /*isNonThrowing*/false);

  // Finish our read access and free the stack memory.
  // end_access %1 : $*(CTensorHandle, CTensorHandle)
  B.createEndAccess(loc, access, /*aborted*/false);
  // dealloc_stack %0 : $*(CTensorHandle, CTensorHandle)
  B.createDeallocStack(loc, stackAlloc);

  // Create the runtime call in the host program that rendezvous with the tensor
  // program and returns the results.

  // Start by creating an uninitialized buffer to receive the values into.
  B.setInsertionPoint(tensorEndPoint);

  // Note that the allocation becomes a scalar value when it has one value.
  tupleEltTypes = SmallVector<TupleTypeElt, 8>(resultValues.size(),
                                               TupleTypeElt(cTensorHandleTy));
  tupleType = TupleType::get(tupleEltTypes, ctx)->getCanonicalType();

  // %0 = alloc_stack $(CTensorHandle, CTensorHandle)
  stackAlloc =
    B.createAllocStack(loc, SILType::getPrimitiveObjectType(tupleType));

  // Ok, now we have our uninitialized array on the stack.  Start a write
  // access, and get the address of the first element as an
  // UnsafePointer<CTensorHandle>.
  // %1 = begin_access [write] [static] %0 : $*(CTensorHandle, CTensorHandle)
  access = B.createBeginAccess(loc, stackAlloc, SILAccessKind::Modify,
                               SILAccessEnforcement::Static);

  // %2 = tuple_element_addr %1 : $*(CTensorHandle, CTensorHandle), 0
  firstPtr = access;
  if (resultValues.size() >= 2)
    firstPtr = B.createTupleElementAddr(loc, firstPtr, 0,
                                        cTensorHandleSILTy.getAddressType());
  // %3 = address_to_pointer %2 : $*CTensorHandle to $Builtin.RawPointer
  firstPtr = B.createAddressToPointer(loc, firstPtr,
                                      SILType::getRawPointerType(ctx));

  // %4 = struct $UnsafeMutablePointer<CTensorHandle>(%3: $Builtin.RawPointer)
  firstPtr = B.createStruct(loc, unsafeMutPointerSILType, firstPtr);

  auto finishComputationFnRef = B.createFunctionRef(loc, finishComputationFn);

  // Create the builtin in the host program that kicks off the tensor program,
  // setting the argument values.
  B.createApply(loc, finishComputationFnRef, /*no substitutions*/{},
                /*args*/ {tensorComputation, firstPtr, numTensorResults},
                /*isNonThrowing*/false);

  // end_access %1 : $*(CTensorHandle, CTensorHandle)
  B.createEndAccess(loc, access, /*aborted*/false);

  // After the call, we have a buffer filled in with CTensorHandle values, load
  // them, taking ownership and RAUW'ing uses of the old value to the newly
  // loaded value.
  for (unsigned resultNumber = 0, e = resultValues.size(); resultNumber != e;
       ++resultNumber) {
    SILValue result = resultValues[resultNumber];
    SILValue eltAddress = stackAlloc;
    if (resultValues.size() > 1) {
      eltAddress =
        B.createTupleElementAddr(result.getLoc(),
                                 eltAddress, resultNumber,
                                 cTensorHandleSILTy.getAddressType());
    }

    // The load takes ownership from the buffer, leaving the buffer
    // uninitialized again.
    SILValue newValue = B.createLoad(result.getLoc(), eltAddress,
                                     loadOwnership);

    // Create a new TensorHandle<T> type to take ownership.
    auto newTH = B.createAllocRef(result.getLoc(), result->getType(),
                                  /*objc*/false, /*canAllocOnStack*/false,
                                  /*elementTypes*/{},
                                  /*elementCountOperands*/{});
    auto fieldAddress =
      B.createRefElementAddr(result.getLoc(), newTH, tensorHandleMember);

    B.createStore(result.getLoc(), newValue, fieldAddress,
                  storeOwnership);

    // Manually walk the use list in a custom way to avoid invalidating the
    // iterator as we potentially change it.
    for (auto UI = result->use_begin(), UE = result->use_end(); UI != UE; ) {
      auto *operand = *UI++;
      auto user = operand->getUser();

      // Users may be either inside (e.g. another tensor op, or a non-tensor
      // op that causes a copy back to the host) or outside the tensor
      // program.  If it is after the tensor op, we can replace the use with
      // the corresponding result value.  If inside, we'll nuke it later.
      if (DI.dominates(tensorEndPoint, user))
        operand->set(newTH);
    }
  }

  // Now that we are done with the buffer of results, get rid of it.  It is
  // uninitialized at this point, so it should not be destroyed.

  // dealloc_stack %0 : $*(CTensorHandle, CTensorHandle)
  B.createDeallocStack(loc, stackAlloc);



  // tensorComputation is the return value of _TFCStartTensorComputation.  By
  // construction, it is known to dominate all abort points and the finish point
  // point of the program.
  //
  // If the host program reaches any of the tensorKillBlocks, it should abort
  // execution of the tensor program.
  for (auto *killBB : tensorKillBlocks) {
    B.setInsertionPoint(&killBB->front());

    auto terminateComputationFnRef =
      B.createFunctionRef(loc, terminateComputationFn);

    // Create the builtin in the host program that kicks off the tensor program,
    // setting the argument values.
    B.createApply(loc, terminateComputationFnRef, /*no substitutions*/{},
                  /*args*/{ tensorComputation }, /*isNonThrowing*/false);
  }

  return {
    nullptr,  // New function hasn't been created yet.
    programPlaceholder,
    programLengthPlaceholder,
    entryFunctionNamePlaceholder,
    tensorComputation
  };
}


/// Run the TensorFlow partitioning pass.  This pass is a very close relative to
/// the standard "Aggressive Dead Code Elimination" (ADCE) optimization which is
/// implemented using post-dominance frontiers and control dependence
/// information, but instead of determining live code, we're determining
/// operations and a subset of the CFG that is profitable and interesting to
/// move to the accelerator.
///
/// This returns null if there is no tensor work to extract, or a function that
/// has been generated if there is.
///
auto TFFunctionPartition::partition() -> PartitionedTensorProgram {
  assert(!markedBlocks.empty() &&
         "Shouldn't run on functions with no tensor ops");

  // Start by determining the result values produced by the tensor program.
  // These will be returned by the tensor end point instead of being sent back
  // from the accelerator to the host.
  SmallVector<SILValue, 4> resultValues;

  // TODO: Should also handle Tensor BB Arguments to avoid sends.
  for (auto markInfo : markedInstructions) {
    // We only care about values that are being moved to the accelerator.  If
    // they are being copied over, we can just use the original value computed
    // on the host.
    if (markInfo.second != Marking::Move)
      continue;

    auto inst = markInfo.first;

    // Scan all the users of return values of the instructions moved over.  If
    // any of the results cannot be handled with a result, then we just send the
    // whole value.
    bool hasAnyNonResultUse = false, hasAnyUse = false;
    for (auto result : inst->getResults())
      for (auto *operand : result->getUses()) {
        auto user = operand->getUser();

        // If the user is to be deleted or moved, then we can safely ignore it.
        auto it = markedInstructions.find(user);
        if (it != markedInstructions.end() &&
            (it->second == Marking::Delete || it->second == Marking::Move))
          continue;

        // Remember if the instruction has any use.  If not, then it never needs
        // to be sent or returned.
        hasAnyUse = true;

        // If the end point dominates the out-of-model use, then we can
        // represent it with the return value of the tensor program.  Otherwise
        // it will turn into a send of data back to the host.
        if (!sinkValueAfterEndPoint(user, tensorEndPoint, DI)) {
          hasAnyNonResultUse = true;
          break;
        }
      }

    // If all of the results can be handled with return values, then handle them
    // that way by appending them to our result list.
    if (hasAnyUse && !hasAnyNonResultUse)
      resultValues.append(inst->getResults().begin(), inst->getResults().end());
  }

  // Insert the start/finish and any terminate runtime calls.
  auto result = insertTensorComputationStartEndTerminate(resultValues);

  // If the TensorFlow module is malformed, bail out without breaking the code.
  if (!result.theTensorComputation)
    return result;

  // Calculate the parameter list for the new function.
  SmallVector<SILParameterInfo, 4> params;
  for (auto v : tensorFnArguments) {
    auto argTy = convertToTensorHandleType(v->getType());
    params.push_back(SILParameterInfo(argTy.getSwiftRValueType(),
                                      ParameterConvention::Direct_Unowned));
  }

  SmallVector<SILResultInfo, 4> results;
  for (auto r : resultValues)
    results.push_back(SILResultInfo(r->getType().getSwiftRValueType(),
                                    ResultConvention::Unowned));


  // Create the partitioned function, which never has arguments or result
  // values, since they get sent and received back and forth.
  auto newFnType =
    SILFunctionType::get(/*genericSig*/nullptr, SILFunctionType::ExtInfo(),
                         SILCoroutineKind::None,
                         ParameterConvention::Direct_Owned, params,
                         /*interfaceYields*/{},
                         results, /*interfaceErrorResult*/None,
                         fn.getModule().getASTContext());
  result.fn =
    fn.getModule().getOrCreateFunction(fn.getLocation(),
                                       fn.getName().str()+".tf_partition",
                                       SILLinkage::Private, newFnType,
                                       /*What's this*/IsBare, IsNotTransparent,
                                       IsNotSerialized);

  PartitionCloner PC(*this, *result.fn);

  // Fill in the cloned function body.
  PC.cloneFunction(resultValues);

  // Clean up the source function, removing the tensor code.
  PC.finalizeOriginal();
  return result;
}


//===----------------------------------------------------------------------===//
//                              Top Level Driver
//===----------------------------------------------------------------------===//


// Our partitioning can leave around lots of unconditional branches between
// blocks that formerly had control edges.  Go through and merge those to make
// later passes simpler.
static void contractUncondBranches(SILFunction *fn) {
  // Iterate carefully to avoid invalidating iterators: we mutate the block list
  // while we walk it.
  for (auto bbi = fn->begin(), e = fn->end(); bbi != e; ) {
    auto *bb = &*bbi;
    ++bbi;  // Increment the iterator in case we do no transformation.

    if (auto succ = bb->getSingleSuccessorBlock()) {
      if (succ != bb && succ->getSinglePredecessorBlock()) {
        if (auto *BI = dyn_cast<BranchInst>(bb->getTerminator())) {
          // If there are any BB arguments in the destination, replace them with
          // the branch operands, since they must dominate the dest block.
          for (unsigned i = 0, e = BI->getArgs().size(); i != e; ++i) {
            assert(succ->getArgument(i) != BI->getArg(i) &&
                   "Cloned code regions are always reachable");
            succ->getArgument(i)->replaceAllUsesWith(BI->getArg(i));
          }

          // Zap BI and move all of the instructions from DestBB into this one.
          BI->eraseFromParent();
          bb->spliceAtEnd(succ);
          succ->eraseFromParent();

          // Revisit this node: we have new successor(s) and may need to
          // contract them as well.  Also, bbi may be invalidated at this point.
          bbi = SILFunction::iterator(bb);
        }
      }
    }
  }
}

namespace {
class TFPartition : public SILFunctionTransform {
  bool isTest = false;
  TensorFunctionClassifier tfc;
public:
  TFPartition(bool isTest) : isTest(isTest) {}

  /// The entry point to the transformation.
  void run() override {
    SILFunction *fn = getFunction();
    auto &ctx = fn->getASTContext();

    // TODO(clattner): This logic will eventually be subsumed by the
    // corresponding logic in the TFDeabstraction pass.  Until deabstraction
    // is done, we have some amount of top-level redundancy here because we have
    // to run partitioning after the optimization passes.

    // If the TensorFlow module hasn't been imported by the program, don't do
    // anything.  This avoids impacting compile time for non-TensorFlow using
    // Swift programs by doing extraneous analysis.
    auto tfModule = ctx.getLoadedModule(ctx.getIdentifier("TensorFlow"));
    if (!tfModule)
      return;

    // If this function is a building block of larger tensor programs (e.g.
    // the ops defined in the TensorFlow module), then don't transform it in
    // isolation.
    if (!tfc.shouldBePartitioned(fn))
      return;

    TFFunctionPartition partitioner(*fn, PM, *tfModule);
    if (!partitioner.markFunction())
      return; // No tensor ops found in the function.

    // Check to see if we cannot transform the function but should.  In this
    // case we emit a compiler error.  This is a limitation of the compiler that
    // will need to be resolved in the future (possibly through a model change),
    // it's not clear if we should allow partitioning to work on unspecialized
    // generics.
    if (fn->getLoweredFunctionType()->isPolymorphic()) {
      auto &ctx = fn->getASTContext();
      diagnose(ctx, fn->getLocation().getSourceLoc(),
               diag::tf_internal_error,
               "TensorFlow partitioning does not work on generic functions yet");
      return;
    }

    // Because we're in active development, it is common to do something wrong
    // in the TensorFlow module.  Detect and reject things here.
    if (fn->getModule().getSwiftModule() == tfModule) {
      auto &ctx = fn->getASTContext();
      diagnose(ctx, fn->getLocation().getSourceLoc(),
               diag::tf_internal_error,
               "nothing in the TensorFlow module should require partitioning, "
               "did you forget @_inlineable on '" + fn->getName().str() + "'?");
      return;
    }

    if (shouldDumpIntermediates()) {
      llvm::outs() << "---- INPUT FUNCTION " << fn->getName() <<" ----------\n";
      fn->print(llvm::outs());
      llvm::outs() << "---- END OF INPUT FUNCTION ----------\n";
    }

    // Actually do the partitioning transformation, splitting out a new SIL
    // function for the tensor program body.
    auto tensorProgram = partitioner.partition();

    // If the TensorFlow module is malformed, exit without breaking the SIL:
    // an error has already been emitted.
    if (!tensorProgram.fn)
      return;

#ifndef NDEBUG
    // Verify that the generated function is ok.
    tensorProgram.fn->verify();
#endif

    // Our partitioning can leave around lots of unconditional branches between
    // blocks that formerly had control edges.  Go through and merge those to
    // make later passes simpler.
    contractUncondBranches(tensorProgram.fn);

    if (isTest || shouldDumpIntermediates()) {
      llvm::outs() << "--- TFPartition Accelerator Result: "
                   << tensorProgram.fn->getName() << "\n";
      tensorProgram.fn->print(llvm::outs());
      llvm::outs() << "----\n";
    }

    // If this is called from sil-opt, we currently just print out the results
    // and quit.  This allows writing regression tests for the tf-partition
    // pass in isolation.  This is pretty unconventional for a SIL pass, but
    // this is an unconventional pass!
    if (isTest) {
      llvm::outs() << "--- TFPartition Host Result: " << fn->getName() << "\n";
      fn->print(llvm::outs());
      llvm::outs() << "---\n";
      llvm::outs().flush();

      // Finally, we're done.  Remove the partitioned function so it doesn't go
      // through the normal compiler flow.
      tensorProgram.fn->getModule().eraseFunction(tensorProgram.fn);
      return;
    }


    // Next translate it to a graph and emit it as a global symbol.
    auto bytes = lowerTFGraph(tensorProgram.fn);

    // Now that we know what the tensor program actually is, we can replace the
    // placeholder instructions for the data + length with the actual bits we
    // want to use.
    {
      SILBuilder B(tensorProgram.programPlaceholder);
      auto data = B.createStringLiteral(fn->getLocation(),
                                        StringRef(bytes.data(), bytes.size()),
                                        StringLiteralInst::Encoding::Bytes);
      auto len = B.createIntegerLiteral(fn->getLocation(),
                              tensorProgram.programLengthPlaceholder->getType(),
                                        bytes.size());

      // Strip off the $ from the start of a function name if present.
      // FIXME: Factor this out somewhere common.
      auto fnName = tensorProgram.fn->getName();
      if (fnName.startswith("$"))
        fnName = fnName.substr(1);

      auto name = B.createStringLiteral(fn->getLocation(), fnName,
                                        StringLiteralInst::Encoding::UTF8);
      tensorProgram.programPlaceholder->replaceAllUsesWith(data);
      tensorProgram.programPlaceholder->eraseFromParent();
      tensorProgram.programLengthPlaceholder->replaceAllUsesWith(len);
      tensorProgram.programLengthPlaceholder->eraseFromParent();
      tensorProgram.entryFunctionNamePlaceholder->replaceAllUsesWith(name);
      tensorProgram.entryFunctionNamePlaceholder->eraseFromParent();
    }

    if (shouldDumpIntermediates()) {
      llvm::outs() << "--- TFPartition Host Result: " << fn->getName() << "\n";
      fn->print(llvm::outs());
      llvm::outs() << "---\n";
      llvm::outs().flush();
    }

    // Finally, we're done.  Remove the partitioned function so it doesn't go
    // through the normal compiler flow.
    tensorProgram.fn->getModule().eraseFunction(tensorProgram.fn);
  }
};

} // end anonymous namespace

SILTransform *swift::createTFPartition() {
  return new TFPartition(/*isTest*/false);
}

/// Create a version of the tf-partition pass that is used by sil-opt for
/// testcases.  TF-Partition is not a normal pass, so we need an unconventional
/// approach here.
SILTransform *swift::createTFPartitionTest() {
  return new TFPartition(/*isTest*/true);
}
